# ADP 필기시험 실전 예상문제 (최신 출제경향 반영)

> **작성일**: 2026년 2월  
> **시험구성**: 객관식 80문제 (각 1점) + 서술형 1문제 (20점)  
> **시험시간**: 180분  
> **합격기준**: 100점 만점 중 70점 이상

---

## 📋 목차
- [1과목: 데이터의 이해 (10문제)](#1과목-데이터의-이해)
- [2과목: 데이터 처리 기술 (10문제)](#2과목-데이터-처리-기술)
- [3과목: 데이터 분석 기획 (10문제)](#3과목-데이터-분석-기획)
- [4과목: 데이터 분석 (40문제)](#4과목-데이터-분석)
- [5과목: 데이터 시각화 (10문제)](#5과목-데이터-시각화)
- [서술형 문제 (20점)](#서술형-문제)

---

# 1과목: 데이터의 이해

## 문제 1
빅데이터의 특징을 나타내는 5V에 해당하지 않는 것은?

① Volume (규모)  
② Velocity (속도)  
③ Variety (다양성)  
④ Verification (검증)

**정답**: ④

**해설**:  
빅데이터의 5V는 다음과 같습니다:
- **Volume (규모)**: 데이터의 양
- **Velocity (속도)**: 데이터 생성·처리 속도
- **Variety (다양성)**: 정형·비정형 등 다양한 형식
- **Veracity (정확성)**: 데이터의 신뢰성
- **Value (가치)**: 데이터로부터 얻는 가치

Verification은 5V에 포함되지 않습니다.

---

## 문제 2
데이터 사이언스의 역할 중 "데이터를 통해 실제적이고 의미 있는 인사이트를 도출하고, 이를 비즈니스에 적용하는 역할"을 가장 잘 설명하는 것은?

① Data Engineer  
② Data Analyst  
③ Data Scientist  
④ Data Architect

**정답**: ②

**해설**:  
- **Data Analyst**: 데이터 분석을 통해 비즈니스 인사이트 도출, 의사결정 지원
- **Data Scientist**: 고급 통계/머신러닝 모델 개발, 예측 모델 구축
- **Data Engineer**: 데이터 파이프라인 구축, 인프라 관리
- **Data Architect**: 데이터 구조 설계, 데이터 거버넌스

---

## 문제 3
데이터 거버넌스의 주요 구성요소가 아닌 것은?

① 데이터 표준화  
② 데이터 품질 관리  
③ 데이터 시각화  
④ 메타데이터 관리

**정답**: ③

**해설**:  
데이터 거버넌스의 주요 구성요소:
- 데이터 표준화 (표준 용어, 코드 체계)
- 데이터 품질 관리 (정확성, 완전성, 일관성)
- 메타데이터 관리 (데이터에 대한 데이터)
- 데이터 보안 및 접근 관리
- 데이터 생명주기 관리

데이터 시각화는 데이터 분석 단계의 활동으로 거버넌스 구성요소가 아닙니다.

---

## 문제 4
CRISP-DM 방법론의 단계를 순서대로 나열한 것은?

ㄱ. 데이터 이해 (Data Understanding)  
ㄴ. 비즈니스 이해 (Business Understanding)  
ㄷ. 데이터 준비 (Data Preparation)  
ㄹ. 모델링 (Modeling)

① ㄱ → ㄴ → ㄷ → ㄹ  
② ㄴ → ㄱ → ㄷ → ㄹ  
③ ㄴ → ㄷ → ㄱ → ㄹ  
④ ㄱ → ㄷ → ㄴ → ㄹ

**정답**: ②

**해설**:  
CRISP-DM(Cross Industry Standard Process for Data Mining)의 6단계:
1. **Business Understanding** (비즈니스 이해)
2. **Data Understanding** (데이터 이해)
3. **Data Preparation** (데이터 준비)
4. **Modeling** (모델링)
5. **Evaluation** (평가)
6. **Deployment** (전개)

순환적 구조로 각 단계 간 피드백이 가능합니다.

---

## 문제 5
데이터 품질의 측정 기준이 아닌 것은?

① 정확성 (Accuracy)  
② 완전성 (Completeness)  
③ 일관성 (Consistency)  
④ 효율성 (Efficiency)

**정답**: ④

**해설**:  
데이터 품질 측정 기준:
- **정확성 (Accuracy)**: 데이터가 실제 값과 일치하는 정도
- **완전성 (Completeness)**: 필요한 모든 데이터가 존재하는 정도
- **일관성 (Consistency)**: 데이터 간 모순이 없는 정도
- **적시성 (Timeliness)**: 데이터가 최신 상태로 유지되는 정도
- **유효성 (Validity)**: 데이터가 정의된 규칙을 준수하는 정도

효율성은 시스템 성능 지표이지 데이터 품질 기준이 아닙니다.

---

## 문제 6
마스터 데이터 관리(MDM)에 대한 설명으로 가장 적절하지 않은 것은?

① 기업 전체의 핵심 데이터를 통합 관리하는 방법론이다  
② 데이터 중복과 불일치를 해소하는 것이 목적이다  
③ 고객, 제품, 자산 등이 관리 대상이 될 수 있다  
④ 트랜잭션 데이터의 실시간 처리가 주요 목적이다

**정답**: ④

**해설**:  
**MDM(Master Data Management)**:
- 기업의 핵심 참조 데이터(고객, 제품, 자산 등)를 통합 관리
- 데이터 중복 제거, 일관성 확보
- 단일 진실 공급원(Single Source of Truth) 구축
- **마스터 데이터**: 거의 변경되지 않는 참조 데이터

트랜잭션 데이터의 실시간 처리는 OLTP 시스템의 목적으로, MDM의 주요 목적이 아닙니다.

---

## 문제 7
데이터 웨어하우스의 특징으로 가장 적절하지 않은 것은?

① 주제 중심적 (Subject-Oriented)  
② 통합적 (Integrated)  
③ 시간 가변적 (Time-Variant)  
④ 정규화된 (Normalized)

**정답**: ④

**해설**:  
데이터 웨어하우스의 4대 특징 (Inmon의 정의):
1. **주제 중심적 (Subject-Oriented)**: 업무 영역별 주제로 구성
2. **통합적 (Integrated)**: 여러 소스의 데이터 통합
3. **시간 가변적 (Time-Variant)**: 시계열 데이터 저장
4. **비휘발적 (Non-Volatile)**: 일단 저장되면 삭제·수정 안 함

데이터 웨어하우스는 조회 성능을 위해 **비정규화(Denormalized)** 구조를 사용합니다. Star Schema, Snowflake Schema 등이 대표적입니다.

---

## 문제 8
데이터 마이닝의 주요 기법이 아닌 것은?

① 분류 (Classification)  
② 군집 (Clustering)  
③ 연관규칙 (Association Rule)  
④ 데이터 정제 (Data Cleansing)

**정답**: ④

**해설**:  
**데이터 마이닝의 주요 기법**:
- **분류 (Classification)**: 미리 정의된 그룹으로 분류 (지도학습)
- **군집 (Clustering)**: 유사한 데이터를 그룹화 (비지도학습)
- **연관규칙 (Association Rule)**: 항목 간 연관성 발견 (장바구니 분석)
- **예측 (Prediction)**: 미래 값 예측 (회귀분석)
- **이상탐지 (Anomaly Detection)**: 비정상 패턴 발견

데이터 정제는 전처리 과정으로 마이닝 기법이 아닙니다.

---

## 문제 9
개인정보 비식별화 기술 중 "생일을 연도로만 표시하거나 소득구간을 범주화하는" 기법은?

① 가명처리 (Pseudonymization)  
② 총계처리 (Aggregation)  
③ 데이터 삭제 (Data Suppression)  
④ 범주화 (Generalization)

**정답**: ④

**해설**:  
**비식별화 기술**:
- **가명처리**: 식별자를 다른 값으로 대체 (예: 홍길동 → A001)
- **총계처리**: 통계 값으로 대체 (예: 평균, 합계)
- **데이터 삭제**: 식별 가능 항목 삭제
- **범주화 (일반화)**: 구체적 값을 범주로 변환
  - 예: 1990-05-15 → 1990년생
  - 예: 소득 4,500만원 → 4,000~5,000만원

---

## 문제 10
분석 과제 발굴의 접근 방식 중 "기존 프로세스의 문제점을 파악하고 개선하는" 방식은?

① 하향식 접근법 (Top-Down)  
② 상향식 접근법 (Bottom-Up)  
③ 프로토타이핑 접근법  
④ 애자일 접근법

**정답**: ②

**해설**:  
**분석 과제 발굴 접근법**:

**상향식 (Bottom-Up)**:
- 현장의 문제점에서 시작
- 업무 프로세스 개선 중심
- 구체적이고 실행 가능한 과제
- Quick Win 가능

**하향식 (Top-Down)**:
- 전략적 목표에서 시작
- 경영진 주도
- 전사적 관점의 과제
- 장기적 관점

---

# 2과목: 데이터 처리 기술

## 문제 11
Hadoop의 HDFS(Hadoop Distributed File System)에 대한 설명으로 적절하지 않은 것은?

① NameNode는 메타데이터를 관리한다  
② DataNode는 실제 데이터를 저장한다  
③ 기본 블록 크기는 64KB이다  
④ 장애 대비를 위해 데이터 복제를 수행한다

**정답**: ③

**해설**:  
**HDFS 특징**:
- **NameNode**: 
  - 메타데이터 관리 (파일명, 권한, 블록 위치)
  - 단일 마스터 노드
  - HA(High Availability) 구성 가능 (Secondary NameNode)

- **DataNode**:
  - 실제 데이터 블록 저장
  - 여러 대의 슬레이브 노드
  - NameNode에 주기적으로 상태 보고 (Heartbeat)

- **블록 크기**: 
  - Hadoop 1.x: **64MB** (기본값)
  - Hadoop 2.x 이상: **128MB** (기본값)
  - 설정 가능 (dfs.blocksize)

- **복제본**: 기본 3개 (dfs.replication)

64KB가 아니라 64MB 또는 128MB입니다.

---

## 문제 12
MapReduce의 처리 과정에 대한 설명으로 가장 적절한 것은?

① Map → Shuffle → Reduce 순서로 진행된다  
② Reduce 단계에서 데이터를 분산 저장한다  
③ Shuffle 단계는 클라이언트에서 수행된다  
④ Map 함수는 Reduce 함수 이후에 실행된다

**정답**: ①

**해설**:  
**MapReduce 처리 과정**:

1. **Input**: HDFS에서 데이터 읽기
2. **Map**: 
   - 입력 데이터를 (key, value) 쌍으로 변환
   - 병렬 처리
   - 예: (word, 1) 생성
   
3. **Shuffle & Sort**:
   - Map 출력을 key 기준으로 정렬
   - 같은 key를 가진 데이터를 같은 Reducer로 전송
   - 네트워크를 통한 데이터 이동 (I/O 집약적)
   
4. **Reduce**:
   - 같은 key의 값들을 집계
   - 병렬 처리
   - 예: (word, count) 생성
   
5. **Output**: HDFS에 결과 저장

**예시 (단어 개수 세기)**:
```
Input: "hello world hello"
Map: (hello,1), (world,1), (hello,1)
Shuffle: (hello,[1,1]), (world,[1])
Reduce: (hello,2), (world,1)
```

---

## 문제 13
Spark의 특징으로 가장 적절하지 않은 것은?

① In-Memory 처리를 통해 Hadoop보다 빠른 성능을 제공한다  
② RDD(Resilient Distributed Dataset)를 기본 데이터 구조로 사용한다  
③ 배치 처리만 지원하며 실시간 처리는 불가능하다  
④ Lazy Evaluation을 통해 연산을 최적화한다

**정답**: ③

**해설**:  
**Apache Spark 특징**:

1. **In-Memory 처리**:
   - 메모리에서 데이터 처리
   - Hadoop MapReduce 대비 100배 빠름 (메모리 기준)
   - 10배 빠름 (디스크 기준)

2. **RDD (Resilient Distributed Dataset)**:
   - 불변성 (Immutable)
   - 분산 처리
   - 장애 복구 (Lineage 정보 활용)

3. **다양한 처리 모드**:
   - **Spark Core**: 배치 처리
   - **Spark Streaming**: 실시간 스트림 처리 (마이크로 배치)
   - **Spark SQL**: SQL 쿼리 처리
   - **MLlib**: 머신러닝
   - **GraphX**: 그래프 처리

4. **Lazy Evaluation**:
   - Transformation은 즉시 실행 안 함
   - Action이 호출될 때 실제 실행
   - DAG(Directed Acyclic Graph)로 최적화

Spark는 실시간 처리(Spark Streaming)도 지원합니다.

---

## 문제 14
NoSQL 데이터베이스의 유형과 예시가 잘못 연결된 것은?

① Key-Value Store - Redis  
② Document Store - MongoDB  
③ Column-Family Store - Cassandra  
④ Graph Database - MySQL

**정답**: ④

**해설**:  
**NoSQL 데이터베이스 유형**:

1. **Key-Value Store**:
   - 가장 단순한 구조
   - 예: Redis, DynamoDB, Riak
   - 사용: 캐싱, 세션 관리

2. **Document Store**:
   - JSON, XML 형식 문서 저장
   - 예: MongoDB, CouchDB, Couchbase
   - 사용: 콘텐츠 관리, 카탈로그

3. **Column-Family Store**:
   - 컬럼 기반 저장
   - 예: Cassandra, HBase, ScyllaDB
   - 사용: 시계열 데이터, 로그 분석

4. **Graph Database**:
   - 노드와 엣지로 관계 표현
   - 예: **Neo4j**, ArangoDB, OrientDB
   - 사용: 소셜 네트워크, 추천 시스템

MySQL은 관계형 데이터베이스(RDBMS)이지 Graph Database가 아닙니다.

---

## 문제 15
데이터베이스 정규화의 목적이 아닌 것은?

① 데이터 중복 최소화  
② 이상 현상 방지  
③ 데이터 무결성 유지  
④ 조회 성능 향상

**정답**: ④

**해설**:  
**정규화(Normalization)의 목적**:

1. **데이터 중복 최소화**:
   - 저장 공간 절약
   - 일관성 유지

2. **이상 현상(Anomaly) 방지**:
   - 삽입 이상 (Insertion Anomaly)
   - 삭제 이상 (Deletion Anomaly)
   - 갱신 이상 (Update Anomaly)

3. **데이터 무결성 유지**:
   - 일관성 보장
   - 데이터 정합성 확보

4. **유연한 데이터 구조**:
   - 확장성 향상
   - 유지보수 용이

**정규화의 단점**:
- 테이블 수 증가
- 조인 연산 증가
- **조회 성능 저하** 가능

따라서 데이터 웨어하우스에서는 의도적으로 **비정규화**를 수행하여 조회 성능을 향상시킵니다.

---

## 문제 16
SQL에서 GROUP BY와 함께 사용할 수 없는 집계 함수는?

① COUNT()  
② SUM()  
③ ROW_NUMBER()  
④ AVG()

**정답**: ③

**해설**:  
**집계 함수 (Aggregate Functions)**:
- GROUP BY와 함께 사용
- 여러 행을 하나의 값으로 집계
- 종류:
  - **COUNT()**: 개수
  - **SUM()**: 합계
  - **AVG()**: 평균
  - **MAX()**: 최대값
  - **MIN()**: 최소값

**윈도우 함수 (Window Functions)**:
- OVER() 절과 함께 사용
- 각 행에 대해 값 계산
- 종류:
  - **ROW_NUMBER()**: 행 번호
  - **RANK()**: 순위 (동순위 있음)
  - **DENSE_RANK()**: 순위 (연속)
  - **LAG()**: 이전 행 값
  - **LEAD()**: 다음 행 값

예시:
```sql
-- 집계 함수
SELECT department, AVG(salary)
FROM employees
GROUP BY department;

-- 윈도우 함수
SELECT name, salary,
       ROW_NUMBER() OVER (ORDER BY salary DESC) as rank
FROM employees;
```

---

## 문제 17
ETL(Extract, Transform, Load)에 대한 설명으로 적절하지 않은 것은?

① Extract는 소스 시스템에서 데이터를 추출하는 단계이다  
② Transform은 데이터를 정제하고 변환하는 단계이다  
③ Load는 데이터를 목적지에 적재하는 단계이다  
④ ELT는 ETL과 동일한 개념으로 순서만 다르다

**정답**: ④

**해설**:  
**ETL vs ELT**:

**ETL (Extract, Transform, Load)**:
1. **Extract**: 소스에서 데이터 추출
2. **Transform**: 중간 서버에서 변환·정제
   - 데이터 형식 변환
   - 중복 제거
   - 검증 및 정제
3. **Load**: 목적지(DW)에 적재

**장점**: 
- 깨끗한 데이터만 적재
- 목적지 시스템 부담 적음

**단점**: 
- 변환 시간 소요
- 중간 스토리지 필요

---

**ELT (Extract, Load, Transform)**:
1. **Extract**: 소스에서 데이터 추출
2. **Load**: 원본 그대로 목적지에 적재
3. **Transform**: 목적지에서 필요 시 변환

**장점**: 
- 빠른 적재
- 유연한 분석 (필요 시 변환)

**단점**: 
- 원본 데이터 저장 공간 필요
- 목적지 시스템 처리 부담

**차이점**:
- ETL: 전통적 DW, 구조화된 데이터
- ELT: 빅데이터, 클라우드 환경 (Snowflake, Redshift 등)

단순히 순서만 다른 것이 아니라 **처리 위치와 방식이 다릅니다**.

---

## 문제 18
데이터 파이프라인의 도구가 아닌 것은?

① Apache Airflow  
② Apache Kafka  
③ Apache NiFi  
④ Apache Hadoop

**정답**: ④

**해설**:  
**데이터 파이프라인 도구**:

1. **Apache Airflow**:
   - 워크플로우 관리 및 스케줄링
   - DAG(Directed Acyclic Graph)로 작업 정의
   - Python 기반
   - 사용: 배치 처리 파이프라인

2. **Apache Kafka**:
   - 분산 스트리밍 플랫폼
   - 실시간 데이터 파이프라인
   - Pub-Sub 메시징
   - 사용: 실시간 데이터 수집·전송

3. **Apache NiFi**:
   - 데이터 흐름 자동화
   - GUI 기반 설계
   - 다양한 소스/목적지 지원
   - 사용: 데이터 통합 및 전송

4. **Apache Hadoop**:
   - 분산 **저장 및 처리** 플랫폼
   - HDFS + MapReduce
   - 파이프라인 도구가 아닌 **처리 프레임워크**

---

## 문제 19
실시간 데이터 처리 아키텍처인 Lambda Architecture의 구성 요소가 아닌 것은?

① Batch Layer  
② Speed Layer  
③ Serving Layer  
④ Storage Layer

**정답**: ④

**해설**:  
**Lambda Architecture (람다 아키텍처)**:

배치 처리와 실시간 처리를 결합한 빅데이터 아키텍처

**3개 레이어**:

1. **Batch Layer (배치 계층)**:
   - 전체 데이터셋에 대한 배치 처리
   - 정확하지만 느림
   - Hadoop, Spark 등 사용
   - 목적: 정확한 마스터 데이터셋 생성

2. **Speed Layer (속도 계층)**:
   - 최신 데이터에 대한 실시간 처리
   - 빠르지만 근사치
   - Storm, Spark Streaming 등 사용
   - 목적: 낮은 지연시간 제공

3. **Serving Layer (서빙 계층)**:
   - Batch View + Real-time View 병합
   - 쿼리 인터페이스 제공
   - 사용자 요청에 응답
   - 목적: 통합된 결과 제공

**데이터 흐름**:
```
데이터 → Batch Layer → Batch View ↘
      ↘ Speed Layer → Real-time View → Serving Layer → 사용자
```

**Kappa Architecture (카파 아키텍처)**:
- Lambda의 복잡성을 개선
- Speed Layer만 사용 (배치 레이어 제거)
- 모든 처리를 스트림으로 통합

---

## 문제 20
데이터베이스 인덱스에 대한 설명으로 적절하지 않은 것은?

① B-Tree 인덱스는 범위 검색에 효율적이다  
② 인덱스가 많을수록 조회 성능이 항상 향상된다  
③ Hash 인덱스는 동등 비교(=)에 효과적이다  
④ 인덱스는 INSERT/UPDATE/DELETE 성능을 저하시킬 수 있다

**정답**: ②

**해설**:  
**인덱스(Index)**:

**장점**:
- 조회(SELECT) 속도 향상
- WHERE, JOIN, ORDER BY 성능 개선

**단점**:
- 추가 저장 공간 필요
- INSERT/UPDATE/DELETE 시 인덱스도 수정 → **성능 저하**
- 인덱스가 많으면:
  - 쓰기 작업 느려짐
  - 저장 공간 증가
  - 옵티마이저 선택 복잡도 증가

**인덱스 종류**:

1. **B-Tree Index** (기본):
   - 균형 트리 구조
   - 범위 검색 효율적
   - 대부분의 DBMS 기본값
   ```sql
   WHERE age BETWEEN 20 AND 30  -- 효율적
   WHERE name LIKE 'Kim%'       -- 효율적
   ```

2. **Hash Index**:
   - 해시 함수 사용
   - 동등 비교(=)만 효과적
   - 범위 검색 불가
   ```sql
   WHERE id = 100  -- 효율적
   WHERE age > 20  -- 비효율적
   ```

3. **Bitmap Index**:
   - 비트맵 사용
   - 카디널리티 낮은 컬럼 (성별, 등급 등)
   - 데이터 웨어하우스에서 주로 사용

**인덱스 설계 원칙**:
- 조회가 빈번한 컬럼에만 생성
- 선택도(Selectivity)가 높은 컬럼
- 쓰기가 많은 테이블은 신중히 결정
- 적정 개수 유지 (일반적으로 테이블당 5개 이하)

---

# 3과목: 데이터 분석 기획

## 문제 21
분석 마스터 플랜 수립 시 고려사항이 아닌 것은?

① 분석 과제의 우선순위 결정  
② 분석 수행을 위한 데이터 확보 방안  
③ 개별 분석가의 성과 평가 방법  
④ 분석 조직 및 인력 운영 방안

**정답**: ③

**해설**:  
**분석 마스터 플랜 구성요소**:

1. **분석 과제 정의**:
   - 과제 발굴 및 우선순위 선정
   - 투자 대비 효과(ROI) 분석
   - 실행 가능성 평가

2. **분석 거버넌스 체계**:
   - 조직 및 역할 정의
   - 표준 프로세스 수립
   - 의사결정 체계

3. **분석 인프라 구축**:
   - 하드웨어/소프트웨어
   - 데이터 확보 방안
   - 분석 환경 구축

4. **분석 역량 확보**:
   - 인력 확보 및 교육
   - 내·외부 역량 활용
   - 변화 관리

5. **실행 로드맵**:
   - 단계별 실행 계획
   - 일정 및 마일스톤
   - 위험 관리

개별 분석가의 **성과 평가**는 인사 관리 영역으로 마스터 플랜의 주요 고려사항이 아닙니다.

---

## 문제 22
분석 과제 우선순위 평가 기준이 아닌 것은?

① 전략적 중요도  
② 실행 가능성  
③ 시급성  
④ 데이터 크기

**정답**: ④

**해설**:  
**분석 과제 우선순위 평가 기준**:

1. **전략적 중요도 (Strategic Value)**:
   - 경영 목표 부합도
   - 비즈니스 영향도
   - 예상 효과(ROI)

2. **실행 가능성 (Feasibility)**:
   - 데이터 확보 가능성
   - 기술적 실현 가능성
   - 자원(예산, 인력) 충분성

3. **시급성 (Urgency)**:
   - 과제의 긴급도
   - 경영진의 관심도
   - 시장 상황

**평가 방법**:

**가. 가중치 기반 평가**:
```
우선순위 점수 = (전략적 중요도 × 0.4) 
               + (실행 가능성 × 0.3)
               + (시급성 × 0.3)
```

**나. 포트폴리오 매트릭스**:
```
           높음 ↑
실행       | Quick Win  | Strategic  |
가능성     |------------|------------|
           | Low        | Challenge  |
           낮음 ← 전략적 중요도 → 높음
```
- **Quick Win**: 빠른 성과, 우선 추진
- **Strategic**: 중장기 투자
- **Challenge**: 신중한 검토
- **Low**: 보류 또는 제외

데이터 크기는 실행 가능성의 하위 요소이지 독립적인 평가 기준이 아닙니다.

---

## 문제 23
분석 프로젝트의 위험 관리 방법으로 가장 적절하지 않은 것은?

① 데이터 품질 문제에 대한 사전 검증  
② 이해관계자와의 지속적인 커뮤니케이션  
③ 분석 결과의 사후 공개  
④ 프로젝트 진행 상황의 정기적 점검

**정답**: ③

**해설**:  
**분석 프로젝트 위험 관리**:

**주요 위험 요소**:
1. **데이터 관련**:
   - 품질 문제 (결측, 오류)
   - 접근 제한
   - 통합의 어려움

2. **기술 관련**:
   - 부적절한 분석 기법
   - 인프라 부족
   - 성능 문제

3. **조직 관련**:
   - 이해관계자 저항
   - 의사소통 부족
   - 자원 부족

**위험 관리 방법**:

1. **예방 (Prevention)**:
   - 데이터 품질 **사전** 검증
   - 요구사항 명확화
   - PoC(Proof of Concept) 수행

2. **완화 (Mitigation)**:
   - 단계적 접근 (Agile)
   - 백업 계획 수립
   - 교육 및 훈련

3. **모니터링 (Monitoring)**:
   - 정기적 진행 상황 점검
   - KPI 모니터링
   - 이슈 추적

4. **커뮤니케이션**:
   - 이해관계자 참여
   - 정기 보고
   - 피드백 수렴

분석 결과의 **사후** 공개는 위험 관리가 아니라 결과 보고 단계입니다. 위험 관리는 **사전 예방과 진행 중 모니터링**이 핵심입니다.

---

## 문제 24
데이터 분석의 3요소가 아닌 것은?

① 데이터 (Data)  
② 통계 (Statistics)  
③ 비즈니스 (Business)  
④ 시각화 (Visualization)

**정답**: ④

**해설**:  
**데이터 분석의 3요소 (Venn Diagram)**:

```
         통계/분석
           ↗  ↖
          /    \
    데이터  ←→  비즈니스
```

1. **데이터 (Data)**:
   - 데이터 수집·저장·관리
   - 데이터 품질
   - 데이터 엔지니어링

2. **통계/분석 (Statistics/Analytics)**:
   - 통계적 기법
   - 머신러닝 알고리즘
   - 분석 방법론

3. **비즈니스 (Business/Domain)**:
   - 도메인 지식
   - 문제 정의
   - 인사이트 도출

**교집합**:
- 데이터 + 통계 = **데이터 마이닝**
- 통계 + 비즈니스 = **전통적 연구**
- 비즈니스 + 데이터 = **비즈니스 인텔리전스**
- 3요소 모두 = **데이터 사이언스**

시각화는 분석 결과를 전달하는 **도구**이지 핵심 요소가 아닙니다.

---

## 문제 25
분석 프로젝트에서 PoC(Proof of Concept)의 주요 목적은?

① 전체 시스템의 완전한 구축  
② 기술적 실현 가능성 검증  
③ 최종 사용자 교육  
④ 운영 환경으로의 배포

**정답**: ②

**해설**:  
**PoC (Proof of Concept, 개념 검증)**:

**목적**:
1. **기술적 실현 가능성 검증**
   - 제안된 기술이 작동하는지 확인
   - 성능 요구사항 충족 여부

2. **위험 감소**
   - 본 프로젝트 전 리스크 파악
   - 예상치 못한 문제 조기 발견

3. **의사결정 지원**
   - 투자 여부 판단 근거
   - 대안 기술 비교

**특징**:
- **소규모**: 제한된 범위와 기능
- **단기**: 일반적으로 2~4주
- **프로토타입**: 완성품이 아님
- **학습 중심**: 실패 허용

**PoC vs 파일럿 vs MVP**:

| 구분 | 목적 | 범위 | 사용자 |
|------|------|------|--------|
| **PoC** | 기술 검증 | 최소 | 내부 |
| **Pilot** | 운영 테스트 | 제한적 | 소규모 실사용자 |
| **MVP** | 시장 검증 | 핵심 기능 | 초기 고객 |

**진행 순서**:
```
PoC → Pilot → MVP → 전체 배포
```

---

## 문제 26
분석 거버넌스 체계의 구성요소가 아닌 것은?

① 분석 지원 조직  
② 분석 과제 관리 프로세스  
③ 분석 결과 활용 및 공유 체계  
④ 개인정보 마케팅 활용 방안

**정답**: ④

**해설**:  
**분석 거버넌스 (Analytics Governance)**:

분석 활동의 효율성과 효과성을 높이기 위한 **관리 체계**

**구성요소**:

1. **조직 체계**:
   - 분석 조직 구조 (CoE, 분산형 등)
   - 역할 및 책임 (RACI)
   - 의사결정 권한

2. **프로세스**:
   - 분석 과제 발굴·선정
   - 분석 수행 표준 프로세스
   - 변경 관리 절차

3. **데이터 관리**:
   - 데이터 품질 관리
   - 메타데이터 관리
   - 데이터 보안 및 개인정보 보호

4. **결과 활용**:
   - 분석 결과 공유 체계
   - 피드백 및 개선
   - 지식 관리

5. **성과 관리**:
   - KPI 정의 및 모니터링
   - 과제별 성과 측정
   - 지속적 개선

**개인정보 마케팅 활용**은 마케팅 전략의 일부로 거버넌스 체계의 구성요소가 아닙니다. 단, "개인정보 **보호**"는 데이터 관리의 중요한 요소입니다.

---

## 문제 27
분석 조직 구조 중 "각 사업부에 분석 조직을 두고, 전사 차원의 CoE가 지원하는" 형태는?

① 집중형 (Centralized)  
② 분산형 (Decentralized)  
③ 기능 중심형 (Functional)  
④ 하이브리드형 (Hybrid)

**정답**: ④

**해설**:  
**분석 조직 구조**:

**1. 집중형 (Centralized)**:
```
        [본사 분석팀]
           ↓ 지원
    ┌──────┼──────┐
  사업부A 사업부B 사업부C
```
- **장점**: 
  - 자원 효율적 활용
  - 전문성 확보
  - 표준화 용이
- **단점**: 
  - 업무 이해 부족
  - 대응 속도 느림

**2. 분산형 (Decentralized)**:
```
  사업부A      사업부B      사업부C
  [분석팀]    [분석팀]    [분석팀]
```
- **장점**: 
  - 업무 전문성
  - 빠른 대응
- **단점**: 
  - 중복 투자
  - 역량 편차

**3. 하이브리드형 (Hybrid)**:
```
        [CoE - 전사 분석팀]
           ↓ 지원·조율
    ┌──────┼──────┐
  [분석팀][분석팀][분석팀]
  사업부A 사업부B 사업부C
```
- **CoE (Center of Excellence)**: 
  - 고급 분석 기법 개발
  - 표준 및 가이드라인
  - 교육 및 컨설팅
- **사업부 분석팀**: 
  - 일상적 분석 수행
  - 현업 밀착 지원

- **장점**: 
  - 집중형과 분산형의 장점 결합
  - 유연성과 전문성 동시 확보
- **단점**: 
  - 관리 복잡도 증가
  - 역할 정의 필요

**4. 기능 중심형 (Functional)**:
특정 분석 기능(예측, 추천, 이상탐지 등)별로 조직을 구성하는 형태로, 일반적인 분류에는 포함되지 않습니다.

---

## 문제 28
데이터 분석 성숙도 모델에서 가장 낮은 단계는?

① 최적화 (Optimized)  
② 관리 (Managed)  
③ 도입 (Introduced)  
④ 정의 (Defined)

**정답**: ③

**해설**:  
**데이터 분석 성숙도 모델** (Gartner의 5단계):

**1단계: 도입 (Introduced)**
- 산발적 분석 활동
- 표준 프로세스 없음
- 부서별 개별 수행
- 데이터 사일로 존재

**2단계: 정의 (Defined)**
- 분석 프로세스 정의
- 초기 거버넌스 구축
- 일부 표준화
- 데이터 품질 인식

**3단계: 반복 가능 (Repeatable)**
- 표준 프로세스 적용
- 체계적 관리
- 재사용 가능한 분석
- 부서 간 협업 시작

**4단계: 관리 (Managed)**
- 전사적 분석 문화
- 통합 데이터 플랫폼
- 고급 분석 기법 활용
- 성과 측정 체계

**5단계: 최적화 (Optimized)**
- 지속적 개선
- AI/ML 전사 적용
- 실시간 의사결정
- 데이터 기반 혁신

**각 단계별 특징**:
```
도입 → 정의 → 반복가능 → 관리 → 최적화
 ↓      ↓       ↓         ↓       ↓
혼돈   체계화   표준화    통합    혁신
```

---

## 문제 29
분석 과제 정의 시 SMART 원칙에 해당하지 않는 것은?

① Specific (구체적)  
② Measurable (측정 가능)  
③ Achievable (달성 가능)  
④ Revolutionary (혁명적)

**정답**: ④

**해설**:  
**SMART 원칙**:

목표를 효과적으로 설정하기 위한 기준

**S - Specific (구체적)**:
- 명확하고 구체적인 목표
- 예: "매출 증대" (X) → "온라인 매출 20% 증대" (O)

**M - Measurable (측정 가능)**:
- 정량적 지표로 측정 가능
- 예: "고객 만족도를 NPS 70점으로 향상"

**A - Achievable (달성 가능)**:
- 현실적으로 실현 가능
- 리소스와 역량 고려
- 예: 1개월 만에 100배 성장 (X)

**R - Relevant (관련성)**:
- 비즈니스 목표와 연계
- 전략적 우선순위 부합
- 예: 소매업체의 우주개발 분석 (X)

**T - Time-bound (기한)**:
- 명확한 목표 시점
- 예: "2026년 Q2까지 이탈률 10% 감소"

**적용 예시**:

❌ **나쁜 목표**:
"고객 만족도를 높인다"
- 구체적이지 않음
- 측정 불가
- 기한 없음

✅ **좋은 목표**:
"2026년 말까지 고객 이탈률을 현재 15%에서 10%로 감소시킨다"
- Specific: 이탈률 5%p 감소
- Measurable: 이탈률 %
- Achievable: 33% 감소, 합리적
- Relevant: 고객 유지 전략과 연계
- Time-bound: 2026년 말

---

## 문제 30
데이터 분석 프로젝트에서 "As-Is" 분석의 목적으로 가장 적절한 것은?

① 미래 목표 상태 정의  
② 현재 문제점 파악  
③ 분석 기법 선정  
④ 최종 결과 평가

**정답**: ②

**해설**:  
**As-Is vs To-Be 분석**:

**As-Is 분석 (현행 분석)**:

**목적**:
1. **현재 상태 파악**
   - 현행 프로세스 이해
   - 데이터 현황 조사
   - 시스템 구조 파악

2. **문제점 도출**
   - 병목 구간 식별
   - 비효율 요소 발견
   - 개선 필요 영역 파악

3. **Gap 분석 기준**
   - To-Be와 비교 기준점
   - 현재 역량 수준 측정

**산출물**:
- 현행 프로세스 맵
- 문제점 목록
- 데이터 현황 보고서

---

**To-Be 분석 (목표 분석)**:

**목적**:
1. **미래 상태 설계**
   - 목표 프로세스 정의
   - 개선 방안 제시
   - 시스템 구조 설계

2. **실행 계획 수립**
   - 변경 사항 정의
   - 이행 로드맵
   - 자원 계획

**산출물**:
- 목표 프로세스 맵
- 시스템 설계서
- 이행 계획서

---

**Gap 분석 (격차 분석)**:
```
As-Is → [Gap] → To-Be
현재    차이    목표

Gap 분석 = To-Be - As-Is
```

**분석 순서**:
```
1. As-Is 분석 (현재 이해)
2. To-Be 정의 (목표 설계)
3. Gap 분석 (차이 파악)
4. 실행 계획 (어떻게 달성)
```

---

# 4과목: 데이터 분석

## 문제 31
중심극한정리(Central Limit Theorem)에 대한 설명으로 가장 적절한 것은?

① 표본 크기가 충분히 크면 표본평균의 분포는 정규분포에 근사한다  
② 모집단이 정규분포여야만 성립한다  
③ 표본 크기가 작을수록 효과가 크다  
④ 표본분산의 분포에 대한 정리이다

**정답**: ①

**해설**:  
**중심극한정리 (CLT: Central Limit Theorem)**:

**핵심 내용**:
모집단의 분포와 **관계없이**, 표본 크기(n)가 충분히 크면 표본평균(X̄)의 분포는 **정규분포**에 근사합니다.

**수식**:
```
X̄ ~ N(μ, σ²/n)

여기서:
- μ: 모평균
- σ²: 모분산
- n: 표본 크기
```

**중요 특징**:

1. **모집단 분포 무관**:
   - 모집단이 균등분포, 지수분포 등 어떤 분포여도 성립
   - 정규분포가 아니어도 됨 (②번 오답 이유)

2. **표본 크기**:
   - 일반적으로 **n ≥ 30**이면 충분
   - 모집단이 정규분포에 가까우면 더 작은 n도 가능
   - 모집단이 심하게 치우치면 더 큰 n 필요

3. **표본평균의 표준편차**:
   ```
   SE(X̄) = σ/√n
   
   - n이 커질수록 표준오차 감소
   - 추정의 정확도 증가
   ```

**실용적 의미**:
- 신뢰구간 계산 가능
- 가설검정 수행 가능
- 모집단 분포 몰라도 추론 가능

**예시**:
```python
import numpy as np
import matplotlib.pyplot as plt

# 균등분포에서 샘플링 (정규분포 아님)
pop = np.random.uniform(0, 10, 100000)

# 표본평균 분포
means = [np.mean(np.random.choice(pop, 30)) for _ in range(1000)]

# 결과: 표본평균은 정규분포 형태
plt.hist(means, bins=30)
# 평균 ≈ 5, 정규분포 모양
```

---

## 문제 32
제1종 오류(Type I Error)와 제2종 오류(Type II Error)에 대한 설명으로 옳은 것은?

① 제1종 오류는 귀무가설이 참인데 기각하는 오류이다  
② 제2종 오류의 확률을 α라고 한다  
③ 제1종 오류와 제2종 오류는 동시에 감소시킬 수 있다  
④ 검정력은 제1종 오류의 확률이다

**정답**: ①

**해설**:  
**가설검정의 오류**:

**혼동 행렬**:
```
                실제 상황
              H₀ 참  |  H₀ 거짓
           ---------|----------
결정   H₀ 채택 | 정답 ✓  | 제2종 오류 (β)
       H₀ 기각 | 제1종 오류 (α) | 정답 ✓ (검정력)
```

**제1종 오류 (Type I Error, α)**:
- **정의**: H₀가 참인데 기각 (False Positive)
- **확률**: α (유의수준)
- **예시**: 
  - 정상인을 환자로 진단
  - 무죄인 사람을 유죄 판결
- **통제**: α를 미리 설정 (0.05, 0.01 등)

**제2종 오류 (Type II Error, β)**:
- **정의**: H₀가 거짓인데 채택 (False Negative)
- **확률**: β
- **예시**:
  - 환자를 정상으로 진단
  - 유죄인 사람을 무죄 판결
- **통제**: 표본 크기 증가로 감소

**검정력 (Power)**:
```
검정력 = 1 - β

- H₀가 거짓일 때 올바르게 기각할 확률
- 높을수록 좋음 (일반적으로 0.8 이상)
```

**Trade-off 관계**:
```
α ↓ → β ↑  (더 보수적: 거짓 양성 줄이면 거짓 음성 증가)
α ↑ → β ↓  (더 민감: 거짓 양성 증가하면 거짓 음성 감소)

해결 방법: 표본 크기(n) 증가
n ↑ → α, β 모두 감소 가능
```

**실무 적용**:

상황별 우선순위:
1. **의료 진단**: β 최소화 (환자 놓치지 않기)
2. **형사 재판**: α 최소화 (무고한 사람 처벌 방지)
3. **스팸 필터**: 상황에 따라 조정

---

## 문제 33
다음 중 비모수 검정 방법이 아닌 것은?

① Mann-Whitney U test  
② Wilcoxon signed-rank test  
③ Kruskal-Wallis test  
④ t-test

**정답**: ④

**해설**:  
**모수 검정 vs 비모수 검정**:

**모수 검정 (Parametric Test)**:
- 모집단의 분포 가정 (주로 정규분포)
- 모수(평균, 분산) 추정
- 검정력 높음 (조건 만족 시)
- **예시**:
  - **t-test**: 평균 비교
  - **F-test**: 분산 비교
  - **ANOVA**: 3개 이상 집단 평균 비교
  - **Pearson 상관계수**

**비모수 검정 (Non-parametric Test)**:
- 분포 가정 없음
- 순위(rank) 또는 중위수 사용
- 강건성(robustness) 높음
- 검정력 낮음 (정규분포일 때)

**주요 비모수 검정**:

| 모수 검정 | 비모수 검정 | 용도 |
|-----------|-------------|------|
| Independent t-test | **Mann-Whitney U test** | 두 독립 집단 비교 |
| Paired t-test | **Wilcoxon signed-rank test** | 대응 표본 비교 |
| One-way ANOVA | **Kruskal-Wallis test** | 3개 이상 독립 집단 |
| Pearson correlation | **Spearman correlation** | 상관관계 |
| - | **Chi-square test** | 범주형 독립성 |

**비모수 검정 사용 시기**:
1. **정규성 가정 위배**
   - 왜도(skewness)가 심함
   - 이상치(outlier) 존재
   
2. **소표본** (n < 30)
   
3. **순서형 데이터**
   - 리커트 척도 등
   
4. **분포 불명확**

**예시**:
```python
from scipy import stats

# 정규분포 가정 O: t-test
group1 = [23, 25, 27, 24, 26]
group2 = [30, 32, 31, 29, 33]
stats.ttest_ind(group1, group2)

# 정규분포 가정 X: Mann-Whitney
stats.mannwhitneyu(group1, group2)
```

---

## 문제 34
회귀분석에서 Durbin-Watson 통계량이 검정하는 것은?

① 다중공선성  
② 이분산성  
③ 자기상관  
④ 정규성

**정답**: ③

**해설**:  
**Durbin-Watson (DW) 통계량**:

**목적**: 회귀분석 잔차의 **자기상관(Autocorrelation)** 검정

**자기상관**:
- 시계열 데이터에서 발생
- 현재 오차가 이전 오차와 상관
- 회귀분석 가정 위배 (독립성 가정)

**DW 통계량**:
```
DW = Σ(eₜ - eₜ₋₁)² / Σeₜ²

여기서 eₜ는 t시점의 잔차
```

**판단 기준**:
```
DW 값 범위: 0 ~ 4

0       1       2       3       4
|-------|-------|-------|-------|
양의     경계     독립    경계    음의
자기상관         자기상관

- DW ≈ 2: 자기상관 없음 (독립)
- DW < 2: 양의 자기상관
- DW > 2: 음의 자기상관

일반적 기준:
- 1.5 < DW < 2.5: 자기상관 없음
- DW < 1 또는 DW > 3: 문제 있음
```

**자기상관 문제점**:
1. 표준오차 과소추정
2. t-검정, F-검정 신뢰성 저하
3. 신뢰구간 부정확

**해결 방법**:
1. **차분(Differencing)**:
   ```python
   df['y_diff'] = df['y'].diff()
   ```

2. **시차 변수 추가**:
   ```python
   df['y_lag1'] = df['y'].shift(1)
   ```

3. **자기회귀 모형**: ARIMA 등

**기타 진단**:
- **다중공선성**: VIF (Variance Inflation Factor)
- **이분산성**: Breusch-Pagan, White test
- **정규성**: Shapiro-Wilk, Kolmogorov-Smirnov

---

## 문제 35
주성분분석(PCA)에 대한 설명으로 가장 적절하지 않은 것은?

① 변수들의 분산을 최대로 하는 축을 찾는다  
② 주성분들은 서로 직교한다  
③ 라벨 정보를 사용하는 지도학습 방법이다  
④ 차원 축소 기법으로 사용된다

**정답**: ③

**해설**:  
**주성분분석 (PCA: Principal Component Analysis)**:

**정의**:
- **비지도학습** 기법
- 고차원 데이터를 저차원으로 축소
- 분산을 최대한 보존

**핵심 개념**:

1. **주성분 (Principal Component)**:
   - 데이터 분산이 최대인 방향
   - 원본 변수들의 선형결합
   - PC1 > PC2 > PC3 ... (분산 크기 순)

2. **직교성 (Orthogonality)**:
   - 주성분들은 서로 **직각**
   - 상관계수 = 0 (독립)
   - 중복 정보 없음

**수학적 원리**:
```
PC₁ = w₁₁X₁ + w₁₂X₂ + ... + w₁ₚXₚ
PC₂ = w₂₁X₁ + w₂₂X₂ + ... + w₂ₚXₚ
...

조건:
1. Var(PC₁) 최대화
2. PC₁ ⊥ PC₂ ⊥ PC₃ ...
3. ||w|| = 1 (정규화)
```

**절차**:
1. 데이터 표준화 (평균 0, 분산 1)
2. 공분산 행렬 또는 상관 행렬 계산
3. 고유값·고유벡터 계산
4. 고유값 큰 순서로 정렬
5. k개 주성분 선택

**주성분 개수 결정**:
1. **누적 기여율**: 80~90% 이상
2. **고유값 > 1** (Kaiser 규칙)
3. **Scree Plot**: Elbow point

**특징**:
- ✅ 차원 축소
- ✅ 다중공선성 해결
- ✅ 노이즈 제거
- ✅ 시각화 (2D, 3D)
- ❌ 해석 어려움 (원본 변수와 달리 의미 불명확)

**Python 예시**:
```python
from sklearn.decomposition import PCA

# PCA 수행
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 설명된 분산
print(pca.explained_variance_ratio_)
# [0.45, 0.32] → 77% 설명
```

**지도학습 vs 비지도학습**:
- **지도학습**: 라벨(y) 사용, 예측 목적
  - 예: 회귀, 분류
- **비지도학습**: 라벨 없음, 패턴 발견
  - 예: **PCA**, K-Means

---

## 문제 36
로지스틱 회귀분석에서 사용하는 링크 함수는?

① 항등 함수 (Identity)  
② 로그 함수 (Log)  
③ 로짓 함수 (Logit)  
④ 프로빗 함수 (Probit)

**정답**: ③

**해설**:  
**일반화 선형 모형 (GLM: Generalized Linear Model)**:

선형 모형을 확장하여 다양한 종속변수 분포를 다룸

**구조**:
```
g(E[Y]) = β₀ + β₁X₁ + ... + βₚXₚ

g(): 링크 함수 (Link Function)
```

**주요 링크 함수**:

| 모형 | 종속변수 | 분포 | 링크 함수 |
|------|----------|------|-----------|
| 선형회귀 | 연속형 | 정규분포 | **Identity** g(μ)=μ |
| 로지스틱 | 이진형 | 베르누이 | **Logit** g(p)=log(p/(1-p)) |
| 프로빗 | 이진형 | 베르누이 | **Probit** g(p)=Φ⁻¹(p) |
| 포아송 | 카운트 | 포아송 | **Log** g(λ)=log(λ) |

**로지스틱 회귀**:

**로짓 함수 (Logit Function)**:
```
logit(p) = log(p/(1-p)) = β₀ + β₁X

여기서:
- p: 성공 확률 (0~1)
- p/(1-p): 오즈 (Odds)
- log(p/(1-p)): 로그 오즈
```

**역함수 (시그모이드)**:
```
p = 1 / (1 + e^(-(β₀ + β₁X)))
```

**로짓의 특징**:
- 확률 (0~1)을 실수 (-∞~+∞)로 변환
- S자 곡선
- 해석: 오즈비(Odds Ratio)로 해석 용이

**프로빗 vs 로짓**:
- **로짓**: 로지스틱 분포, 꼬리 두꺼움
- **프로빗**: 표준정규분포, 꼬리 얇음
- 실무에서는 로짓이 더 일반적 (해석 용이)

---

## 문제 37
K-Means 군집분석에서 최적의 군집 수를 결정하는 방법이 아닌 것은?

① Elbow Method  
② Silhouette Score  
③ Gap Statistics  
④ p-value

**정답**: ④

**해설**:  
**K-Means 최적 군집 수 결정**:

**1. Elbow Method (팔꿈치 방법)**:

**원리**: WSS(Within-cluster Sum of Squares) 그래프에서 급격한 감소가 둔화되는 지점

```python
wss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    wss.append(kmeans.inertia_)

plt.plot(range(1, 11), wss)
plt.xlabel('Number of Clusters')
plt.ylabel('WSS')
# Elbow 지점 찾기
```

**특징**:
- 직관적
- 주관적 판단 필요
- 명확한 elbow 없을 수도

---

**2. Silhouette Score (실루엣 점수)**:

**정의**:
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))

a(i): 같은 군집 내 평균 거리
b(i): 가장 가까운 다른 군집까지 평균 거리

범위: -1 ~ 1
- 1에 가까울수록: 잘 분류됨
- 0 근처: 경계에 위치
- 음수: 잘못 분류됨
```

**사용법**:
```python
from sklearn.metrics import silhouette_score

scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    scores.append(score)

# 최대값을 가지는 k 선택
```

**특징**:
- 정량적 평가
- 계산 비용 높음 (O(n²))

---

**3. Gap Statistic**:

**원리**: 실제 데이터의 WSS와 랜덤 데이터의 WSS 비교

```
Gap(k) = E[log(WSS_random)] - log(WSS_actual)

- Gap이 큰 k 선택
- 랜덤 대비 얼마나 더 잘 군집화되는지
```

**특징**:
- 통계적 근거
- 계산 복잡도 높음
- 참조 분포 생성 필요

---

**4. 기타 방법**:

**Davies-Bouldin Index**:
```
DB = (1/k) Σ max((σᵢ + σⱼ) / d(cᵢ, cⱼ))

- 값이 작을수록 좋음
- 군집 내 산포도 vs 군집 간 거리
```

**Calinski-Harabasz Index**:
```
CH = (BSS/(k-1)) / (WSS/(n-k))

- 값이 클수록 좋음
- F-statistic과 유사
```

---

**p-value는 왜 안 되나?**

- K-Means는 **비지도학습**
- 가설검정이 아님
- 통계적 유의성 개념 적용 안 됨
- 군집은 "발견"이지 "검증"이 아님

p-value는 가설검정(지도학습)에서 사용하는 개념입니다.

---

## 문제 38
배깅(Bagging)과 부스팅(Boosting)의 차이점으로 가장 적절한 것은?

① 배깅은 순차적, 부스팅은 병렬적으로 학습한다  
② 배깅은 분산 감소, 부스팅은 편향 감소에 효과적이다  
③ 배깅은 가중치 조정, 부스팅은 복원추출을 사용한다  
④ 배깅만 의사결정나무를 사용할 수 있다

**정답**: ②

**해설**:  
**앙상블 학습 (Ensemble Learning)**:

여러 모델을 결합하여 성능 향상

---

**배깅 (Bagging: Bootstrap Aggregating)**:

**원리**:
1. **부트스트랩 샘플링** (복원추출)
   - 원본 데이터에서 중복 허용하여 샘플 생성
   - 각 모델이 서로 다른 샘플로 학습

2. **병렬 학습**
   - 독립적으로 모델 학습
   - 순서 상관없음

3. **결과 집계**
   - 분류: 다수결 투표 (Majority Voting)
   - 회귀: 평균

**효과**:
- **분산(Variance) 감소**
- 과적합 완화
- 안정성 향상

**대표 알고리즘**:
- **Random Forest**

---

**부스팅 (Boosting)**:

**원리**:
1. **순차적 학습**
   - 이전 모델의 오차를 보완
   - 잘못 예측한 샘플에 가중치 부여

2. **가중치 조정**
   - 어려운 샘플에 집중
   - 각 모델의 중요도 다름

3. **가중 결합**
   - 모델 성능에 따라 가중치 부여

**효과**:
- **편향(Bias) 감소**
- 예측 정확도 향상
- 과적합 위험 (주의 필요)

**대표 알고리즘**:
- **AdaBoost**: 가중치 조정
- **Gradient Boosting**: 잔차 학습
- **XGBoost**: 정규화 + 병렬 처리

---

**비교표**:

| 특성 | Bagging | Boosting |
|------|---------|----------|
| 학습 방식 | **병렬** | **순차** |
| 샘플링 | 복원추출 | 가중치 조정 |
| 모델 독립성 | 독립적 | 의존적 |
| 주요 효과 | **분산 ↓** | **편향 ↓** |
| 과적합 | 완화 | 위험 |
| 학습 속도 | 빠름 | 느림 |
| 대표 알고리즘 | Random Forest | XGBoost, AdaBoost |

---

**편향-분산 트레이드오프**:

```
Total Error = Bias² + Variance + Noise

Bias (편향):
- 모델의 단순화로 인한 오차
- 과소적합(Underfitting)
- 해결: 복잡한 모델, 부스팅

Variance (분산):
- 데이터 변화에 민감한 정도
- 과적합(Overfitting)
- 해결: 정규화, 배깅
```

---

**실무 선택**:

**배깅 (Random Forest) 사용**:
- 안정성 중요
- 병렬 처리 가능
- 해석보다 성능

**부스팅 (XGBoost) 사용**:
- 최고 성능 필요
- 캐글 대회
- 충분한 튜닝 시간

---

## 문제 39
교차검증(Cross-Validation)의 목적으로 가장 적절한 것은?

① 학습 시간 단축  
② 모델의 일반화 성능 평가  
③ 특성 중요도 계산  
④ 데이터 전처리

**정답**: ②

**해설**:  
**교차검증 (Cross-Validation)**:

**목적**:
1. **일반화 성능 평가**
   - 과적합 여부 확인
   - 실전 성능 추정

2. **데이터 효율적 활용**
   - 모든 데이터를 학습과 검증에 사용
   - 소규모 데이터셋에 특히 유용

3. **모델 비교**
   - 여러 모델 중 최선 선택
   - 하이퍼파라미터 튜닝

---

**주요 방법**:

**1. K-Fold Cross-Validation**:

```
데이터를 K개로 분할

Fold 1: [Test] [Train] [Train] [Train] [Train]
Fold 2: [Train] [Test] [Train] [Train] [Train]
Fold 3: [Train] [Train] [Test] [Train] [Train]
Fold 4: [Train] [Train] [Train] [Test] [Train]
Fold 5: [Train] [Train] [Train] [Train] [Test]

최종 성능 = 평균(Fold 1~5 성능)
```

**장점**:
- 모든 데이터 활용
- 안정적 평가

**단점**:
- 계산 비용 (K배)

**Python**:
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"평균: {scores.mean():.3f} (±{scores.std():.3f})")
```

---

**2. Stratified K-Fold**:

불균형 데이터에서 클래스 비율 유지

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5)
for train_idx, test_idx in skf.split(X, y):
    # 각 fold에 클래스 비율 동일
```

---

**3. Leave-One-Out (LOO)**:

K = n (데이터 개수)
- 극단적 K-Fold
- 소규모 데이터

**장점**: 최대 활용
**단점**: 매우 느림

---

**4. Time Series Split**:

시계열 데이터 전용

```
Split 1: [Train      ] [Test]
Split 2: [Train          ] [Test]
Split 3: [Train              ] [Test]

- 과거로 미래 예측
- 데이터 누수 방지
```

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
```

---

**주의사항**:

**데이터 누수(Data Leakage) 방지**:
```python
# ❌ 잘못된 방법
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # 전체 데이터로 fit
cross_val_score(model, X_scaled, y, cv=5)

# ✅ 올바른 방법
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
cross_val_score(pipe, X, y, cv=5)  # 각 fold마다 fit
```

---

**vs Hold-Out**:

| 방법 | 데이터 활용 | 안정성 | 계산 비용 |
|------|-------------|--------|-----------|
| Hold-Out | Train 70% + Test 30% | 낮음 | 낮음 |
| Cross-Validation | 전체 | 높음 | 높음 |

---

## 문제 40
SMOTE(Synthetic Minority Over-sampling Technique)에 대한 설명으로 가장 적절한 것은?

① 다수 클래스를 언더샘플링하는 기법이다  
② 소수 클래스의 합성 샘플을 생성한다  
③ 특성 선택 기법이다  
④ 차원 축소 기법이다

**정답**: ②

**해설**:  
**불균형 데이터 (Imbalanced Data)**:

클래스 비율이 심하게 치우친 경우
- 예: 사기 거래 (0.1%), 정상 거래 (99.9%)

**문제점**:
- 모델이 다수 클래스에 편향
- 소수 클래스 예측 실패
- 정확도는 높지만 재현율 낮음

---

**SMOTE (Synthetic Minority Over-sampling Technique)**:

**원리**:
소수 클래스의 **합성 샘플** 생성

**알고리즘**:
1. 소수 클래스 샘플 선택
2. K-최근접 이웃 찾기
3. 이웃과의 **중간 지점**에 새 샘플 생성

```
기존 샘플: A(2, 3)
이웃: B(4, 5)

합성 샘플 = A + λ(B - A)
여기서 λ는 0~1 사이 랜덤값

예: λ=0.5 → (3, 4)
```

**Python**:
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = smote.fit_resample(X, y)

# 결과: 소수 클래스 증가
print(Counter(y))          # {0: 900, 1: 100}
print(Counter(y_resampled)) # {0: 900, 1: 900}
```

---

**불균형 데이터 처리 방법**:

**1. 리샘플링 (Resampling)**:

**오버샘플링 (Over-sampling)**:
- 소수 클래스 증가
- **SMOTE**: 합성 생성
- **ADASYN**: 적응적 샘플링
- **Random Over-sampling**: 단순 복제

**언더샘플링 (Under-sampling)**:
- 다수 클래스 감소
- **Random Under-sampling**: 무작위 제거
- **Tomek Links**: 경계 샘플 제거
- **NearMiss**: 근접 샘플 제거

---

**2. 알고리즘 수준**:

**가중치 조정**:
```python
from sklearn.linear_model import LogisticRegression

# 클래스 가중치 자동 조정
model = LogisticRegression(class_weight='balanced')

# 또는 수동 설정
model = LogisticRegression(class_weight={0: 1, 1: 10})
```

**앙상블 기법**:
- **Balanced Random Forest**
- **EasyEnsemble**
- **BalancedBagging**

---

**3. 평가 지표 변경**:

**정확도 대신**:
- **F1-Score**
- **Precision-Recall AUC**
- **Matthews Correlation Coefficient (MCC)**

---

**SMOTE 변형**:

1. **Borderline-SMOTE**:
   - 경계 샘플만 오버샘플링
   - 효율적

2. **SMOTE-NC**:
   - 범주형 변수 포함

3. **ADASYN**:
   - 분류 어려운 샘플에 더 많은 합성

---

**주의사항**:

1. **과적합 위험**:
   - 너무 많은 합성 → 과적합
   - 적절한 비율 유지 (1:1 or 1:2)

2. **데이터 누수**:
   ```python
   # ❌ 잘못
   X_resampled, y_resampled = smote.fit_resample(X, y)
   X_train, X_test, y_train, y_test = train_test_split(...)
   
   # ✅ 올바름
   X_train, X_test, y_train, y_test = train_test_split(...)
   X_train, y_train = smote.fit_resample(X_train, y_train)
   # Test는 원본 유지
   ```

3. **테스트 데이터**:
   - 원본 분포 유지
   - SMOTE는 학습 데이터만

---

## 문제 41-70
(4과목 나머지 30문제 - 지면 관계상 목차만 제시)

41. 정규화 vs 표준화
42. ROC Curve와 AUC
43. Precision-Recall Trade-off
44. 신경망의 활성화 함수
45. Gradient Descent vs Stochastic Gradient Descent
46. Dropout vs Batch Normalization
47. CNN의 합성곱 연산
48. RNN과 LSTM
49. Decision Tree의 불순도 지표
50. Gini Index vs Entropy
51. 특성 중요도 (Feature Importance)
52. Forward Selection vs Backward Elimination
53. Ridge vs Lasso
54. Elastic Net
55. 시계열 분석의 정상성
56. ACF와 PACF
57. 협업 필터링 (Collaborative Filtering)
58. Content-based Filtering
59. K-NN 알고리즘
60. SVM의 Kernel Trick
61. Naive Bayes 가정
62. EM Algorithm
63. Apriori Algorithm
64. Lift와 Confidence
65. TF-IDF
66. Word2Vec
67. Attention Mechanism
68. 전이 학습 (Transfer Learning)
69. 강화학습 기본 개념
70. A/B 테스트 설계

---

# 5과목: 데이터 시각화

## 문제 71
데이터 시각화의 3요소에 해당하지 않는 것은?

① 데이터 (Data)  
② 스토리 (Story)  
③ 시각적 표현 (Visual)  
④ 알고리즘 (Algorithm)

**정답**: ④

**해설**:  
**데이터 시각화의 3요소**:

```
        스토리
         ↗  ↖
        /    \
    데이터 ←→ 시각적표현
```

**1. 데이터 (Data)**:
- 시각화의 기반
- 정확성, 신뢰성 필수
- 적절한 전처리

**2. 스토리 (Story)**:
- 전달하고자 하는 메시지
- 맥락과 인사이트
- 목적과 대상 고려

**3. 시각적 표현 (Visual)**:
- 차트, 그래프, 색상
- 디자인 원칙
- 인지적 효과

**효과적 시각화**:
```
좋은 시각화 = 데이터 × 스토리 × 시각적표현

- 데이터만: 원시 데이터
- 데이터 + 시각: 인포그래픽 (스토리 없음)
- 스토리 + 시각: 예술 (데이터 없음)
```

알고리즘은 데이터 분석 단계의 요소이지 시각화의 핵심 요소가 아닙니다.

---

## 문제 72
시계열 데이터 시각화에 가장 적합한 차트는?

① 산점도 (Scatter Plot)  
② 선 그래프 (Line Chart)  
③ 파이 차트 (Pie Chart)  
④ 박스플롯 (Box Plot)

**정답**: ②

**해설**:  
**차트 유형별 용도**:

**1. 선 그래프 (Line Chart)**:
- **시계열 데이터** 최적
- 추세 파악
- 여러 계열 비교

**사용 예시**:
- 주가 변동
- 매출 추이
- 온도 변화

```python
plt.plot(dates, values)
plt.xlabel('Date')
plt.ylabel('Value')
```

---

**2. 산점도 (Scatter Plot)**:
- **두 변수 관계**
- 상관관계 파악
- 이상치 발견

**사용 예시**:
- 키 vs 몸무게
- 광고비 vs 매출
- 집값 vs 면적

---

**3. 파이 차트 (Pie Chart)**:
- **비율/구성** (전체의 %)
- 범주 6개 이하
- 부분과 전체 관계

**주의**:
- 많은 범주: 막대 차트가 나음
- 비교 어려움
- 3D 파이: 왜곡

---

**4. 박스플롯 (Box Plot)**:
- **분포와 이상치**
- 사분위수 표시
- 그룹 간 비교

**구성요소**:
```
    최대값(whisker)
       ─┬─
        │
       ┌─┐  3사분위수(Q3)
       │ │
       │─│  중위수(median)
       │ │
       └─┘  1사분위수(Q1)
        │
       ─┴─  최소값(whisker)
      
      ×    이상치(outlier)
```

---

**기타 중요 차트**:

**5. 막대 차트 (Bar Chart)**:
- 범주형 데이터 비교
- 수직/수평

**6. 히스토그램 (Histogram)**:
- 연속형 변수 분포
- 빈도수

**7. 히트맵 (Heatmap)**:
- 상관관계 행렬
- 2차원 데이터 밀도

---

**차트 선택 가이드**:

| 목적 | 추천 차트 |
|------|----------|
| 시간 추이 | 선 그래프 |
| 비교 | 막대 차트 |
| 비율 | 파이 차트 |
| 분포 | 히스토그램, 박스플롯 |
| 관계 | 산점도 |
| 상관 | 히트맵 |
| 지리 | 지도 |

---

## 문제 73
색상 활용에 대한 설명으로 가장 적절하지 않은 것은?

① 연속형 변수는 그라디언트 색상을 사용한다  
② 범주형 변수는 구별되는 색상을 사용한다  
③ 색맹을 고려한 색상 선택이 필요하다  
④ 많은 색상을 사용할수록 효과적이다

**정답**: ④

**해설**:  
**색상 사용 원칙**:

**1. 목적에 맞는 색상 팔레트**:

**연속형 (Sequential)**:
```
낮음 ─────→ 높음
밝음        어두움

예: 온도, 매출액, 인구밀도
```
- 단일 색조 그라디언트
- 예: 흰색 → 파란색 → 진한파랑

**발산형 (Diverging)**:
```
음수 ←── 0 ──→ 양수
빨강       흰색    파랑

예: 증감률, 상관계수
```
- 양 끝 반대 색상
- 중간 중립 색상

**범주형 (Categorical)**:
```
A: 빨강
B: 파랑
C: 초록
D: 노랑

예: 제품군, 지역
```
- 구별되는 색상
- **6~8개 이하** 권장

---

**2. 색맹 고려 (Color Blindness)**:

**적록색맹 (가장 흔함)**:
- 빨강-초록 구별 어려움
- 파랑-노랑 조합 사용

**해결**:
```python
import seaborn as sns

# 색맹 친화적 팔레트
sns.set_palette("colorblind")

# 또는
plt.style.use('seaborn-colorblind')
```

**검증 도구**:
- Coblis (색맹 시뮬레이터)
- Color Oracle

---

**3. 색상 수 제한**:

**문제점**:
- 너무 많은 색상: 혼란
- 인지 부하 증가
- 구별 어려움

**권장**:
- 메인 색상: 2~3개
- 강조: 1개
- 보조: 회색 계열

**배색 원칙**:
```
70% - 기본 색상 (배경)
25% - 보조 색상 (차트)
5%  - 강조 색상 (하이라이트)
```

---

**4. 의미 있는 색상**:

**문화적 의미**:
- 빨강: 위험, 감소, 열정
- 초록: 안전, 증가, 성장
- 파랑: 신뢰, 차가움
- 노랑: 주의, 밝음

**일관성**:
- 같은 의미 = 같은 색상
- 예: 매출=파랑, 비용=빨강

---

**5. 명도 대비**:

**가독성**:
- 배경-전경 충분한 대비
- WCAG 기준: 최소 4.5:1

**예시**:
```
❌ 밝은회색 배경 + 노란글자
✅ 흰색 배경 + 검정글자
✅ 진한회색 배경 + 흰글자
```

---

**도구**:

1. **Color Brewer**: 지도 색상 팔레트
2. **Adobe Color**: 배색 조합 생성
3. **Coolors**: 팔레트 생성기

---

## 문제 74-80
(5과목 나머지 7문제 - 지면 관계상 목차만)

74. 대시보드 설계 원칙
75. 인터랙티브 시각화 vs 정적 시각화
76. D3.js vs Matplotlib
77. Tableau vs Power BI
78. 시각적 인코딩 (Visual Encoding)
79. 게슈탈트 원리 (Gestalt Principles)
80. 차트 왜곡 방지 (Misleading Charts)

---

# 서술형 문제 (20점)

## 문제 81 (서술형)

다음은 온라인 쇼핑몰의 고객 이탈 예측 모델을 개발한 결과입니다.

**[데이터 설명]**
- 종속변수: 이탈 여부 (0: 유지, 1: 이탈)
- 독립변수: 구매횟수, 평균구매금액, 최근구매일, 고객등급
- 데이터: 총 10,000명 (이탈 500명, 유지 9,500명)

**[모델 결과]**

**1. 로지스틱 회귀 결과**
```
Coefficients:
                    Estimate  Std.Error  z value  Pr(>|z|)  Odds Ratio
(Intercept)         2.5       0.3        8.33     <0.001    12.18
구매횟수            -0.15     0.02      -7.50     <0.001    0.86
평균구매금액        -0.0003   0.0001    -3.00     0.003     0.9997
최근구매일(일)       0.05     0.01       5.00     <0.001    1.05
고객등급(VIP)       -1.2      0.25      -4.80     <0.001    0.30

AIC: 2850
McFadden R²: 0.35
```

**2. Confusion Matrix (Test Set)**
```
              Predicted
              유지   이탈
Actual  유지  1850   50
        이탈  30     70
```

**[질문]**

**(1) 로지스틱 회귀 결과를 해석하시오. (8점)**
- 각 독립변수의 영향을 오즈비를 활용하여 설명
- 통계적 유의성 검정 결과 해석
- 모델 전체의 적합도 평가

**(2) Confusion Matrix를 바탕으로 다음을 계산하고 해석하시오. (6점)**
- 정확도(Accuracy)
- 정밀도(Precision)
- 재현율(Recall)
- F1-Score

**(3) 불균형 데이터 문제가 모델에 미치는 영향과 해결방안을 제시하시오. (6점)**
- 현재 모델의 문제점
- 리샘플링 기법 (SMOTE 등)
- 임계값 조정
- 평가지표 변경

---

## 서술형 모범답안

### (1) 로지스틱 회귀 결과 해석 (8점)

**각 독립변수의 영향**:

**① 구매횟수 (계수 = -0.15, OR = 0.86, p < 0.001)**
- 구매횟수가 1회 증가하면, 다른 조건이 동일할 때 **이탈 오즈가 0.86배**로 감소합니다.
- 즉, 구매횟수가 증가할수록 이탈 확률이 **14% 감소**합니다.
- p-value < 0.001로 통계적으로 **매우 유의**합니다.
- **해석**: 자주 구매하는 고객일수록 이탈 가능성이 낮습니다.

**② 평균구매금액 (계수 = -0.0003, OR = 0.9997, p = 0.003)**
- 평균구매금액이 1원 증가하면 이탈 오즈가 0.9997배로 감소합니다.
- 금액 단위가 작아 영향이 미미해 보이지만, 10,000원 증가 시:
  - OR = 0.9997^10000 ≈ 0.97 (약 3% 감소)
- p = 0.003 < 0.05로 통계적으로 **유의**합니다.
- **해석**: 고액 구매 고객일수록 이탈률이 낮습니다.

**③ 최근구매일 (계수 = 0.05, OR = 1.05, p < 0.001)**
- 최근 구매로부터 1일 경과할 때마다 이탈 오즈가 **1.05배 증가**합니다.
- 30일 경과 시: OR = 1.05^30 ≈ 4.32 (약 332% 증가)
- p < 0.001로 **매우 유의**합니다.
- **해석**: 마지막 구매가 오래될수록 이탈 위험이 급격히 증가합니다.

**④ 고객등급-VIP (계수 = -1.2, OR = 0.30, p < 0.001)**
- VIP 고객은 일반 고객 대비 이탈 오즈가 **0.30배** (70% 감소)입니다.
- p < 0.001로 **매우 유의**합니다.
- **해석**: VIP 등급 고객의 충성도가 매우 높습니다.

---

**통계적 유의성 검정**:

모든 독립변수의 p-value < 0.05로 **통계적으로 유의**합니다.
- 귀무가설 (H₀: β = 0)을 기각
- 각 변수가 이탈 예측에 의미 있는 영향을 미침

---

**모델 적합도 평가**:

**McFadden R² = 0.35**:
- 0.2~0.4 범위로 **양호한 적합도**
- 로지스틱 회귀에서 0.2 이상이면 좋은 모델로 평가
- 모형이 이탈 변동의 35%를 설명

**AIC = 2850**:
- 절대값 자체보다는 **모델 비교**에 활용
- 다른 모델과 비교 시 AIC가 낮을수록 우수

**결론**:
이 모델은 통계적으로 유의하며 적합도가 양호합니다. 4개 변수 모두 이탈 예측에 유의미하게 기여하며, 특히 최근구매일과 VIP 등급이 가장 큰 영향을 미칩니다.

---

### (2) 평가지표 계산 및 해석 (6점)

**혼동행렬**:
```
              Predicted
              유지(0)  이탈(1)
Actual  유지  TN=1850  FP=50
        이탈  FN=30    TP=70

TP = 70 (True Positive)
TN = 1850 (True Negative)
FP = 50 (False Positive)
FN = 30 (False Negative)
Total = 2000
```

---

**① 정확도 (Accuracy)**:
```
Accuracy = (TP + TN) / Total
         = (70 + 1850) / 2000
         = 1920 / 2000
         = 0.96 (96%)
```

**해석**:
전체 예측 중 96%가 정확합니다. 하지만 **불균형 데이터**에서는 정확도가 높아도 소수 클래스(이탈) 예측이 부정확할 수 있어 주의가 필요합니다.

---

**② 정밀도 (Precision)**:
```
Precision = TP / (TP + FP)
          = 70 / (70 + 50)
          = 70 / 120
          = 0.583 (58.3%)
```

**해석**:
이탈로 예측한 고객 중 실제 이탈 고객은 58.3%입니다. **거짓 양성**(FP)이 많아 정밀도가 낮습니다. 이탈 캠페인 시 비용 낭비가 발생할 수 있습니다.

---

**③ 재현율 (Recall, Sensitivity)**:
```
Recall = TP / (TP + FN)
       = 70 / (70 + 30)
       = 70 / 100
       = 0.70 (70%)
```

**해석**:
실제 이탈 고객 중 70%를 탐지했습니다. 30%의 이탈 고객을 놓쳤으므로(**거짓 음성**), 이탈 방지 기회를 상실했습니다.

---

**④ F1-Score**:
```
F1-Score = 2 × (Precision × Recall) / (Precision + Recall)
         = 2 × (0.583 × 0.70) / (0.583 + 0.70)
         = 2 × 0.408 / 1.283
         = 0.636 (63.6%)
```

**해석**:
정밀도와 재현율의 조화평균으로, **불균형 데이터**에서 유용한 지표입니다. 63.6%는 개선 여지가 있습니다.

---

**종합 평가**:

| 지표 | 값 | 평가 |
|------|-----|------|
| 정확도 | 96% | 높음 (하지만 불균형으로 인한 착시) |
| 정밀도 | 58.3% | 낮음 (FP 많음) |
| 재현율 | 70% | 보통 (FN 30% 놓침) |
| F1-Score | 63.6% | 개선 필요 |

**문제점**:
- 재현율 70%: 이탈 고객의 30%를 놓침
- 정밀도 58%: 이탈 아닌 고객을 이탈로 잘못 예측

**비즈니스 영향**:
- 놓친 이탈 고객(FN=30): 매출 손실
- 잘못 타겟팅(FP=50): 마케팅 비용 낭비

---

### (3) 불균형 데이터 문제 및 해결방안 (6점)

**현재 모델의 문제점**:

**① 클래스 불균형**:
```
유지: 9,500명 (95%)
이탈: 500명 (5%)

비율 = 19:1
```

**② 편향된 학습**:
- 모델이 다수 클래스(유지)에 편향
- "모두 유지로 예측"해도 정확도 95%
- 소수 클래스(이탈) 학습 부족

**③ 낮은 재현율**:
- 이탈 고객의 30%를 놓침 (FN=30)
- 비즈니스 목표(이탈 방지)에 부합하지 않음

---

**해결방안**:

**1. 리샘플링 기법**:

**A. SMOTE (Synthetic Minority Over-sampling)**:
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy=0.5)  # 소수:다수 = 1:2
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

**장점**:
- 소수 클래스 합성 샘플 생성
- 정보 손실 없음

**주의**:
- 학습 데이터만 적용
- 테스트 데이터는 원본 유지

---

**B. 언더샘플링 + 오버샘플링 결합**:
```python
from imblearn.combine import SMOTETomek

smt = SMOTETomek(sampling_strategy='auto')
X_resampled, y_resampled = smt.fit_resample(X_train, y_train)
```

---

**2. 클래스 가중치 조정**:

```python
from sklearn.linear_model import LogisticRegression

# 자동 가중치
model = LogisticRegression(class_weight='balanced')

# 또는 수동 설정 (이탈에 19배 가중치)
model = LogisticRegression(class_weight={0: 1, 1: 19})
```

**효과**:
- 소수 클래스 오분류 페널티 증가
- 재현율 향상

---

**3. 임계값(Threshold) 조정**:

**기본 임계값**: 0.5
```
P(이탈) >= 0.5 → 이탈로 분류
```

**조정 방법**:
```python
from sklearn.metrics import precision_recall_curve

# 최적 임계값 찾기
precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)

# 재현율 80% 달성하는 임계값 선택
optimal_threshold = thresholds[recalls >= 0.8][0]

# 예: 0.3으로 낮춤
y_pred_adjusted = (y_prob >= 0.3).astype(int)
```

**효과**:
```
임계값 0.5 → 0.3:
- 재현율: 70% → 85% (↑)
- 정밀도: 58% → 45% (↓)

Trade-off 존재
```

**비즈니스 판단**:
- **이탈 방지 중요**: 임계값 낮춤 (재현율 ↑)
- **마케팅 비용 중요**: 임계값 높임 (정밀도 ↑)

---

**4. 평가지표 변경**:

**정확도 대신**:

**A. F1-Score**:
- 정밀도와 재현율 균형

**B. F2-Score (β=2)**:
```
F2 = (1+2²) × (Precision × Recall) / (2² × Precision + Recall)
   = 5 × P × R / (4P + R)
```
- 재현율에 2배 가중치
- 이탈 탐지 중요 시 사용

**C. PR-AUC (Precision-Recall AUC)**:
- 불균형 데이터에 적합
- ROC-AUC보다 민감

**D. Matthews Correlation Coefficient (MCC)**:
```
MCC = (TP×TN - FP×FN) / √((TP+FP)(TP+FN)(TN+FP)(TN+FN))

범위: -1 ~ 1
- 1: 완벽
- 0: 랜덤
- -1: 완전 불일치
```
- 불균형 데이터에서도 신뢰도 높음

---

**5. 앙상블 기법**:

**Balanced Random Forest**:
```python
from imblearn.ensemble import BalancedRandomForestClassifier

brf = BalancedRandomForestClassifier(
    sampling_strategy='auto',
    replacement=True
)
```

**Cost-Sensitive Learning**:
- 오분류 비용 고려
- 이탈 놓침(FN) 비용 > 잘못 예측(FP) 비용

---

**6. 이상치 탐지 기법**:

소수 클래스를 "이상치"로 간주:
- **Isolation Forest**
- **One-Class SVM**
- **Autoencoder**

---

**종합 추천**:

**단계별 접근**:

1. **데이터 수준**:
   - SMOTE로 학습 데이터 균형화
   - 비율: 1:2 또는 1:3 (완전 균형 불필요)

2. **모델 수준**:
   - class_weight='balanced' 설정
   - 앙상블 모델 (Random Forest, XGBoost)

3. **평가 수준**:
   - F2-Score 또는 PR-AUC 사용
   - 비즈니스 목표에 맞는 임계값 설정

4. **모니터링**:
   - A/B 테스트로 실전 성능 검증
   - 정기적 재학습 (데이터 드리프트 대응)

**예상 효과**:
```
현재:
- 재현율: 70%
- 정밀도: 58%

개선 후:
- 재현율: 85% (이탈 고객 85% 탐지)
- 정밀도: 50% (일부 하락 허용)
- F2-Score: 0.75 (개선)
```

**비즈니스 가치**:
- 이탈 방지율 15% 향상
- 고객 생애 가치(LTV) 증대
- 마케팅 ROI 개선

---

## 채점 기준

**총점: 20점**

**(1) 로지스틱 회귀 해석 (8점)**:
- 오즈비 해석: 3점
- 통계적 유의성: 2점
- 모델 적합도: 2점
- 비즈니스 해석: 1점

**(2) 평가지표 계산 (6점)**:
- 계산 정확성: 3점 (각 0.75점)
- 해석 및 의미: 2점
- 문제점 파악: 1점

**(3) 해결방안 (6점)**:
- 문제점 인식: 1점
- 리샘플링 기법: 2점
- 임계값/가중치 조정: 1.5점
- 평가지표 변경: 1.5점

---

# 정답 및 해설 요약

## 과목별 출제 비중
- 1과목 (데이터의 이해): 10문제
- 2과목 (데이터 처리 기술): 10문제
- 3과목 (데이터 분석 기획): 10문제
- 4과목 (데이터 분석): 40문제
- 5과목 (데이터 시각화): 10문제
- 서술형: 1문제 (20점)

## 핵심 출제 주제

### 통계/분석 이론
- 가설검정, 회귀분석, 주성분분석
- 중심극한정리, 제1·2종 오류
- 비모수 검정, Durbin-Watson

### 머신러닝
- 분류/회귀 알고리즘
- 앙상블 (배깅/부스팅)
- 과적합 해결, 교차검증
- 불균형 데이터 (SMOTE)

### 평가지표
- Accuracy, Precision, Recall, F1-Score
- ROC-AUC, Silhouette Score
- 혼동행렬 해석

### 빅데이터 기술
- Hadoop (HDFS, MapReduce)
- Spark, NoSQL
- ETL/ELT, 데이터 파이프라인

### 데이터 관리
- 데이터 거버넌스, MDM
- 데이터 품질, 정규화
- 개인정보 비식별화

---

# 📚 학습 가이드

## 과목별 중요도

| 과목 | 배점 | 중요도 | 학습 시간 |
|------|------|--------|-----------|
| 4과목 (데이터 분석) | 40점 | ★★★★★ | 50% |
| 서술형 | 20점 | ★★★★★ | 20% |
| 2과목 (처리 기술) | 10점 | ★★★☆☆ | 10% |
| 3과목 (분석 기획) | 10점 | ★★★☆☆ | 10% |
| 1과목 (데이터의 이해) | 10점 | ★★☆☆☆ | 5% |
| 5과목 (시각화) | 10점 | ★★☆☆☆ | 5% |

## 학습 전략

### 4과목 집중
- 회귀분석, 가설검정 완벽 이해
- 머신러닝 알고리즘 원리와 특징
- 평가지표 계산 연습

### 서술형 대비
- 회귀 결과 해석 템플릿 암기
- 평가지표 공식 암기
- 과적합 해결방안 정리

### 실전 연습
- 기출문제 3회분 이상 풀이
- 시간 배분 연습 (객관식 120분, 서술형 60분)
- 서술형 직접 작성 연습

---

**작성: ADP 수험 전문가 팀**  
**문의: adp.study@example.com**  
**최종 수정: 2026-02-06**