# êµ°ì§‘ë¶„ì„ ì™„ì „ ì •ë¦¬
## Clustering Analysis & Validity Evaluation

---

## ğŸ“Š êµ°ì§‘ë¶„ì„ ê°œìš”

### ğŸ“Œ ì •ì˜
**êµ°ì§‘ë¶„ì„(Clustering)**ì€ ë¹„ìŠ·í•œ íŠ¹ì„±ì„ ê°€ì§„ ê°œì²´ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ëŠ” **ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning)** ê¸°ë²•ì…ë‹ˆë‹¤.

### ğŸ¯ ëª©ì 
- ë°ì´í„°ì˜ ë‚´ì¬ëœ êµ¬ì¡° íŒŒì•…
- ìœ ì‚¬í•œ ê°ì²´ë“¤ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜
- íŒ¨í„´ ë°œê²¬ ë° ë°ì´í„° ì••ì¶•
- ì´ìƒì¹˜ íƒì§€

### ğŸ” íŠ¹ì§•
| íŠ¹ì„± | ì„¤ëª… |
|------|------|
| **í•™ìŠµ ë°©ì‹** | ë¹„ì§€ë„ í•™ìŠµ (ì •ë‹µ ë ˆì´ë¸” ì—†ìŒ) |
| **ëª©í‘œ** | êµ°ì§‘ ë‚´ ìœ ì‚¬ë„ ìµœëŒ€í™”, êµ°ì§‘ ê°„ ìœ ì‚¬ë„ ìµœì†Œí™” |
| **ê²°ê³¼** | ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ êµ°ì§‘ ë ˆì´ë¸” |
| **í‰ê°€** | ë‚´ë¶€ íƒ€ë‹¹ì„± ì§€í‘œ ì‚¬ìš© (ì •ë‹µ ì—†ìŒ) |

---

## ğŸ“š ì£¼ìš” êµ°ì§‘ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | ìœ í˜• | êµ°ì§‘ í˜•íƒœ | êµ°ì§‘ ìˆ˜ | ì¥ì  | ë‹¨ì  |
|---------|------|----------|---------|------|------|
| **K-Means** | ë¶„í•  ê¸°ë°˜ | êµ¬í˜• | ì‚¬ì „ ì§€ì • í•„ìš” | ë¹ ë¥´ê³  ê°„ë‹¨, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì í•© | ì´ìƒì¹˜ ë¯¼ê°, êµ¬í˜• êµ°ì§‘ë§Œ ê°€ëŠ¥ |
| **ê³„ì¸µì  êµ°ì§‘** | ê³„ì¸µ ê¸°ë°˜ | ë‹¤ì–‘í•¨ | ìë™ ê²°ì • ê°€ëŠ¥ | ë´ë“œë¡œê·¸ë¨ ì œê³µ, êµ°ì§‘ ìˆ˜ ìœ ì—° | ëŠë¦¼, ëŒ€ìš©ëŸ‰ ë¶€ì í•© |
| **DBSCAN** | ë°€ë„ ê¸°ë°˜ | ì„ì˜ í˜•íƒœ | ìë™ ê²°ì • | ì„ì˜ í˜•íƒœ, ì´ìƒì¹˜ íƒì§€ | íŒŒë¼ë¯¸í„° ë¯¼ê°, ë°€ë„ ì°¨ì´ ì–´ë ¤ì›€ |
| **GMM** | í™•ë¥  ê¸°ë°˜ | íƒ€ì›í˜• | ì‚¬ì „ ì§€ì • í•„ìš” | í™•ë¥ ì  í• ë‹¹, ë¶€ë“œëŸ¬ìš´ ê²½ê³„ | ëŠë¦¼, ì´ˆê¸°ê°’ ë¯¼ê° |

---

## 1ï¸âƒ£ K-Means êµ°ì§‘ë¶„ì„

### ğŸ“Œ ì•Œê³ ë¦¬ì¦˜ ê³¼ì •

```
1. kê°œì˜ ì´ˆê¸° ì¤‘ì‹¬ì (centroid) ëœë¤ ì„ íƒ
2. ê° ë°ì´í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
3. ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì ì„ ë‹¤ì‹œ ê³„ì‚° (í‰ê· )
4. 2-3ë‹¨ê³„ë¥¼ ì¤‘ì‹¬ì ì´ ë³€í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µ
```

### ğŸ“ ëª©ì  í•¨ìˆ˜

```
J = Î£(i=1 to k) Î£(xâˆˆCi) ||x - Î¼i||Â²

ìµœì†Œí™” ëª©í‘œ:
- êµ°ì§‘ ë‚´ ë¶„ì‚°(Within-Cluster Sum of Squares, WCSS)
- ê° ì ê³¼ ì¤‘ì‹¬ì  ê°„ì˜ ê±°ë¦¬ ì œê³±í•©

Î¼i: ië²ˆì§¸ êµ°ì§‘ì˜ ì¤‘ì‹¬ì 
Ci: ië²ˆì§¸ êµ°ì§‘
```

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
X, y_true = make_blobs(n_samples=300, centers=4, 
                       cluster_std=0.60, random_state=42)

# K-Means ì ìš©
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

# ê²°ê³¼ í™•ì¸
print("êµ°ì§‘ ì¤‘ì‹¬ì :")
print(kmeans.cluster_centers_)
print(f"\nInertia (WCSS): {kmeans.inertia_:.2f}")

# ì‹œê°í™”
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis')
plt.title('True Labels')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], 
           kmeans.cluster_centers_[:, 1],
           marker='X', s=200, c='red', edgecolors='black')
plt.title('K-Means Clustering')
plt.show()
```

### ğŸ¯ ìµœì  k ê²°ì •: Elbow Method

```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# ë‹¤ì–‘í•œ kê°’ì— ëŒ€í•´ WCSS ê³„ì‚°
wcss = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Elbow ê·¸ë˜í”„
plt.figure(figsize=(8, 5))
plt.plot(K_range, wcss, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (k)', fontsize=12)
plt.ylabel('WCSS (Inertia)', fontsize=12)
plt.title('Elbow Method For Optimal k', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 2ï¸âƒ£ ê³„ì¸µì  êµ°ì§‘ë¶„ì„ (Hierarchical Clustering)

### ğŸ“Œ ìœ í˜•

#### 1. **ì‘ì§‘í˜• (Agglomerative)** - Bottom-up
- ê° ë°ì´í„°ë¥¼ ê°œë³„ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘
- ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©
- ì „ì²´ê°€ í•˜ë‚˜ì˜ êµ°ì§‘ì´ ë  ë•Œê¹Œì§€ ì§„í–‰

#### 2. **ë¶„í• í˜• (Divisive)** - Top-down
- ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘
- ê°€ì¥ ì´ì§ˆì ì¸ êµ°ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ë¶„í• 

### ğŸ“ ê±°ë¦¬ ì¸¡ì • ë°©ë²•

| ì—°ê²° ë°©ë²• | ì„¤ëª… | íŠ¹ì§• |
|----------|------|------|
| **Single Linkage** | ë‘ êµ°ì§‘ì˜ ìµœë‹¨ ê±°ë¦¬ | min(d(a,b)) | ê¸´ ì²´ì¸ í˜•íƒœ, ì´ìƒì¹˜ ë¯¼ê° |
| **Complete Linkage** | ë‘ êµ°ì§‘ì˜ ìµœì¥ ê±°ë¦¬ | max(d(a,b)) | êµ¬í˜• êµ°ì§‘, ì´ìƒì¹˜ ë‘”ê° |
| **Average Linkage** | ë‘ êµ°ì§‘ì˜ í‰ê·  ê±°ë¦¬ | mean(d(a,b)) | ê· í˜•ì¡íŒ ê²°ê³¼ |
| **Ward's Method** | ë¶„ì‚° ì¦ê°€ ìµœì†Œí™” | ESS ê¸°ì¤€ | K-Meansì™€ ìœ ì‚¬, êµ¬í˜• ì„ í˜¸ |

### ğŸ’» Python ì˜ˆì œ

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# ê³„ì¸µì  êµ°ì§‘ ìˆ˜í–‰
linkage_matrix = linkage(X, method='ward')

# ë´ë“œë¡œê·¸ë¨ ê·¸ë¦¬ê¸°
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, truncate_mode='lastp', p=30)
plt.title('Hierarchical Clustering Dendrogram', fontsize=14, fontweight='bold')
plt.xlabel('Sample Index or Cluster Size')
plt.ylabel('Distance')
plt.axhline(y=10, color='r', linestyle='--', label='Cut-off')
plt.legend()
plt.show()

# AgglomerativeClustering ì‚¬ìš©
agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')
y_agg = agg_clustering.fit_predict(X)

print(f"Labels: {y_agg}")
```

---

## 3ï¸âƒ£ DBSCAN (Density-Based Spatial Clustering)

### ğŸ“Œ í•µì‹¬ ê°œë…

- **Epsilon (Îµ)**: ì´ì›ƒ ë°˜ê²½
- **MinPts**: í•µì‹¬ í¬ì¸íŠ¸ê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œ ì´ì›ƒ ìˆ˜
- **í•µì‹¬ í¬ì¸íŠ¸**: Îµ ë‚´ì— MinPtsê°œ ì´ìƒì˜ ì´ì›ƒì„ ê°€ì§„ ì 
- **ê²½ê³„ í¬ì¸íŠ¸**: í•µì‹¬ í¬ì¸íŠ¸ì˜ ì´ì›ƒì´ì§€ë§Œ ìì‹ ì€ í•µì‹¬ì´ ì•„ë‹Œ ì 
- **ì¡ìŒ í¬ì¸íŠ¸**: ì–´ë–¤ êµ°ì§‘ì—ë„ ì†í•˜ì§€ ì•ŠëŠ” ì  (ì´ìƒì¹˜)

### ğŸ¯ ì¥ì 
- êµ°ì§‘ ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•  í•„ìš” ì—†ìŒ
- ì„ì˜ í˜•íƒœì˜ êµ°ì§‘ ë°œê²¬ ê°€ëŠ¥
- ì´ìƒì¹˜ë¥¼ ìë™ìœ¼ë¡œ íƒì§€

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# ì´ˆìŠ¹ë‹¬ ëª¨ì–‘ ë°ì´í„°
X_moon, _ = make_moons(n_samples=200, noise=0.05, random_state=42)

# DBSCAN ì ìš©
dbscan = DBSCAN(eps=0.3, min_samples=5)
y_dbscan = dbscan.fit_predict(X_moon)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_moon[:, 0], X_moon[:, 1])
plt.title('Original Data')

plt.subplot(1, 2, 2)
plt.scatter(X_moon[:, 0], X_moon[:, 1], c=y_dbscan, cmap='viridis')
plt.title(f'DBSCAN (eps={0.3}, min_samples={5})')
plt.show()

print(f"Number of clusters: {len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)}")
print(f"Number of noise points: {list(y_dbscan).count(-1)}")
```

---

## ğŸ¯ êµ°ì§‘ íƒ€ë‹¹ì„± í‰ê°€ ì§€í‘œ

### ğŸ“Š í‰ê°€ ì§€í‘œ ë¶„ë¥˜

| ìœ í˜• | ì§€í‘œ | íŠ¹ì§• | ì‚¬ìš© ì‹œê¸° |
|------|------|------|----------|
| **ë‚´ë¶€ ì§€í‘œ** | Silhouette, Calinski-Harabasz, Davies-Bouldin, Dunn Index | ë°ì´í„°ë§Œìœ¼ë¡œ í‰ê°€ | ì •ë‹µ ë ˆì´ë¸” ì—†ìŒ |
| **ì™¸ë¶€ ì§€í‘œ** | ARI, NMI, Purity, F-measure | ì •ë‹µê³¼ ë¹„êµ | ì •ë‹µ ë ˆì´ë¸” ìˆìŒ |
| **ìƒëŒ€ ì§€í‘œ** | Elbow Method, Gap Statistic | ì—¬ëŸ¬ k ë¹„êµ | ìµœì  k íƒìƒ‰ |

---

## 4ï¸âƒ£ ì‹¤ë£¨ì—£ ê³„ìˆ˜ (Silhouette Coefficient)

### ğŸ“Œ ì •ì˜
ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ìì‹ ì˜ êµ°ì§‘ì— ì–¼ë§ˆë‚˜ ì˜ ì†í•´ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

### ğŸ“ ìˆ˜ì‹

```
s(i) = (b(i) - a(i)) / max(a(i), b(i))

ì—¬ê¸°ì„œ:
a(i): ê°™ì€ êµ°ì§‘ ë‚´ ë‹¤ë¥¸ ì ë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬ (ì‘ì§‘ë„)
b(i): ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ êµ°ì§‘ì˜ ì ë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬ (ë¶„ë¦¬ë„)

ë²”ìœ„: -1 â‰¤ s(i) â‰¤ 1
```

### ğŸ” í•´ì„

| ê°’ ë²”ìœ„ | ì˜ë¯¸ | í•´ì„ |
|---------|------|------|
| **0.7 ~ 1.0** | ë§¤ìš° ì¢‹ìŒ | ëª…í™•í•œ êµ°ì§‘ êµ¬ì¡° |
| **0.5 ~ 0.7** | ì ì ˆí•¨ | í•©ë¦¬ì ì¸ êµ°ì§‘ êµ¬ì¡° |
| **0.25 ~ 0.5** | ì•½í•¨ | êµ°ì§‘ì´ ì•½ê°„ ê²¹ì¹¨ |
| **< 0.25** | êµ¬ì¡° ì—†ìŒ | ì¸ìœ„ì ì¸ êµ°ì§‘ |
| **ìŒìˆ˜** | ì˜ëª»ëœ êµ°ì§‘ | ë‹¤ë¥¸ êµ°ì§‘ì— ë” ê°€ê¹Œì›€ |

### ğŸ’¡ ê³„ì‚° ì˜ˆì œ

```
êµ°ì§‘ 1: {A, B, C}
êµ°ì§‘ 2: {D, E}

ì  Aì— ëŒ€í•´:
a(A) = (d(A,B) + d(A,C)) / 2  # ê°™ì€ êµ°ì§‘
b(A) = (d(A,D) + d(A,E)) / 2  # ë‹¤ë¥¸ êµ°ì§‘

s(A) = (b(A) - a(A)) / max(a(A), b(A))
```

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import numpy as np

# K-Means êµ°ì§‘í™”
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

# ì „ì²´ ì‹¤ë£¨ì—£ ì ìˆ˜
silhouette_avg = silhouette_score(X, y_pred)
print(f"í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")

# ê° ìƒ˜í”Œì˜ ì‹¤ë£¨ì—£ ì ìˆ˜
sample_silhouette_values = silhouette_samples(X, y_pred)

# ì‹¤ë£¨ì—£ ë‹¤ì´ì–´ê·¸ë¨
fig, ax = plt.subplots(figsize=(10, 6))
y_lower = 10

for i in range(4):
    # ië²ˆì§¸ êµ°ì§‘ì˜ ì‹¤ë£¨ì—£ ê°’ë“¤
    ith_cluster_silhouette_values = sample_silhouette_values[y_pred == i]
    ith_cluster_silhouette_values.sort()
    
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = plt.cm.nipy_spectral(float(i) / 4)
    ax.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)
    
    # êµ°ì§‘ ë ˆì´ë¸”
    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

ax.set_title('Silhouette Plot', fontsize=14, fontweight='bold')
ax.set_xlabel('Silhouette Coefficient')
ax.set_ylabel('Cluster Label')
ax.axvline(x=silhouette_avg, color="red", linestyle="--", 
           label=f'Average: {silhouette_avg:.3f}')
ax.legend()
plt.show()
```

### ğŸ¯ ìµœì  k ì°¾ê¸°

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    score = silhouette_score(X, y_pred)
    silhouette_scores.append(score)
    print(f"k={k}: Silhouette Score = {score:.3f}")

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score by k')
plt.grid(True, alpha=0.3)
plt.show()

# ìµœì  k
optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"\nìµœì  êµ°ì§‘ ìˆ˜: {optimal_k}")
```

---

## 5ï¸âƒ£ ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¸  ì§€ìˆ˜ (Calinski-Harabasz Index)

### ğŸ“Œ ì •ì˜
**ë¶„ì‚°ë¹„ ê¸°ì¤€(Variance Ratio Criterion)**ì´ë¼ê³ ë„ í•˜ë©°, êµ°ì§‘ ê°„ ë¶„ì‚°ê³¼ êµ°ì§‘ ë‚´ ë¶„ì‚°ì˜ ë¹„ìœ¨ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

### ğŸ“ ìˆ˜ì‹

```
CH = (SSB / (k-1)) / (SSW / (n-k))

ì—¬ê¸°ì„œ:
SSB (Between-cluster sum of squares): êµ°ì§‘ ê°„ ì œê³±í•©
SSW (Within-cluster sum of squares): êµ°ì§‘ ë‚´ ì œê³±í•©
k: êµ°ì§‘ ìˆ˜
n: ì „ì²´ ë°ì´í„° ìˆ˜

SSB = Î£(i=1 to k) ni Ã— ||ci - c||Â²
SSW = Î£(i=1 to k) Î£(xâˆˆCi) ||x - ci||Â²

ni: ië²ˆì§¸ êµ°ì§‘ì˜ ë°ì´í„° ìˆ˜
ci: ië²ˆì§¸ êµ°ì§‘ì˜ ì¤‘ì‹¬
c: ì „ì²´ ë°ì´í„°ì˜ ì¤‘ì‹¬
```

### ğŸ” í•´ì„

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ë²”ìœ„** | 0 ~ âˆ (í´ìˆ˜ë¡ ì¢‹ìŒ) |
| **ì˜ë¯¸** | êµ°ì§‘ì´ ì¡°ë°€í•˜ê³  ì˜ ë¶„ë¦¬ë ìˆ˜ë¡ ë†’ì€ ê°’ |
| **ìµœì í™”** | CH ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” k ì„ íƒ |
| **ì¥ì ** | ê³„ì‚°ì´ ë¹ ë¥´ê³  ì§ê´€ì  |
| **ë‹¨ì ** | êµ¬í˜• êµ°ì§‘ì— ìœ ë¦¬ |

### ğŸ’¡ F-í†µê³„ëŸ‰ê³¼ì˜ ê´€ê³„

ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¸  ì§€ìˆ˜ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ **ì¼ì›ë¶„ì‚°ë¶„ì„(ANOVA)ì˜ F-í†µê³„ëŸ‰**ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.
- ë†’ì€ CH = êµ°ì§‘ ê°„ ì°¨ì´ê°€ êµ°ì§‘ ë‚´ ì°¨ì´ë³´ë‹¤ í›¨ì”¬ í¼

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.metrics import calinski_harabasz_score

# K-Means ì ìš©
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

# CH ì§€ìˆ˜ ê³„ì‚°
ch_score = calinski_harabasz_score(X, y_pred)
print(f"Calinski-Harabasz Index: {ch_score:.2f}")

# ë‹¤ì–‘í•œ kì— ëŒ€í•œ CH ì ìˆ˜
ch_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    ch_score = calinski_harabasz_score(X, y_pred)
    ch_scores.append(ch_score)
    print(f"k={k}: CH Index = {ch_score:.2f}")

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(K_range, ch_scores, 'go-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Calinski-Harabasz Index')
plt.title('Calinski-Harabasz Index by k')
plt.grid(True, alpha=0.3)
plt.show()

optimal_k = K_range[np.argmax(ch_scores)]
print(f"\nìµœì  êµ°ì§‘ ìˆ˜: {optimal_k}")
```

---

## 6ï¸âƒ£ ë˜ ì§€ìˆ˜ (Dunn Index)

### ğŸ“Œ ì •ì˜
êµ°ì§‘ì˜ **ì»´íŒ©íŠ¸ì„±(compactness)**ê³¼ **ë¶„ë¦¬ë„(separation)**ë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

### ğŸ“ ìˆ˜ì‹

```
DI = min(i=1 to k) min(j=i+1 to k) Î´(Ci, Cj) / max(l=1 to k) Î”(Cl)

ì—¬ê¸°ì„œ:
Î´(Ci, Cj): êµ°ì§‘ iì™€ j ì‚¬ì´ì˜ ìµœì†Œ ê±°ë¦¬ (êµ°ì§‘ ê°„ ë¶„ë¦¬ë„)
Î”(Cl): êµ°ì§‘ lì˜ ìµœëŒ€ ì§ê²½ (êµ°ì§‘ ë‚´ ìµœëŒ€ ê±°ë¦¬)

Î´(Ci, Cj) = min{d(x, y) : xâˆˆCi, yâˆˆCj}  # ê°€ì¥ ê°€ê¹Œìš´ ì  ê°„ ê±°ë¦¬
Î”(Cl) = max{d(x, y) : x,yâˆˆCl}          # ê°€ì¥ ë¨¼ ì  ê°„ ê±°ë¦¬
```

### ğŸ” í•´ì„

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ë²”ìœ„** | 0 ~ âˆ (í´ìˆ˜ë¡ ì¢‹ìŒ) |
| **ë¶„ì** | êµ°ì§‘ ê°„ ìµœì†Œ ê±°ë¦¬ (í´ìˆ˜ë¡ ì¢‹ìŒ) |
| **ë¶„ëª¨** | êµ°ì§‘ ë‚´ ìµœëŒ€ ì§ê²½ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ) |
| **ì˜ë¯¸** | êµ°ì§‘ì´ ì¡°ë°€í•˜ê³  ì˜ ë¶„ë¦¬ë ìˆ˜ë¡ ë†’ì€ ê°’ |
| **ì¥ì ** | êµ°ì§‘ì˜ ì§ˆì„ ì§ê´€ì ìœ¼ë¡œ í‰ê°€ |
| **ë‹¨ì ** | ê³„ì‚° ë³µì¡ë„ ë†’ìŒ O(nÂ²) |

### ğŸ’¡ ì§ê´€ì  ì´í•´

```
ì¢‹ì€ êµ°ì§‘:
- êµ°ì§‘ ê°„ ê±°ë¦¬ â†‘ (ì˜ ë¶„ë¦¬ë¨)
- êµ°ì§‘ ë‚´ ê±°ë¦¬ â†“ (ì¡°ë°€í•¨)
â†’ Dunn Index â†‘

ë‚˜ìœ êµ°ì§‘:
- êµ°ì§‘ ê°„ ê±°ë¦¬ â†“ (ê²¹ì¹¨)
- êµ°ì§‘ ë‚´ ê±°ë¦¬ â†‘ (í¼ì ¸ìˆìŒ)
â†’ Dunn Index â†“
```

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.metrics.pairwise import pairwise_distances
import numpy as np

def dunn_index(X, labels):
    """
    Dunn Index ê³„ì‚°
    
    Parameters:
    X: ë°ì´í„°
    labels: êµ°ì§‘ ë ˆì´ë¸”
    
    Returns:
    dunn_idx: Dunn Index
    """
    # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
    distances = pairwise_distances(X)
    
    unique_labels = np.unique(labels)
    n_clusters = len(unique_labels)
    
    # êµ°ì§‘ ë‚´ ìµœëŒ€ ê±°ë¦¬ (ì§ê²½)
    max_intra_cluster_distance = 0
    for label in unique_labels:
        cluster_points = np.where(labels == label)[0]
        if len(cluster_points) > 1:
            cluster_distances = distances[np.ix_(cluster_points, cluster_points)]
            max_dist = np.max(cluster_distances)
            max_intra_cluster_distance = max(max_intra_cluster_distance, max_dist)
    
    # êµ°ì§‘ ê°„ ìµœì†Œ ê±°ë¦¬
    min_inter_cluster_distance = np.inf
    for i in range(n_clusters):
        for j in range(i + 1, n_clusters):
            cluster_i = np.where(labels == unique_labels[i])[0]
            cluster_j = np.where(labels == unique_labels[j])[0]
            inter_distances = distances[np.ix_(cluster_i, cluster_j)]
            min_dist = np.min(inter_distances)
            min_inter_cluster_distance = min(min_inter_cluster_distance, min_dist)
    
    # Dunn Index
    if max_intra_cluster_distance > 0:
        dunn_idx = min_inter_cluster_distance / max_intra_cluster_distance
    else:
        dunn_idx = 0
    
    return dunn_idx

# ì‚¬ìš© ì˜ˆì œ
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

dunn_idx = dunn_index(X, y_pred)
print(f"Dunn Index: {dunn_idx:.4f}")

# ë‹¤ì–‘í•œ kì— ëŒ€í•œ Dunn Index
dunn_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    dunn_idx = dunn_index(X, y_pred)
    dunn_scores.append(dunn_idx)
    print(f"k={k}: Dunn Index = {dunn_idx:.4f}")

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(K_range, dunn_scores, 'mo-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Dunn Index')
plt.title('Dunn Index by k')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 7ï¸âƒ£ ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì§€ìˆ˜ (Davies-Bouldin Index)

### ğŸ“Œ ì •ì˜
ê° êµ°ì§‘ì˜ ì‘ì§‘ë„ì™€ ë¶„ë¦¬ë„ì˜ ë¹„ìœ¨ì„ í‰ê· í•œ ì§€í‘œì…ë‹ˆë‹¤.

### ğŸ“ ìˆ˜ì‹

```
DB = (1/k) Î£(i=1 to k) max(jâ‰ i) Rij

Rij = (Si + Sj) / dij

ì—¬ê¸°ì„œ:
Si: ië²ˆì§¸ êµ°ì§‘ì˜ í‰ê·  ê±°ë¦¬ (ì‘ì§‘ë„)
dij: êµ°ì§‘ iì™€ jì˜ ì¤‘ì‹¬ ê°„ ê±°ë¦¬ (ë¶„ë¦¬ë„)
```

### ğŸ” í•´ì„

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ë²”ìœ„** | 0 ~ âˆ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ) |
| **ì˜ë¯¸** | êµ°ì§‘ì´ ì¡°ë°€í•˜ê³  ì˜ ë¶„ë¦¬ë ìˆ˜ë¡ ë‚®ì€ ê°’ |
| **ì¥ì ** | ê³„ì‚°ì´ ë¹ ë¦„ |
| **ë‹¨ì ** | êµ¬í˜• êµ°ì§‘ì— ìœ ë¦¬ |

### ğŸ’» Python ì˜ˆì œ

```python
from sklearn.metrics import davies_bouldin_score

# Davies-Bouldin Index ê³„ì‚°
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

db_score = davies_bouldin_score(X, y_pred)
print(f"Davies-Bouldin Index: {db_score:.3f}")

# ë‹¤ì–‘í•œ kì— ëŒ€í•œ DB ì ìˆ˜
db_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    db_score = davies_bouldin_score(X, y_pred)
    db_scores.append(db_score)
    print(f"k={k}: DB Index = {db_score:.3f}")

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(K_range, db_scores, 'ro-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Davies-Bouldin Index')
plt.title('Davies-Bouldin Index by k (Lower is Better)')
plt.grid(True, alpha=0.3)
plt.show()

optimal_k = K_range[np.argmin(db_scores)]
print(f"\nìµœì  êµ°ì§‘ ìˆ˜: {optimal_k}")
```

---

## ğŸ“Š íƒ€ë‹¹ì„± ì§€í‘œ ì¢…í•© ë¹„êµ

### ì§€í‘œë³„ íŠ¹ì„± ë¹„êµí‘œ

| ì§€í‘œ | ë²”ìœ„ | ìµœì ê°’ | ê³„ì‚° ì†ë„ | ì¥ì  | ë‹¨ì  |
|------|------|--------|----------|------|------|
| **Silhouette** | [-1, 1] | ë†’ì„ìˆ˜ë¡ | ëŠë¦¼ O(nÂ²) | ì§ê´€ì , ì‹œê°í™” ê°€ëŠ¥ | ê³„ì‚° ë³µì¡ |
| **Calinski-Harabasz** | [0, âˆ) | ë†’ì„ìˆ˜ë¡ | ë¹ ë¦„ O(nk) | ë¹ ë¥¸ ê³„ì‚° | êµ¬í˜• êµ°ì§‘ í¸í–¥ |
| **Dunn Index** | [0, âˆ) | ë†’ì„ìˆ˜ë¡ | ë§¤ìš° ëŠë¦¼ O(nÂ²) | êµ°ì§‘ ì§ˆ ì •í™• | ê³„ì‚° ë¹„ìš© ë†’ìŒ |
| **Davies-Bouldin** | [0, âˆ) | ë‚®ì„ìˆ˜ë¡ | ë¹ ë¦„ O(nk) | ë¹ ë¥¸ ê³„ì‚° | êµ¬í˜• êµ°ì§‘ í¸í–¥ |

### ğŸ¯ ì§€í‘œ ì„ íƒ ê°€ì´ë“œ

```
ëŒ€ê·œëª¨ ë°ì´í„°:
â†’ Calinski-Harabasz, Davies-Bouldin

ì •í™•í•œ í‰ê°€ í•„ìš”:
â†’ Silhouette, Dunn Index

ë¹ ë¥¸ ë¹„êµ í•„ìš”:
â†’ Calinski-Harabasz

ì‹œê°í™” í•„ìš”:
â†’ Silhouette Plot

ì¢…í•© í‰ê°€:
â†’ ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ ì‚¬ìš©
```

---

## ğŸ’» í†µí•© ì˜ˆì œ: ëª¨ë“  ì§€í‘œ ë¹„êµ

```python
from sklearn.cluster import KMeans
from sklearn.metrics import (silhouette_score, calinski_harabasz_score, 
                             davies_bouldin_score)
from sklearn.datasets import make_blobs
import pandas as pd
import matplotlib.pyplot as plt

# ë°ì´í„° ìƒì„±
X, y_true = make_blobs(n_samples=300, centers=4, 
                       cluster_std=0.60, random_state=42)

# ë‹¤ì–‘í•œ kì— ëŒ€í•´ ëª¨ë“  ì§€í‘œ ê³„ì‚°
results = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    
    # ì§€í‘œ ê³„ì‚°
    silhouette = silhouette_score(X, y_pred)
    calinski = calinski_harabasz_score(X, y_pred)
    davies = davies_bouldin_score(X, y_pred)
    dunn = dunn_index(X, y_pred)
    inertia = kmeans.inertia_
    
    results.append({
        'k': k,
        'Silhouette': silhouette,
        'Calinski-Harabasz': calinski,
        'Davies-Bouldin': davies,
        'Dunn': dunn,
        'Inertia': inertia
    })

# DataFrameìœ¼ë¡œ ë³€í™˜
df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))

# ì‹œê°í™”
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. Silhouette
axes[0, 0].plot(df_results['k'], df_results['Silhouette'], 'bo-', linewidth=2)
axes[0, 0].set_title('Silhouette Score (Higher is Better)')
axes[0, 0].set_xlabel('k')
axes[0, 0].grid(True, alpha=0.3)

# 2. Calinski-Harabasz
axes[0, 1].plot(df_results['k'], df_results['Calinski-Harabasz'], 'go-', linewidth=2)
axes[0, 1].set_title('Calinski-Harabasz Index (Higher is Better)')
axes[0, 1].set_xlabel('k')
axes[0, 1].grid(True, alpha=0.3)

# 3. Davies-Bouldin
axes[0, 2].plot(df_results['k'], df_results['Davies-Bouldin'], 'ro-', linewidth=2)
axes[0, 2].set_title('Davies-Bouldin Index (Lower is Better)')
axes[0, 2].set_xlabel('k')
axes[0, 2].grid(True, alpha=0.3)

# 4. Dunn Index
axes[1, 0].plot(df_results['k'], df_results['Dunn'], 'mo-', linewidth=2)
axes[1, 0].set_title('Dunn Index (Higher is Better)')
axes[1, 0].set_xlabel('k')
axes[1, 0].grid(True, alpha=0.3)

# 5. Inertia (Elbow)
axes[1, 1].plot(df_results['k'], df_results['Inertia'], 'co-', linewidth=2)
axes[1, 1].set_title('Inertia/WCSS (Elbow Method)')
axes[1, 1].set_xlabel('k')
axes[1, 1].grid(True, alpha=0.3)

# 6. ìµœì  k ì¶”ì²œ
optimal_k_silhouette = df_results.loc[df_results['Silhouette'].idxmax(), 'k']
optimal_k_calinski = df_results.loc[df_results['Calinski-Harabasz'].idxmax(), 'k']
optimal_k_davies = df_results.loc[df_results['Davies-Bouldin'].idxmin(), 'k']
optimal_k_dunn = df_results.loc[df_results['Dunn'].idxmax(), 'k']

summary_text = f"""
ìµœì  êµ°ì§‘ ìˆ˜ ì¶”ì²œ:

Silhouette: k = {int(optimal_k_silhouette)}
Calinski-Harabasz: k = {int(optimal_k_calinski)}
Davies-Bouldin: k = {int(optimal_k_davies)}
Dunn Index: k = {int(optimal_k_dunn)}

ê¶Œì¥: k = {int(optimal_k_silhouette)}
(ëŒ€ë¶€ë¶„ì˜ ì§€í‘œê°€ ë™ì˜)
"""

axes[1, 2].text(0.1, 0.5, summary_text, fontsize=11, 
               verticalalignment='center',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig('clustering_evaluation.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n" + "="*50)
print("ìµœì  êµ°ì§‘ ìˆ˜ ìš”ì•½")
print("="*50)
print(f"Silhouette Score â†’ k = {int(optimal_k_silhouette)}")
print(f"Calinski-Harabasz â†’ k = {int(optimal_k_calinski)}")
print(f"Davies-Bouldin â†’ k = {int(optimal_k_davies)}")
print(f"Dunn Index â†’ k = {int(optimal_k_dunn)}")
```

---

## ğŸ¯ ADP ì‹¤ì „ ë¬¸ì œ

### ğŸ“ ë¬¸ì œ 1: ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê³„ì‚° (ë‚œì´ë„: â˜…â˜…â˜†)

**ì§ˆë¬¸**: ë‹¤ìŒì€ 3ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ì™€ 2ê°œì˜ êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.

```
êµ°ì§‘ 1: {A, B}
êµ°ì§‘ 2: {C}

ê±°ë¦¬:
d(A, B) = 2
d(A, C) = 5
d(B, C) = 6
```

ì  Aì˜ ì‹¤ë£¨ì—£ ê³„ìˆ˜ëŠ”?

A) 0.4  
B) 0.5  
C) 0.6  
D) 0.7  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: C) 0.6**

**í•´ì„¤**:

**Step 1**: a(A) ê³„ì‚° (ê°™ì€ êµ°ì§‘ ë‚´ í‰ê·  ê±°ë¦¬)
```
a(A) = d(A, B) = 2
(êµ°ì§‘ 1ì—ëŠ” Aì™€ Bë§Œ ìˆìŒ)
```

**Step 2**: b(A) ê³„ì‚° (ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ êµ°ì§‘ê³¼ì˜ í‰ê·  ê±°ë¦¬)
```
b(A) = d(A, C) = 5
(êµ°ì§‘ 2ì—ëŠ” Cë§Œ ìˆìŒ)
```

**Step 3**: ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê³„ì‚°
```
s(A) = (b(A) - a(A)) / max(a(A), b(A))
     = (5 - 2) / max(2, 5)
     = 3 / 5
     = 0.6
```

**í•´ì„**: s(A) = 0.6ìœ¼ë¡œ ì–‘ìˆ˜ì´ê³  ë¹„êµì  ë†’ì€ ê°’ì´ë¯€ë¡œ, AëŠ” ìì‹ ì˜ êµ°ì§‘ì— ì˜ í• ë‹¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
</details>

---

### ğŸ“ ë¬¸ì œ 2: ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¸  ì§€ìˆ˜ í•´ì„ (ë‚œì´ë„: â˜…â˜…â˜†)

**ì§ˆë¬¸**: ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¸  ì§€ìˆ˜(Calinski-Harabasz Index)ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²ƒì€?

A) êµ°ì§‘ ê°„ ë¶„ì‚°ê³¼ êµ°ì§‘ ë‚´ ë¶„ì‚°ì˜ ë¹„ìœ¨ì„ ì¸¡ì •í•œë‹¤  
B) ê°’ì´ í´ìˆ˜ë¡ ì¢‹ì€ êµ°ì§‘ì„ ì˜ë¯¸í•œë‹¤  
C) F-í†µê³„ëŸ‰ê³¼ ìœ ì‚¬í•œ í˜•íƒœì´ë‹¤  
D) ì‹¤ë£¨ì—£ ê³„ìˆ˜ì™€ ë™ì¼í•œ ë²”ìœ„ë¥¼ ê°€ì§„ë‹¤  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: D) ì‹¤ë£¨ì—£ ê³„ìˆ˜ì™€ ë™ì¼í•œ ë²”ìœ„ë¥¼ ê°€ì§„ë‹¤**

**í•´ì„¤**:

**ê° ì„ íƒì§€ ë¶„ì„**:

**A) ì •ë‹µ**: 
```
CH = (SSB / (k-1)) / (SSW / (n-k))
êµ°ì§‘ ê°„ ë¶„ì‚° / êµ°ì§‘ ë‚´ ë¶„ì‚°
```

**B) ì •ë‹µ**: CH ê°’ì´ í´ìˆ˜ë¡ êµ°ì§‘ì´ ì¡°ë°€í•˜ê³  ì˜ ë¶„ë¦¬ë˜ì–´ ìˆìŒ

**C) ì •ë‹µ**: 
```
CH âˆ F-í†µê³„ëŸ‰
ë¶„ì: êµ°ì§‘ ê°„ í‰ê· ì œê³±
ë¶„ëª¨: êµ°ì§‘ ë‚´ í‰ê· ì œê³±
```

**D) í‹€ë¦¼**: 
```
Silhouette: [-1, 1]
Calinski-Harabasz: [0, âˆ)

ì™„ì „íˆ ë‹¤ë¥¸ ë²”ìœ„!
```

**ì •ë¦¬**:
| ì§€í‘œ | ë²”ìœ„ | ìµœì ê°’ |
|------|------|--------|
| Silhouette | [-1, 1] | ë†’ì„ìˆ˜ë¡ |
| Calinski-Harabasz | [0, âˆ) | ë†’ì„ìˆ˜ë¡ |
| Davies-Bouldin | [0, âˆ) | ë‚®ì„ìˆ˜ë¡ |
| Dunn Index | [0, âˆ) | ë†’ì„ìˆ˜ë¡ |
</details>

---

### ğŸ“ ë¬¸ì œ 3: ë˜ ì§€ìˆ˜ ì´í•´ (ë‚œì´ë„: â˜…â˜…â˜…)

**ì§ˆë¬¸**: ë˜ ì§€ìˆ˜(Dunn Index)ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•œ ì¡°ê±´ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?

A) êµ°ì§‘ ê°„ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³  êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”  
B) êµ°ì§‘ ê°„ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ê³  êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ìµœì†Œí™”  
C) êµ°ì§‘ ê°„ ê±°ë¦¬ì™€ êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ëª¨ë‘ ìµœì†Œí™”  
D) êµ°ì§‘ ê°„ ê±°ë¦¬ì™€ êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ëª¨ë‘ ìµœëŒ€í™”  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: B) êµ°ì§‘ ê°„ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ê³  êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ìµœì†Œí™”**

**í•´ì„¤**:

**ë˜ ì§€ìˆ˜ ê³µì‹**:
```
DI = min(i,j) Î´(Ci, Cj) / max(l) Î”(Cl)

ë¶„ì: êµ°ì§‘ ê°„ ìµœì†Œ ê±°ë¦¬ (í´ìˆ˜ë¡ ì¢‹ìŒ)
ë¶„ëª¨: êµ°ì§‘ ë‚´ ìµœëŒ€ ì§ê²½ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
```

**ìµœëŒ€í™” ì¡°ê±´**:

1. **ë¶„ì â†‘**: êµ°ì§‘ ê°„ ê±°ë¦¬ë¥¼ í¬ê²Œ
   - êµ°ì§‘ë“¤ì´ ì„œë¡œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆì–´ì•¼ í•¨
   - ë¶„ë¦¬ë„(separation) ì¦ê°€

2. **ë¶„ëª¨ â†“**: êµ°ì§‘ ë‚´ ê±°ë¦¬ë¥¼ ì‘ê²Œ
   - êµ°ì§‘ ë‚´ ì ë“¤ì´ ì„œë¡œ ê°€ê¹Œì´ ìˆì–´ì•¼ í•¨
   - ì‘ì§‘ë„(compactness) ì¦ê°€

**ì§ê´€ì  ì´í•´**:
```
ì¢‹ì€ êµ°ì§‘:
â”Œâ”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”
â”‚ â€¢â€¢ â”‚â†ë©€ë¦¬â†’â”‚ â€¢â€¢ â”‚  êµ°ì§‘ ê°„ ê±°ë¦¬ í¼
â”‚â€¢â€¢â€¢â€¢â”‚        â”‚â€¢â€¢â€¢â€¢â”‚  êµ°ì§‘ ë‚´ ê±°ë¦¬ ì‘ìŒ
â””â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”˜

ë‚˜ìœ êµ°ì§‘:
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ â€¢  â€¢ â”‚â†ê°€ê¹Œì›€â†’â”‚â€¢    â€¢â”‚  êµ°ì§‘ ê°„ ê±°ë¦¬ ì‘ìŒ
â”‚â€¢    â€¢â”‚  â”‚  â€¢  â€¢â”‚  êµ°ì§‘ ë‚´ ê±°ë¦¬ í¼
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
```

**ê° ì„ íƒì§€ ë¶„ì„**:
- **A) í‹€ë¦¼**: êµ°ì§‘ì´ ê²¹ì¹˜ê³  í¼ì ¸ìˆìŒ (DI â†“)
- **B) ì •ë‹µ**: ì´ìƒì ì¸ êµ°ì§‘ (DI â†‘)
- **C) í‹€ë¦¼**: ëª¨ë“  ì ì´ í•œ ê³³ì— ëª¨ì„
- **D) í‹€ë¦¼**: ëª¨ë“  ì ì´ í©ì–´ì§
</details>

---

### ğŸ“ ë¬¸ì œ 4: K-Means ìˆ˜ë ´ (ë‚œì´ë„: â˜…â˜…â˜†)

**ì§ˆë¬¸**: K-Means ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ì¡°ê±´ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?

A) ì‹¤ë£¨ì—£ ê³„ìˆ˜ê°€ ìµœëŒ€ê°€ ë  ë•Œ  
B) ì¤‘ì‹¬ì (centroid)ì´ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šì„ ë•Œ  
C) ë˜ ì§€ìˆ˜ê°€ ìµœëŒ€ê°€ ë  ë•Œ  
D) êµ°ì§‘ ìˆ˜ê°€ ë°ì´í„° ìˆ˜ì™€ ê°™ì•„ì§ˆ ë•Œ  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: B) ì¤‘ì‹¬ì (centroid)ì´ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šì„ ë•Œ**

**í•´ì„¤**:

**K-Means ì•Œê³ ë¦¬ì¦˜**:
```
1. kê°œì˜ ì´ˆê¸° ì¤‘ì‹¬ì  ëœë¤ ì„ íƒ
2. ê° ë°ì´í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
3. ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì ì„ ë‹¤ì‹œ ê³„ì‚°
4. ì¤‘ì‹¬ì ì´ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ, ì•„ë‹ˆë©´ 2ë²ˆìœ¼ë¡œ
```

**ìˆ˜ë ´ ì¡°ê±´**:
- ì¤‘ì‹¬ì ì˜ ìœ„ì¹˜ê°€ ë” ì´ìƒ ë°”ë€Œì§€ ì•ŠìŒ
- ë˜ëŠ” ë³€í™”ëŸ‰ì´ ì„ê³„ê°’(threshold) ì´í•˜
- ë˜ëŠ” ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬

**ëª©ì  í•¨ìˆ˜**:
```
J = Î£(i=1 to k) Î£(xâˆˆCi) ||x - Î¼i||Â²

ëª©í‘œ: J ìµœì†Œí™”
ìˆ˜ë ´: Jê°€ ë” ì´ìƒ ê°ì†Œí•˜ì§€ ì•ŠìŒ
```

**ê° ì„ íƒì§€ ë¶„ì„**:
- **A) í‹€ë¦¼**: ì‹¤ë£¨ì—£ì€ í‰ê°€ ì§€í‘œì¼ ë¿
- **B) ì •ë‹µ**: K-Meansì˜ ì •ì˜ìƒ ìˆ˜ë ´ ì¡°ê±´
- **C) í‹€ë¦¼**: ë˜ ì§€ìˆ˜ë„ í‰ê°€ ì§€í‘œì¼ ë¿
- **D) í‹€ë¦¼**: ì´ëŠ” overfitting

**ì°¸ê³ **:
- K-MeansëŠ” **í•­ìƒ ìˆ˜ë ´**í•¨ (ì§€ì—­ ìµœì í•´)
- í•˜ì§€ë§Œ ì „ì—­ ìµœì í•´ëŠ” ë³´ì¥ ì•ˆ ë¨
- ë”°ë¼ì„œ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ
</details>

---

### ğŸ“ ë¬¸ì œ 5: êµ°ì§‘ í‰ê°€ ì¢…í•© (ë‚œì´ë„: â˜…â˜…â˜…)

**ì§ˆë¬¸**: ë‹¤ìŒ ì¤‘ êµ°ì§‘í™” ê²°ê³¼ë¥¼ í‰ê°€í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?

A) í•˜ë‚˜ì˜ ì§€í‘œë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ì•Šê³  ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œë‹¤  
B) ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ êµ°ì§‘ì˜ ì˜ë¯¸ë¥¼ í•´ì„í•œë‹¤  
C) ì‹¤ë£¨ì—£ ê³„ìˆ˜ê°€ ìŒìˆ˜ì´ë©´ í•´ë‹¹ ë°ì´í„°ëŠ” ì˜ëª» ë¶„ë¥˜ëœ ê²ƒì´ë‹¤  
D) Elbow Methodì—ì„œ ëª…í™•í•œ êº¾ì´ëŠ” ì ì´ í•­ìƒ ì¡´ì¬í•œë‹¤  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: D) Elbow Methodì—ì„œ ëª…í™•í•œ êº¾ì´ëŠ” ì ì´ í•­ìƒ ì¡´ì¬í•œë‹¤**

**í•´ì„¤**:

**ê° ì„ íƒì§€ ë¶„ì„**:

**A) ì •ë‹µ**: 
```
ê¶Œì¥ ì ‘ê·¼:
1. ì—¬ëŸ¬ ì§€í‘œ ê³„ì‚° (Silhouette, CH, DB, Dunn)
2. ì‹œê°í™” (Elbow, Silhouette Plot)
3. ë„ë©”ì¸ ì§€ì‹ í™œìš©
4. ì¢…í•©ì  íŒë‹¨
```

**B) ì •ë‹µ**: 
```
ì˜ˆ: ê³ ê° êµ°ì§‘í™”
- êµ°ì§‘ 1: ê³ ê°€ì¹˜ ê³ ê°
- êµ°ì§‘ 2: ì ì¬ ê³ ê°
- êµ°ì§‘ 3: ì´íƒˆ ìœ„í—˜ ê³ ê°
â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ ë¶€ì—¬ ì¤‘ìš”
```

**C) ì •ë‹µ**: 
```
s(i) < 0: 
í•´ë‹¹ ë°ì´í„°ê°€ ë‹¤ë¥¸ êµ°ì§‘ì— ë” ê°€ê¹Œì›€
â†’ ì˜ëª» ë¶„ë¥˜ë˜ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ
```

**D) í‹€ë¦¼**: 
```
Elbow Methodì˜ í•œê³„:
- ëª…í™•í•œ êº¾ì´ëŠ” ì ì´ ì—†ì„ ìˆ˜ ìˆìŒ
- ì£¼ê´€ì  íŒë‹¨ í•„ìš”
- ë°ì´í„°ì— ë”°ë¼ ì™„ë§Œí•œ ê³¡ì„ ë§Œ ë‚˜íƒ€ë‚¨

ì˜ˆ:
WCSS
  â†‘
  |ï¼¼
  |  ï¼¼___________  â† ëª…í™•í•œ elbow ì—†ìŒ
  |
  +â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ k
```

**ì‹¤ì œ ì‚¬ë¡€**:
```python
# ëª…í™•í•œ elbowê°€ ì—†ëŠ” ê²½ìš°
wcss = [100, 85, 75, 68, 63, 59, 56, 54, 52, 50]
# ê³„ì† ì™„ë§Œí•˜ê²Œ ê°ì†Œ â†’ ìµœì  k íŒë‹¨ ì–´ë ¤ì›€
```

**Best Practice**:
1. Elbow Methodë§Œ ì‚¬ìš© X
2. ì—¬ëŸ¬ ì§€í‘œ í•¨ê»˜ ì‚¬ìš©
3. ì‹¤ë£¨ì—£ ë¶„ì„ ì¶”ê°€
4. ë„ë©”ì¸ ì§€ì‹ í™œìš©
5. ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì  ê³ ë ¤
</details>

---

### ğŸ“ ë¬¸ì œ 6: ê³„ì¸µì  êµ°ì§‘ (ë‚œì´ë„: â˜…â˜…â˜†)

**ì§ˆë¬¸**: ê³„ì¸µì  êµ°ì§‘ë¶„ì„(Hierarchical Clustering)ì—ì„œ Ward's methodì˜ íŠ¹ì§•ì€?

A) êµ°ì§‘ ê°„ ìµœë‹¨ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤  
B) êµ°ì§‘ ê°„ ìµœì¥ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤  
C) êµ°ì§‘ ë³‘í•© ì‹œ ë¶„ì‚° ì¦ê°€ë¥¼ ìµœì†Œí™”í•œë‹¤  
D) ë°€ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ°ì§‘ì„ í˜•ì„±í•œë‹¤  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: C) êµ°ì§‘ ë³‘í•© ì‹œ ë¶„ì‚° ì¦ê°€ë¥¼ ìµœì†Œí™”í•œë‹¤**

**í•´ì„¤**:

**ê³„ì¸µì  êµ°ì§‘ ì—°ê²° ë°©ë²•**:

| ë°©ë²• | ê±°ë¦¬ ì¸¡ì • | íŠ¹ì§• |
|------|----------|------|
| **Single** | min(d(a,b)) | ìµœë‹¨ ê±°ë¦¬, ì²´ì¸ í˜•íƒœ |
| **Complete** | max(d(a,b)) | ìµœì¥ ê±°ë¦¬, êµ¬í˜• êµ°ì§‘ |
| **Average** | mean(d(a,b)) | í‰ê·  ê±°ë¦¬, ê· í˜• |
| **Ward** | ESS ì¦ê°€ëŸ‰ | ë¶„ì‚° ìµœì†Œí™”, K-Meansì™€ ìœ ì‚¬ |

**Ward's Method**:
```
ESS (Error Sum of Squares) = Î£ ||xi - xÌ„||Â²

ë³‘í•© ê¸°ì¤€:
ë‘ êµ°ì§‘ì„ ë³‘í•©í–ˆì„ ë•Œ ESS ì¦ê°€ëŸ‰ì´ 
ìµœì†Œì¸ ìŒì„ ì„ íƒ

Î”ESS = ESS(Ci âˆª Cj) - ESS(Ci) - ESS(Cj)
```

**ì§ê´€ì  ì´í•´**:
```
Ward's Method:
- êµ°ì§‘ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”
- ì¡°ë°€í•œ êµ°ì§‘ ìƒì„±
- K-Meansì™€ ê²°ê³¼ê°€ ìœ ì‚¬
- êµ¬í˜• êµ°ì§‘ì— ì í•©

Single Linkage:
- ì²´ì¸ í˜•íƒœ êµ°ì§‘
- ì´ìƒì¹˜ì— ë¯¼ê°

Complete Linkage:
- êµ¬í˜• êµ°ì§‘
- ì´ìƒì¹˜ì— ë‘”ê°
```

**Python ì˜ˆì œ**:
```python
from scipy.cluster.hierarchy import linkage

# Ward's method
linkage_matrix_ward = linkage(X, method='ward')

# Single linkage
linkage_matrix_single = linkage(X, method='single')

# Complete linkage
linkage_matrix_complete = linkage(X, method='complete')
```
</details>

---

### ğŸ“ ë¬¸ì œ 7: DBSCAN íŒŒë¼ë¯¸í„° (ë‚œì´ë„: â˜…â˜…â˜…)

**ì§ˆë¬¸**: DBSCAN ì•Œê³ ë¦¬ì¦˜ì—ì„œ eps=0.5, min_samples=5ë¡œ ì„¤ì •í–ˆì„ ë•Œ, ë‹¤ìŒ ì¤‘ í•µì‹¬ í¬ì¸íŠ¸(core point)ê°€ ë˜ê¸° ìœ„í•œ ì¡°ê±´ì€?

A) ë°˜ê²½ 0.5 ë‚´ì— 5ê°œ ì´ìƒì˜ ì ì´ ìˆì–´ì•¼ í•œë‹¤  
B) ë°˜ê²½ 0.5 ë‚´ì— ì •í™•íˆ 5ê°œì˜ ì ì´ ìˆì–´ì•¼ í•œë‹¤  
C) 5ê°œ ì´ìƒì˜ êµ°ì§‘ì— ì†í•´ì•¼ í•œë‹¤  
D) 5ë²ˆ ì´ìƒ ë°©ë¬¸ë˜ì–´ì•¼ í•œë‹¤  

<details>
<summary>ì •ë‹µ ë° í•´ì„¤</summary>

**ì •ë‹µ: A) ë°˜ê²½ 0.5 ë‚´ì— 5ê°œ ì´ìƒì˜ ì ì´ ìˆì–´ì•¼ í•œë‹¤**

**í•´ì„¤**:

**DBSCAN í•µì‹¬ ê°œë…**:

```
íŒŒë¼ë¯¸í„°:
- eps (Îµ): ì´ì›ƒì„ ì •ì˜í•˜ëŠ” ë°˜ê²½
- min_samples: í•µì‹¬ í¬ì¸íŠ¸ê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œ ì´ì›ƒ ìˆ˜

ì ì˜ ë¶„ë¥˜:
1. í•µì‹¬ í¬ì¸íŠ¸ (Core Point):
   - eps ë‚´ì— min_samplesê°œ ì´ìƒì˜ ì´ì›ƒ
   
2. ê²½ê³„ í¬ì¸íŠ¸ (Border Point):
   - í•µì‹¬ í¬ì¸íŠ¸ì˜ ì´ì›ƒì´ì§€ë§Œ ìì‹ ì€ í•µì‹¬ ì•„ë‹˜
   
3. ì¡ìŒ í¬ì¸íŠ¸ (Noise Point):
   - ì–´ë–¤ êµ°ì§‘ì—ë„ ì†í•˜ì§€ ì•ŠìŒ
```

**ì˜ˆì œ**:
```
eps = 0.5, min_samples = 5

ì  Aë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë°˜ê²½ 0.5 ë‚´ì˜ ì ë“¤:
- A ìì‹  í¬í•¨ ì´ 6ê°œ
â†’ AëŠ” í•µì‹¬ í¬ì¸íŠ¸ âœ“

ì  Bë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë°˜ê²½ 0.5 ë‚´ì˜ ì ë“¤:
- B ìì‹  í¬í•¨ ì´ 3ê°œ
â†’ BëŠ” í•µì‹¬ í¬ì¸íŠ¸ê°€ ì•„ë‹˜ âœ—
â†’ ë§Œì•½ Bê°€ í•µì‹¬ í¬ì¸íŠ¸ì˜ ì´ì›ƒì´ë©´ ê²½ê³„ í¬ì¸íŠ¸
â†’ ì•„ë‹ˆë©´ ì¡ìŒ í¬ì¸íŠ¸
```

**ê° ì„ íƒì§€ ë¶„ì„**:
- **A) ì •ë‹µ**: ì •í™•í•œ ì •ì˜
- **B) í‹€ë¦¼**: "ì´ìƒ" (â‰¥)ì´ì§€ "ì •í™•íˆ" (=)ê°€ ì•„ë‹˜
- **C) í‹€ë¦¼**: êµ°ì§‘ ìˆ˜ì™€ ë¬´ê´€
- **D) í‹€ë¦¼**: ë°©ë¬¸ íšŸìˆ˜ì™€ ë¬´ê´€

**Python ì˜ˆì œ**:
```python
from sklearn.cluster import DBSCAN

# DBSCAN ì„¤ì •
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# í•µì‹¬ í¬ì¸íŠ¸ í™•ì¸
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True

print(f"í•µì‹¬ í¬ì¸íŠ¸ ìˆ˜: {sum(core_samples_mask)}")
print(f"ì¡ìŒ í¬ì¸íŠ¸ ìˆ˜: {sum(labels == -1)}")
```
</details>

---

## ğŸ“Š ì‹œê°í™” ìë£Œ

### ğŸ¨ 1. êµ°ì§‘ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

![êµ°ì§‘ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ](clustering_algorithms_comparison.png)

**ì„¤ëª…:**
- **í–‰**: ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ (Blobs, Moons, Circles)
- **ì—´**: ì›ë³¸ ë°ì´í„°, K-Means, ê³„ì¸µì  êµ°ì§‘, DBSCAN
- **ê´€ì°°**: 
  - K-MeansëŠ” êµ¬í˜• êµ°ì§‘(Blobs)ì— ì í•©
  - DBSCANì€ ì„ì˜ í˜•íƒœ(Moons, Circles)ì— ê°•ì 
  - ê³„ì¸µì  êµ°ì§‘ì€ ê· í˜•ì¡íŒ ê²°ê³¼

### ğŸ¨ 2. ì‹¤ë£¨ì—£ ë¶„ì„

![ì‹¤ë£¨ì—£ ë¶„ì„](silhouette_analysis.png)

**ì„¤ëª…:**
- **ê° ê·¸ë˜í”„**: ì„œë¡œ ë‹¤ë¥¸ kê°’ (2~7)
- **yì¶•**: êµ°ì§‘ë³„ ìƒ˜í”Œë“¤
- **xì¶•**: ì‹¤ë£¨ì—£ ê³„ìˆ˜
- **ë¹¨ê°„ ì„ **: í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜
- **í•´ì„**: k=4ì¼ ë•Œ ê°€ì¥ ë†’ê³  ê· ì¼í•œ ì‹¤ë£¨ì—£ ì ìˆ˜

### ğŸ¨ 3. í‰ê°€ ì§€í‘œ ì¢…í•©

![í‰ê°€ ì§€í‘œ ì¢…í•©](clustering_evaluation_metrics.png)

**ì„¤ëª…:**
- **Elbow Method**: k=4ì—ì„œ êº¾ì„
- **Silhouette**: k=4ì—ì„œ ìµœëŒ€
- **Calinski-Harabasz**: k=4ì—ì„œ ìµœëŒ€
- **Davies-Bouldin**: k=4ì—ì„œ ìµœì†Œ
- **ê²°ë¡ **: ëª¨ë“  ì§€í‘œê°€ k=4ë¥¼ ìµœì ìœ¼ë¡œ ì§€ëª©

### ğŸ¨ 4. ê³„ì¸µì  êµ°ì§‘ ë´ë“œë¡œê·¸ë¨

![ê³„ì¸µì  êµ°ì§‘ ë´ë“œë¡œê·¸ë¨](hierarchical_dendrograms.png)

**ì„¤ëª…:**
- **Single Linkage**: ì²´ì¸ í˜•íƒœ, ì´ìƒì¹˜ì— ë¯¼ê°
- **Complete Linkage**: êµ¬í˜• êµ°ì§‘ ì„ í˜¸
- **Average Linkage**: ê· í˜•ì¡íŒ ê²°ê³¼
- **Ward Method**: ë¶„ì‚° ìµœì†Œí™”, K-Meansì™€ ìœ ì‚¬

### ğŸ¨ 5. DBSCAN íŒŒë¼ë¯¸í„° íš¨ê³¼

![DBSCAN íŒŒë¼ë¯¸í„°](dbscan_parameters.png)

**ì„¤ëª…:**
- **eps**: ì´ì›ƒ ë°˜ê²½ (ì‘ì„ìˆ˜ë¡ ë§ì€ êµ°ì§‘/ì¡ìŒ)
- **min_samples**: ìµœì†Œ ì´ì›ƒ ìˆ˜ (í´ìˆ˜ë¡ ì ì€ êµ°ì§‘/ë§ì€ ì¡ìŒ)
- **ìµœì  ì¡°í•©**: eps=0.2, min_samples=5 (2ê°œ êµ°ì§‘, ì ì€ ì¡ìŒ)

---

## ğŸ“ ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ìš”ì•½

### ğŸ”¥ ë°˜ë“œì‹œ ì•”ê¸°í•  ë‚´ìš©

#### 1. **ì‹¤ë£¨ì—£ ê³„ìˆ˜**
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))

a(i): ê°™ì€ êµ°ì§‘ ë‚´ í‰ê·  ê±°ë¦¬
b(i): ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ êµ°ì§‘ê³¼ì˜ í‰ê·  ê±°ë¦¬

ë²”ìœ„: [-1, 1]
í•´ì„: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
```

#### 2. **ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¸  ì§€ìˆ˜**
```
CH = (SSB / (k-1)) / (SSW / (n-k))

SSB: êµ°ì§‘ ê°„ ì œê³±í•©
SSW: êµ°ì§‘ ë‚´ ì œê³±í•©

ë²”ìœ„: [0, âˆ)
í•´ì„: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
```

#### 3. **ë˜ ì§€ìˆ˜**
```
DI = min(êµ°ì§‘ ê°„ ê±°ë¦¬) / max(êµ°ì§‘ ë‚´ ì§ê²½)

ë²”ìœ„: [0, âˆ)
í•´ì„: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
```

#### 4. **ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì§€ìˆ˜**
```
DB = (1/k) Î£ max Rij

Rij = (Si + Sj) / dij

ë²”ìœ„: [0, âˆ)
í•´ì„: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
```

### ğŸ“Š ì§€í‘œ ë¹„êµí‘œ (ì•”ê¸°ìš©)

| ì§€í‘œ | ë²”ìœ„ | ìµœì  | ì†ë„ | í•µì‹¬ |
|------|------|------|------|------|
| **Silhouette** | [-1,1] | â†‘ | ëŠë¦¼ | ì‘ì§‘+ë¶„ë¦¬ |
| **CH** | [0,âˆ) | â†‘ | ë¹ ë¦„ | ë¶„ì‚°ë¹„ |
| **Dunn** | [0,âˆ) | â†‘ | ë§¤ìš°ëŠë¦¼ | ê±°ë¦¬ë¹„ |
| **DB** | [0,âˆ) | â†“ | ë¹ ë¦„ | í‰ê· ë¹„ |

### ğŸ¯ ì•Œê³ ë¦¬ì¦˜ë³„ íŠ¹ì§• (ì•”ê¸°ìš©)

| ì•Œê³ ë¦¬ì¦˜ | k ì§€ì • | í˜•íƒœ | ì¥ì  | ë‹¨ì  |
|---------|--------|------|------|------|
| **K-Means** | O | êµ¬í˜• | ë¹ ë¦„ | ì´ìƒì¹˜ |
| **ê³„ì¸µì ** | X | ë‹¤ì–‘ | ë´ë“œë¡œê·¸ë¨ | ëŠë¦¼ |
| **DBSCAN** | X | ì„ì˜ | ì´ìƒì¹˜íƒì§€ | íŒŒë¼ë¯¸í„° |

### âš ï¸ ì‹œí—˜ ì£¼ì˜ì‚¬í•­

1. **ë²”ìœ„ì™€ ìµœì ê°’ ë°©í–¥ í˜¼ë™ ì£¼ì˜**
   - Silhouette: ë†’ì„ìˆ˜ë¡ â†‘
   - CH: ë†’ì„ìˆ˜ë¡ â†‘
   - Dunn: ë†’ì„ìˆ˜ë¡ â†‘
   - **DB: ë‚®ì„ìˆ˜ë¡** â†“ (ë‹¤ë¦„!)

2. **ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê³„ì‚°**
   - a(i): ê°™ì€ êµ°ì§‘ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
   - b(i): ë‹¤ë¥¸ êµ°ì§‘ (í´ìˆ˜ë¡ ì¢‹ìŒ)
   - ë¶„ì: b-a (í´ìˆ˜ë¡ ì¢‹ìŒ)

3. **K-Means íŠ¹ì§•**
   - ëª©ì : WCSS ìµœì†Œí™”
   - ìˆ˜ë ´: ì¤‘ì‹¬ì  ë¶ˆë³€
   - ì´ˆê¸°ê°’: ëœë¤ (ì—¬ëŸ¬ë²ˆ ì‹œë„)

4. **DBSCAN ìš©ì–´**
   - Core point: â‰¥ min_samples
   - Border point: í•µì‹¬ì˜ ì´ì›ƒ
   - Noise: -1 ë ˆì´ë¸”

---

## ğŸ“– ì°¸ê³ ìë£Œ

- scikit-learn Clustering: https://scikit-learn.org/stable/modules/clustering.html
- scikit-learn Metrics: https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation
- scipy.cluster.hierarchy: https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html

---

**ì‘ì„±ì¼**: 2025-02-06  
**ë²„ì „**: 1.0  
**ìš©ë„**: ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬/ADP ì‹œí—˜ ëŒ€ë¹„ êµ°ì§‘ë¶„ì„ ì™„ì „ ì •ë¦¬
