# νλΌλ―Έν„° vs ν•μ΄νΌνλΌλ―Έν„° μ™„λ²½ κ°€μ΄λ“

## λ©μ°¨
1. [κΈ°λ³Έ κ°λ…](#1-κΈ°λ³Έ-κ°λ…)
2. [νλΌλ―Έν„° vs ν•μ΄νΌνλΌλ―Έν„° λΉ„κµ](#2-νλΌλ―Έν„°-vs-ν•μ΄νΌνλΌλ―Έν„°-λΉ„κµ)
3. [λ¨λΈλ³„ νλΌλ―Έν„°μ™€ ν•μ΄νΌνλΌλ―Έν„°](#3-λ¨λΈλ³„-νλΌλ―Έν„°μ™€-ν•μ΄νΌνλΌλ―Έν„°)
4. [ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ°©λ²•](#4-ν•μ΄νΌνλΌλ―Έν„°-νλ‹-λ°©λ²•)

---

## 1. κΈ°λ³Έ κ°λ…

### 1.1 νλΌλ―Έν„° (Parameter)

**μ •μ**: λ¨λΈμ΄ ν•™μµ κ³Όμ •μ—μ„ **μλ™μΌλ΅** ν•™μµν•μ—¬ κ²°μ •λλ” κ°’

**νΉμ§•**:
- λ°μ΄ν„°λ΅λ¶€ν„° ν•™μµλ¨
- λ¨λΈ λ‚΄λ¶€ λ³€μ
- ν•™μµ μ•κ³ λ¦¬μ¦μ΄ μµμ κ°’μ„ μ°Ύμ
- μμΈ΅μ— μ§μ ‘ μ‚¬μ©λ¨
- ν•™μµμ΄ λλ‚λ©΄ κ³ μ •λ¨

**μμ‹**:
```
μ„ ν•νκ·€: y = wβ‚xβ‚ + wβ‚‚xβ‚‚ + ... + b

νλΌλ―Έν„°:
- κ°€μ¤‘μΉ(weights): wβ‚, wβ‚‚, ...
- νΈν–¥(bias): b

β†’ κ²½μ‚¬ν•κ°•λ²•μΌλ΅ μλ™ ν•™μµ
```

### 1.2 ν•μ΄νΌνλΌλ―Έν„° (Hyperparameter)

**μ •μ**: ν•™μµ κ³Όμ •μ„ μ μ–΄ν•κΈ° μ„ν•΄ **μ‚¬μ©μκ°€ μ§μ ‘** μ„¤μ •ν•λ” κ°’

**νΉμ§•**:
- ν•™μµ μ „μ— λ―Έλ¦¬ μ„¤μ •
- λ¨λΈ μ™Έλ¶€ λ³€μ
- λ¨λΈμ μ„±λ¥κ³Ό ν•™μµ κ³Όμ •μ— μν–¥
- λ°μ΄ν„°λ΅λ¶€ν„° ν•™μµλμ§€ μ•μ
- νλ‹μ΄ ν•„μ”ν•¨

**μμ‹**:
```
μ‹ κ²½λ§ ν•™μµ:

ν•μ΄νΌνλΌλ―Έν„°:
- ν•™μµλ¥ (learning rate): 0.001
- λ°°μΉ ν¬κΈ°(batch size): 32
- μ—ν¬ν¬(epochs): 100
- μ€λ‹‰μΈµ μ: 3
- κ° μΈµμ λ…Έλ“ μ: [128, 64, 32]

β†’ μ‚¬μ©μκ°€ μ§μ ‘ μ„¤μ •
```

---

## 2. νλΌλ―Έν„° vs ν•μ΄νΌνλΌλ―Έν„° λΉ„κµ

### 2.1 ν•µμ‹¬ μ°¨μ΄μ 

| κµ¬λ¶„ | νλΌλ―Έν„° | ν•μ΄νΌνλΌλ―Έν„° |
|------|---------|---------------|
| **μ„¤μ • μ£Όμ²΄** | μ•κ³ λ¦¬μ¦μ΄ μλ™ ν•™μµ | μ‚¬μ©μκ°€ μλ™ μ„¤μ • |
| **ν•™μµ μ—¬λ¶€** | ν•™μµλ¨ | ν•™μµλμ§€ μ•μ |
| **μ„¤μ • μ‹μ ** | ν•™μµ μ¤‘ | ν•™μµ μ „ |
| **μν–¥ λ²”μ„** | μμΈ΅ κ²°κ³Όμ— μ§μ ‘ μν–¥ | ν•™μµ κ³Όμ •κ³Ό μµμΆ… μ„±λ¥μ— μν–¥ |
| **κ°’μ νΉμ„±** | λ°μ΄ν„° μμ΅΄μ  | λ¬Έμ μ™€ λ„λ©”μΈ μμ΅΄μ  |
| **μ΅°μ • λ°©λ²•** | μµμ ν™” μ•κ³ λ¦¬μ¦ | νλ‹ (Grid Search, Random Search λ“±) |
| **κ°μ** | μΌλ°μ μΌλ΅ λ§μ (μμ²~μλ°±λ§) | μΌλ°μ μΌλ΅ μ μ (μκ°~μμ‹­κ°) |

### 2.2 κ΄€κ³„λ„

```
ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚   ν•™μµ κ³Όμ •              β”‚
β”‚   (Training Process)    β”‚
β”‚                         β”‚
β”‚   λ°μ΄ν„° + λ¨λΈ κµ¬μ΅°    β”‚
β”‚         β†“               β”‚
β”‚   μµμ ν™” μ•κ³ λ¦¬μ¦        β”‚
β”‚         β†“               β”‚
β”‚   νλΌλ―Έν„° μ—…λ°μ΄νΈ      β”‚
β”‚   (w, b ν•™μµ)           β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
ν•™μµλ λ¨λΈ (νλΌλ―Έν„° ν™•μ •)
    β†“
μμΈ΅/μ¶”λ΅ 
```

### 2.3 μ‹¤μ  μμ‹ λΉ„κµ

```python
# μ„ ν•νκ·€ μμ‹

# ν•μ΄νΌνλΌλ―Έν„° (μ‚¬μ©μ μ„¤μ •)
learning_rate = 0.01        # ν•™μµλ¥ 
iterations = 1000           # λ°λ³µ νμ
regularization = 'L2'       # μ •κ·ν™” λ°©λ²•
alpha = 0.1                 # μ •κ·ν™” κ°•λ„

# νλΌλ―Έν„° (ν•™μµμΌλ΅ κ²°μ •)
# μ΄κΈ°κ°’
w = np.random.randn(n_features)
b = 0

# ν•™μµ κ³Όμ •
for i in range(iterations):
    # μμΈ΅
    y_pred = w @ X + b
    
    # μ†μ‹¤ κ³„μ‚°
    loss = mse(y, y_pred)
    
    # νλΌλ―Έν„° μ—…λ°μ΄νΈ (μλ™ ν•™μµ)
    w = w - learning_rate * gradient_w
    b = b - learning_rate * gradient_b

# μµμΆ… ν•™μµλ νλΌλ―Έν„°
print(f"ν•™μµλ κ°€μ¤‘μΉ: {w}")
print(f"ν•™μµλ νΈν–¥: {b}")
```

---

## 3. λ¨λΈλ³„ νλΌλ―Έν„°μ™€ ν•μ΄νΌνλΌλ―Έν„°

### 3.1 μ„ ν• νκ·€ (Linear Regression)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ°€μ¤‘μΉ (wβ‚, wβ‚‚, ..., wβ‚™) | κ° νΉμ§•μ κ³„μ |
| | νΈν–¥ (b) | μ νΈ |
| **ν•μ΄νΌνλΌλ―Έν„°** | ν•™μµλ¥  (learning rate) | κ²½μ‚¬ν•κ°•λ²• μ‚¬μ© μ‹ |
| | μ •κ·ν™” μ ν• (None/L1/L2/Elastic) | Ridge, Lasso λ“± |
| | μ •κ·ν™” κ°•λ„ (Ξ±, Ξ») | μ •κ·ν™” νλ„ν‹° ν¬κΈ° |
| | λ°λ³µ νμ (iterations) | κ²½μ‚¬ν•κ°•λ²• μ‚¬μ© μ‹ |
| | fit_intercept | μ νΈ ν¬ν•¨ μ—¬λ¶€ |

**μμ‹**:
```
λ¨λΈ: Ε· = wβ‚xβ‚ + wβ‚‚xβ‚‚ + ... + wβ‚™xβ‚™ + b

νλΌλ―Έν„°: w = [wβ‚, wβ‚‚, ..., wβ‚™], b
ν•μ΄νΌνλΌλ―Έν„°: Ξ± (μ •κ·ν™” κ°•λ„)

Loss = MSE + Ξ±Β·||w||Β² (L2 μ •κ·ν™”)
```

### 3.2 λ΅μ§€μ¤ν‹± νκ·€ (Logistic Regression)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ°€μ¤‘μΉ (w) | μ„ ν• κ²°ν•© κ³„μ |
| | νΈν–¥ (b) | μ νΈ |
| **ν•μ΄νΌνλΌλ―Έν„°** | C (μ—­μ •κ·ν™” κ°•λ„) | 1/Ξ», ν΄μλ΅ μ •κ·ν™” μ•½ν•¨ |
| | penalty ('l1', 'l2', 'elasticnet') | μ •κ·ν™” μ ν• |
| | solver ('liblinear', 'saga', 'lbfgs') | μµμ ν™” μ•κ³ λ¦¬μ¦ |
| | max_iter | μµλ€ λ°λ³µ νμ |
| | class_weight | ν΄λμ¤ κ°€μ¤‘μΉ |

**μμ‹**:
```
λ¨λΈ: P(y=1|x) = Οƒ(wΒ·x + b) = 1/(1+e^(-(wΒ·x+b)))

νλΌλ―Έν„°: w, b
ν•μ΄νΌνλΌλ―Έν„°: C, penalty
```

### 3.3 μμ‚¬κ²°μ •λ‚λ¬΄ (Decision Tree)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | λ¶„ν•  μ§€μ  (split points) | κ° λ…Έλ“μ λ¶„ν•  κΈ°μ¤€κ°’ |
| | λ¶„ν•  νΉμ§• (split features) | μ–΄λ–¤ νΉμ§•μΌλ΅ λ¶„ν• ν• μ§€ |
| | λ¦¬ν”„ λ…Έλ“ κ°’ | μµμΆ… μμΈ΅κ°’ |
| | νΈλ¦¬ κµ¬μ΅° | λ…Έλ“ μ—°κ²° κ΄€κ³„ |
| **ν•μ΄νΌνλΌλ―Έν„°** | max_depth | μµλ€ νΈλ¦¬ κΉμ΄ |
| | min_samples_split | λ…Έλ“ λ¶„ν•  μµμ† μƒν” μ |
| | min_samples_leaf | λ¦¬ν”„ λ…Έλ“ μµμ† μƒν” μ |
| | max_features | λ¶„ν•  μ‹ κ³ λ ¤ν•  μµλ€ νΉμ§• μ |
| | criterion ('gini', 'entropy') | λ¶μλ„ μΈ΅μ • λ°©λ²• |
| | max_leaf_nodes | μµλ€ λ¦¬ν”„ λ…Έλ“ μ |
| | min_impurity_decrease | λ¶„ν•  μµμ† λ¶μλ„ κ°μ†λ‰ |

**νΈλ¦¬ κµ¬μ΅° μμ‹**:
```
              [Xβ‚ < 5.0]  β† νλΌλ―Έν„° (ν•™μµμΌλ΅ κ²°μ •)
              /         \
         [Xβ‚‚ < 3.0]    [Class B]
         /        \
    [Class A]  [Class C]

ν•μ΄νΌνλΌλ―Έν„°:
- max_depth = 2
- min_samples_split = 10
- criterion = 'gini'
```

### 3.4 λλ¤ ν¬λ μ¤νΈ (Random Forest)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ° νΈλ¦¬μ λ¶„ν•  μ§€μ  | κ°λ³„ νΈλ¦¬μ νλΌλ―Έν„°λ“¤ |
| | κ° νΈλ¦¬μ κµ¬μ΅° | νΈλ¦¬λ³„ λ…Έλ“ κµ¬μ„± |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_estimators | νΈλ¦¬ κ°μ |
| | max_depth | κ° νΈλ¦¬μ μµλ€ κΉμ΄ |
| | min_samples_split | λ…Έλ“ λ¶„ν•  μµμ† μƒν” μ |
| | min_samples_leaf | λ¦¬ν”„ λ…Έλ“ μµμ† μƒν” μ |
| | max_features ('auto', 'sqrt', 'log2') | λ¶„ν•  μ‹ κ³ λ ¤ν•  νΉμ§• μ |
| | bootstrap (True/False) | λ¶€νΈμ¤νΈλ© μƒν”λ§ μ—¬λ¶€ |
| | max_samples | κ° νΈλ¦¬μ μƒν” λΉ„μ¨ |
| | criterion ('gini', 'entropy') | λ¶μλ„ μΈ΅μ • λ°©λ²• |
| | oob_score | Out-of-bag ν‰κ°€ μ—¬λ¶€ |

**μ•™μƒλΈ” κµ¬μ΅°**:
```
Bootstrap μƒν” β†’ νΈλ¦¬1 (νλΌλ―Έν„°β‚) β”
Bootstrap μƒν” β†’ νΈλ¦¬2 (νλΌλ―Έν„°β‚‚) β”β†’ ν‰κ· /ν¬ν‘
Bootstrap μƒν” β†’ νΈλ¦¬3 (νλΌλ―Έν„°β‚ƒ) β”

ν•μ΄νΌνλΌλ―Έν„°: n_estimators=3, max_depth=10, ...
```

### 3.5 μ„ν¬νΈ λ²΅ν„° λ¨Έμ‹  (SVM)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ°€μ¤‘μΉ λ²΅ν„° (w) | κ²°μ • κ²½κ³„μ λ°©ν–¥ |
| | νΈν–¥ (b) | κ²°μ • κ²½κ³„μ μ„μΉ |
| | μ„ν¬νΈ λ²΅ν„° | κ²°μ • κ²½κ³„λ¥Ό μ •μν•λ” μƒν”λ“¤ |
| | Ξ± (λΌκ·Έλ‘μ£Ό μΉμ) | μ„ν¬νΈ λ²΅ν„°μ μ¤‘μ”λ„ |
| **ν•μ΄νΌνλΌλ―Έν„°** | C | μ¤λ¶„λ¥ νλ„ν‹° κ°•λ„ |
| | kernel ('linear', 'rbf', 'poly', 'sigmoid') | μ»¤λ„ ν•¨μ μ ν• |
| | gamma ('scale', 'auto', float) | RBF/poly/sigmoid μ»¤λ„ κ³„μ |
| | degree | polynomial μ»¤λ„ μ°¨μ |
| | coef0 | poly/sigmoid μ»¤λ„ μƒμν•­ |

**μμ‹**:
```
μ„ ν• SVM:
f(x) = sign(wΒ·x + b)

νλΌλ―Έν„°: w, b (μµμ ν™”λ΅ ν•™μµ)
ν•μ΄νΌνλΌλ―Έν„°: C, kernel, gamma

RBF μ»¤λ„:
K(x, x') = exp(-gammaΒ·||x-x'||Β²)
```

### 3.6 K-μµκ·Όμ ‘ μ΄μ›ƒ (KNN)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | ν•™μµ λ°μ΄ν„° μμ²΄ | μ „μ²΄ ν•™μµ λ°μ΄ν„° μ €μ¥ |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_neighbors (k) | κ³ λ ¤ν•  μ΄μ›ƒ κ°μ |
| | weights ('uniform', 'distance') | μ΄μ›ƒ κ°€μ¤‘μΉ λ°©λ²• |
| | algorithm ('auto', 'ball_tree', 'kd_tree', 'brute') | νƒμƒ‰ μ•κ³ λ¦¬μ¦ |
| | metric ('euclidean', 'manhattan', 'minkowski') | κ±°λ¦¬ μΈ΅μ • λ°©λ²• |
| | p | Minkowski κ±°λ¦¬μ μ°¨μ |
| | leaf_size | Tree μ•κ³ λ¦¬μ¦μ λ¦¬ν”„ ν¬κΈ° |

**νΉμ§•**:
```
KNNμ€ κ²μΌλ¥Έ ν•™μµ(Lazy Learning)
β†’ ν•™μµ λ‹¨κ³„μ—μ„ νλΌλ―Έν„°λ¥Ό ν•™μµν•μ§€ μ•μ
β†’ ν•™μµ λ°μ΄ν„° μμ²΄κ°€ "νλΌλ―Έν„°" μ—­ν• 

μμΈ΅ μ‹:
1. ν…μ¤νΈ μƒν”κ³Ό λ¨λ“  ν•™μµ μƒν”μ κ±°λ¦¬ κ³„μ‚°
2. κ°€μ¥ κ°€κΉμ΄ kκ° μ„ νƒ (kλ” ν•μ΄νΌνλΌλ―Έν„°)
3. λ‹¤μκ²° λλ” ν‰κ· μΌλ΅ μμΈ΅
```

### 3.7 λ‚μ΄λΈ λ² μ΄μ¦ (Naive Bayes)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | P(y) | ν΄λμ¤ μ‚¬μ „ ν™•λ¥  |
| | P(xαµΆ\|y) | μ΅°κ±΄λ¶€ ν™•λ¥  (νΉμ§•μ μ°λ„) |
| | ΞΌ, ΟƒΒ² (Gaussian NB) | ν‰κ· κ³Ό λ¶„μ‚° |
| | ΞΈ (Multinomial NB) | νΉμ§• ν™•λ¥  |
| **ν•μ΄νΌνλΌλ―Έν„°** | alpha (Laplace smoothing) | ν‰ν™ν™” νλΌλ―Έν„° |
| | fit_prior | ν΄λμ¤ μ‚¬μ „ ν™•λ¥  ν•™μµ μ—¬λ¶€ |
| | var_smoothing (Gaussian) | λ¶„μ‚° ν‰ν™ν™” |

**μμ‹**:
```
Naive Bayes:
P(y|x) β P(y)Β·βP(xαµΆ|y)

νλΌλ―Έν„° (λ°μ΄ν„°λ΅λ¶€ν„° μ¶”μ •):
- P(y): ν΄λμ¤ λΉλ„λ΅ μ¶”μ •
- P(xαµΆ|y): μ΅°κ±΄λ¶€ λΉλ„λ΅ μ¶”μ •

ν•μ΄νΌνλΌλ―Έν„°:
- alpha: P(xαµΆ|y) = (count + alpha) / (total + alphaΒ·n_features)
```

### 3.8 μΈκ³µμ‹ κ²½λ§ (Neural Network / MLP)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ°€μ¤‘μΉ (WΒΉ, WΒ², ..., Wα΄Έ) | κ° μΈµμ κ°€μ¤‘μΉ ν–‰λ ¬ |
| | νΈν–¥ (bΒΉ, bΒ², ..., bα΄Έ) | κ° μΈµμ νΈν–¥ λ²΅ν„° |
| | Batch Norm νλΌλ―Έν„° (Ξ³, Ξ²) | μ¤μΌ€μΌ, μ‹ν”„νΈ νλΌλ―Έν„° |
| **ν•μ΄νΌνλΌλ―Έν„°** | λ„¤νΈμ›ν¬ κµ¬μ΅° | μΈµ μ, κ° μΈµμ λ…Έλ“ μ |
| | ν•™μµλ¥  (learning_rate) | κ°€μ¤‘μΉ μ—…λ°μ΄νΈ ν¬κΈ° |
| | λ°°μΉ ν¬κΈ° (batch_size) | ν• λ²μ— μ²λ¦¬ν•  μƒν” μ |
| | μ—ν¬ν¬ (epochs) | μ „μ²΄ λ°μ΄ν„° ν•™μµ λ°λ³µ νμ |
| | ν™μ„±ν™” ν•¨μ | ReLU, Sigmoid, Tanh λ“± |
| | μµν‹°λ§μ΄μ € | SGD, Adam, RMSProp λ“± |
| | μ†μ‹¤ ν•¨μ | MSE, Cross-Entropy λ“± |
| | dropout λΉ„μ¨ | λ“λ΅­μ•„μ›ƒ λΉ„μ¨ |
| | L2 μ •κ·ν™” κ°•λ„ (lambda) | κ°€μ¤‘μΉ κ°μ‡  |
| | momentum | λ¨λ©ν…€ κ³„μ |
| | beta1, beta2 (Adam) | Adamμ λ¨λ©ν…€ νλΌλ―Έν„° |

**κµ¬μ΅° μμ‹**:
```
λ„¤νΈμ›ν¬ κµ¬μ΅° (ν•μ΄νΌνλΌλ―Έν„°):
μ…λ ¥μΈµ(4) β†’ μ€λ‹‰μΈµ1(64, ReLU) β†’ μ€λ‹‰μΈµ2(32, ReLU) β†’ μ¶λ ¥μΈµ(3, Softmax)

νλΌλ―Έν„° (ν•™μµλ¨):
WΒΉ: [4 Γ— 64]    bΒΉ: [64]
WΒ²: [64 Γ— 32]   bΒ²: [32]
WΒ³: [32 Γ— 3]    bΒ³: [3]

μ΄ νλΌλ―Έν„° μ = 4Γ—64 + 64 + 64Γ—32 + 32 + 32Γ—3 + 3 = 2,531κ°
```

### 3.9 XGBoost / Gradient Boosting

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | κ° νΈλ¦¬μ κ°€μ¤‘μΉ (wβ‚) | νΈλ¦¬μ κΈ°μ—¬λ„ |
| | κ° νΈλ¦¬μ κµ¬μ΅° | λ¶„ν•  μ§€μ , λ¦¬ν”„ κ°’ |
| | λ¦¬ν”„ λ…Έλ“ μ μ | κ° λ¦¬ν”„μ μμΈ΅κ°’ |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_estimators | λ¶€μ¤ν… λΌμ΄λ“ μ (νΈλ¦¬ κ°μ) |
| | learning_rate (eta) | ν•™μµλ¥  (0.01~0.3) |
| | max_depth | νΈλ¦¬ μµλ€ κΉμ΄ |
| | min_child_weight | μμ‹ λ…Έλ“ μµμ† κ°€μ¤‘μΉ ν•© |
| | subsample | ν–‰ μƒν”λ§ λΉ„μ¨ |
| | colsample_bytree | μ—΄ μƒν”λ§ λΉ„μ¨ |
| | gamma | λ¶„ν•  μµμ† μ†μ‹¤ κ°μ† |
| | reg_alpha (L1) | L1 μ •κ·ν™” |
| | reg_lambda (L2) | L2 μ •κ·ν™” |
| | objective | μ†μ‹¤ ν•¨μ |
| | scale_pos_weight | λ¶κ· ν• λ°μ΄ν„° κ°€μ¤‘μΉ |

**λ¶€μ¤ν… κ³Όμ •**:
```
Fβ‚€(x) = μ΄κΈ° μμΈ΅

Fβ‚(x) = Fβ‚€(x) + Ξ·Β·hβ‚(x)  β† Ξ·: ν•μ΄νΌνλΌλ―Έν„°
Fβ‚‚(x) = Fβ‚(x) + Ξ·Β·hβ‚‚(x)     hβ‚, hβ‚‚: νλΌλ―Έν„° (νΈλ¦¬)
...
Fβ‚(x) = Fβ‚β‚‹β‚(x) + Ξ·Β·hβ‚(x)

μµμΆ…: F(x) = Ξ£(Ξ·Β·hβ‚(x))
```

### 3.10 LightGBM

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | νΈλ¦¬ κµ¬μ΅° | λ¦¬ν”„ κ°’, λ¶„ν•  μ§€μ  |
| | νΈλ¦¬ κ°€μ¤‘μΉ | κ° νΈλ¦¬μ κΈ°μ—¬λ„ |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_estimators | νΈλ¦¬ κ°μ |
| | learning_rate | ν•™μµλ¥  |
| | max_depth | νΈλ¦¬ κΉμ΄ (-1: μ ν• μ—†μ) |
| | num_leaves | λ¦¬ν”„ λ…Έλ“ μµλ€ κ°μ |
| | min_data_in_leaf | λ¦¬ν”„μ μµμ† μƒν” μ |
| | feature_fraction | νΉμ§• μƒν”λ§ λΉ„μ¨ |
| | bagging_fraction | λ°μ΄ν„° μƒν”λ§ λΉ„μ¨ |
| | bagging_freq | λ°°κΉ… λΉλ„ |
| | lambda_l1, lambda_l2 | μ •κ·ν™” |
| | boosting_type ('gbdt', 'dart', 'goss') | λ¶€μ¤ν… μ ν• |

### 3.11 K-Means ν΄λ¬μ¤ν„°λ§

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | μ¤‘μ‹¬μ  μΆν‘ (ΞΌβ‚, ΞΌβ‚‚, ..., ΞΌβ‚–) | κ° ν΄λ¬μ¤ν„°μ μ¤‘μ‹¬ |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_clusters (k) | ν΄λ¬μ¤ν„° κ°μ |
| | init ('k-means++', 'random') | μ΄κΈ° μ¤‘μ‹¬μ  μ„¤μ • λ°©λ²• |
| | n_init | λ‹¤λ¥Έ μ΄κΈ°κ°’μΌλ΅ μ‹¤ν–‰ νμ |
| | max_iter | μµλ€ λ°λ³µ νμ |
| | tol | μλ ΄ ν—μ© μ¤μ°¨ |
| | algorithm ('auto', 'full', 'elkan') | μ•κ³ λ¦¬μ¦ μ„ νƒ |

**μ•κ³ λ¦¬μ¦**:
```
1. kκ° μ΄κΈ° μ¤‘μ‹¬μ  μ„ νƒ (ν•μ΄νΌνλΌλ―Έν„°: k, init)
2. λ°λ³µ (ν•μ΄νΌνλΌλ―Έν„°: max_iter):
   a. κ° μƒν”μ„ κ°€μ¥ κ°€κΉμ΄ μ¤‘μ‹¬μ μ— ν• λ‹Ή
   b. κ° ν΄λ¬μ¤ν„°μ μ¤‘μ‹¬μ  μ¬κ³„μ‚° (νλΌλ―Έν„° μ—…λ°μ΄νΈ)
3. μλ ΄κΉμ§€ λ°λ³µ

μµμΆ… νλΌλ―Έν„°: ΞΌβ‚, ΞΌβ‚‚, ..., ΞΌβ‚–
```

### 3.12 μ£Όμ„±λ¶„ λ¶„μ„ (PCA)

| κµ¬λ¶„ | λ‚΄μ© | μ„¤λ… |
|------|------|------|
| **νλΌλ―Έν„°** | μ£Όμ„±λ¶„ (PCβ‚, PCβ‚‚, ..., PCβ‚–) | κ³ μ λ²΅ν„° |
| | μ„¤λ… λ¶„μ‚° (Ξ»β‚, Ξ»β‚‚, ..., Ξ»β‚–) | κ³ μ κ°’ |
| | ν‰κ·  λ²΅ν„° (ΞΌ) | λ°μ΄ν„° μ¤‘μ‹¬ν™” |
| **ν•μ΄νΌνλΌλ―Έν„°** | n_components | μ£Όμ„±λ¶„ κ°μ λλ” μ„¤λ… λ¶„μ‚° λΉ„μ¨ |
| | svd_solver ('auto', 'full', 'arpack', 'randomized') | SVD μ•κ³ λ¦¬μ¦ |
| | whiten | λ°±μƒ‰ν™” μ—¬λ¶€ |

**μμ‹**:
```
PCA λ³€ν™:
X_transformed = (X - ΞΌ)Β·V

νλΌλ―Έν„° (λ°μ΄ν„°λ΅λ¶€ν„° κ³„μ‚°):
- ΞΌ: ν‰κ·  λ²΅ν„°
- V: μ£Όμ„±λ¶„ (κ³ μ λ²΅ν„°)
- Ξ»: μ„¤λ… λ¶„μ‚° (κ³ μ κ°’)

ν•μ΄νΌνλΌλ―Έν„°:
- n_components: λ‡ κ°μ μ£Όμ„±λ¶„ μ‚¬μ©ν• μ§€
```

---

## 4. ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ°©λ²•

### 4.1 μ£Όμ” νλ‹ κΈ°λ²• λΉ„κµ

| λ°©λ²• | μ„¤λ… | μ¥μ  | λ‹¨μ  | μ‚¬μ© μ‹λ‚λ¦¬μ¤ |
|------|------|------|------|--------------|
| **Grid Search** | λ¨λ“  μ΅°ν•© νƒμƒ‰ | μµμ κ°’ λ³΄μ¥ (λ²”μ„ λ‚΄) | κ³„μ‚° λΉ„μ© λ†’μ | ν•μ΄νΌνλΌλ―Έν„° μ κ³  λ²”μ„ μΆμ„ λ• |
| **Random Search** | λ¬΄μ‘μ„ μƒν”λ§ | Gridλ³΄λ‹¤ ν¨μ¨μ  | μµμ κ°’ λ³΄μ¥ μ• λ¨ | ν•μ΄νΌνλΌλ―Έν„° λ§μ„ λ• |
| **Bayesian Optimization** | ν™•λ¥ μ  λ¨λΈ κΈ°λ° | ν¨μ¨μ , μ μ€ μ‹λ„ | κµ¬ν„ λ³µμ΅ | ν‰κ°€ λΉ„μ© λ†’μ„ λ• |
| **Halving Grid/Random** | μ μ§„μ  μμ› ν• λ‹Ή | λΉ λ¦„ | scikit-learn μµμ‹  λ²„μ „ ν•„μ” | ν° λ°μ΄ν„°μ…‹ |
| **Optuna** | μλ™ μµμ ν™” ν”„λ μ„μ›ν¬ | κ°•λ ¥ν•¨, μ‹κ°ν™” | μ™Έλ¶€ λΌμ΄λΈλ¬λ¦¬ | λ³µμ΅ν• μµμ ν™” |

### 4.2 Grid Search μμ‹

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# ν•μ΄νΌνλΌλ―Έν„° κ·Έλ¦¬λ“ μ •μ
param_grid = {
    'n_estimators': [50, 100, 200],           # νΈλ¦¬ κ°μ
    'max_depth': [10, 20, 30, None],          # νΈλ¦¬ κΉμ΄
    'min_samples_split': [2, 5, 10],          # λ¶„ν•  μµμ† μƒν”
    'min_samples_leaf': [1, 2, 4],            # λ¦¬ν”„ μµμ† μƒν”
    'max_features': ['sqrt', 'log2']          # νΉμ§• κ°μ
}

# λ¨λΈ μƒμ„±
rf = RandomForestClassifier(random_state=42)

# Grid Search
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,                    # 5-fold κµμ°¨κ²€μ¦
    scoring='accuracy',
    n_jobs=-1,              # λ³‘λ ¬ μ²λ¦¬
    verbose=2
)

# μ‹¤ν–‰
grid_search.fit(X_train, y_train)

# μµμ  ν•μ΄νΌνλΌλ―Έν„°
print("μµμ  νλΌλ―Έν„°:", grid_search.best_params_)
print("μµμ  μ μ:", grid_search.best_score_)

# μ΄ μ‹λ„ νμ: 3 Γ— 4 Γ— 3 Γ— 3 Γ— 2 = 216 μ΅°ν•©
```

### 4.3 Random Search μμ‹

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# ν•μ΄νΌνλΌλ―Έν„° λ¶„ν¬ μ •μ
param_distributions = {
    'n_estimators': randint(50, 500),           # 50~500 μ‚¬μ΄ μ •μ
    'max_depth': randint(5, 50),                # 5~50 μ‚¬μ΄ μ •μ
    'min_samples_split': randint(2, 20),        # 2~20 μ‚¬μ΄ μ •μ
    'min_samples_leaf': randint(1, 10),         # 1~10 μ‚¬μ΄ μ •μ
    'max_features': uniform(0.1, 0.9)           # 0.1~1.0 μ‚¬μ΄ μ‹¤μ
}

# Random Search
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions,
    n_iter=100,              # 100λ² μ‹λ„
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=2
)

random_search.fit(X_train, y_train)

# μ΄ μ‹λ„ νμ: 100λ² (Gridλ³΄λ‹¤ ν›¨μ”¬ μ μ)
```

### 4.4 Bayesian Optimization μμ‹

```python
from sklearn.model_selection import cross_val_score
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# νƒμƒ‰ κ³µκ°„ μ •μ
search_spaces = {
    'n_estimators': Integer(50, 500),
    'max_depth': Integer(5, 50),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Real(0.1, 1.0)
}

# Bayesian Optimization
bayes_search = BayesSearchCV(
    estimator=rf,
    search_spaces=search_spaces,
    n_iter=50,               # 50λ²λ§ μ‹λ„
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

bayes_search.fit(X_train, y_train)

# 50λ²μΌλ΅ μµμ μ— κ°€κΉμ΄ κ²°κ³Ό λ„μ¶
```

### 4.5 ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ „λµ

```
λ‹¨κ³„λ³„ μ ‘κ·Ό:

1λ‹¨κ³„: κΈ°λ³Έκ°’μΌλ΅ μ‹μ‘
   - λ€λ¶€λ¶„μ λΌμ΄λΈλ¬λ¦¬λ” ν•©λ¦¬μ μΈ κΈ°λ³Έκ°’ μ κ³µ
   - λ² μ΄μ¤λΌμΈ μ„±λ¥ ν™•μΈ

2λ‹¨κ³„: μ¤‘μ”ν• ν•μ΄νΌνλΌλ―Έν„° μ‹λ³„
   - λ¨λΈλ³„ μ£Όμ” ν•μ΄νΌνλΌλ―Έν„° νμ•…
   - μν–¥λ ¥μ΄ ν° κ²ƒλ¶€ν„° νλ‹

3λ‹¨κ³„: λ„“μ€ λ²”μ„μ—μ„ νƒμƒ‰
   - Random Searchλ΅ λ€λµμ μΈ λ²”μ„ νμ•…
   - 10^-4 ~ 10^2 λ“± λ΅κ·Έ μ¤μΌ€μΌ μ‚¬μ©

4λ‹¨κ³„: μΆμ€ λ²”μ„μ—μ„ μ •λ°€ νƒμƒ‰
   - Grid Searchλ΅ μ„Έλ°€ν•κ² νƒμƒ‰
   - 3λ‹¨κ³„μ—μ„ μ°Ύμ€ μΆ‹μ€ κ°’ μ£Όλ³€ νƒμƒ‰

5λ‹¨κ³„: κµμ°¨ κ²€μ¦μΌλ΅ ν‰κ°€
   - K-Fold CVλ΅ μΌλ°ν™” μ„±λ¥ ν‰κ°€
   - κ³Όμ ν•© μ—¬λ¶€ ν™•μΈ

6λ‹¨κ³„: μµμΆ… λ¨λΈ μ„ νƒ
   - κ²€μ¦ λ°μ΄ν„°λ΅ μµμΆ… ν™•μΈ
   - ν…μ¤νΈ λ°μ΄ν„°λ” λ§μ§€λ§‰μ—λ§ μ‚¬μ©
```

### 4.6 λ¨λΈλ³„ μ£Όμ” νλ‹ μ°μ„ μμ„

#### Random Forest
```
1μμ„: n_estimators (λ§μ„μλ΅ μΆ‹μ, λ‹¨ κ³„μ‚° λΉ„μ© κ³ λ ¤)
2μμ„: max_depth (κ³Όμ ν•© λ°©μ§€)
3μμ„: min_samples_split, min_samples_leaf
4μμ„: max_features
```

#### XGBoost
```
1μμ„: learning_rate + n_estimators (ν•¨κ» μ΅°μ •)
2μμ„: max_depth (3~10)
3μμ„: subsample, colsample_bytree (0.6~1.0)
4μμ„: reg_alpha, reg_lambda (μ •κ·ν™”)
```

#### μ‹ κ²½λ§
```
1μμ„: λ„¤νΈμ›ν¬ κµ¬μ΅° (μΈµ μ, λ…Έλ“ μ)
2μμ„: learning_rate (0.0001~0.1)
3μμ„: batch_size (32, 64, 128)
4μμ„: dropout λΉ„μ¨ (0.2~0.5)
5μμ„: optimizer (Adamμ΄ μΌλ°μ μΌλ΅ μΆ‹μ)
```

#### SVM
```
1μμ„: kernel μ„ νƒ (linear, rbf)
2μμ„: C (0.1~1000, λ΅κ·Έ μ¤μΌ€μΌ)
3μμ„: gamma (rbf μ»¤λ„ μ‚¬μ© μ‹, 0.001~1)
```

### 4.7 ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ²΄ν¬λ¦¬μ¤νΈ

```
β–΅ λ°μ΄ν„° μ „μ²λ¦¬ μ™„λ£
  - μ¤μΌ€μΌλ§/μ •κ·ν™”
  - κ²°μΈ΅μΉ μ²λ¦¬
  - λ²”μ£Όν• μΈμ½”λ”©

β–΅ ν›λ ¨/κ²€μ¦/ν…μ¤νΈ λ¶„ν• 
  - ν…μ¤νΈ μ„ΈνΈλ” μ λ€ νλ‹μ— μ‚¬μ© κΈμ§€
  - κµμ°¨ κ²€μ¦ ν™μ©

β–΅ λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ •
  - κΈ°λ³Έ μ„¤μ •μΌλ΅ μ„±λ¥ ν™•μΈ

β–΅ νƒμƒ‰ λ²”μ„ μ„¤μ •
  - λ„λ¬΄ μΆμΌλ©΄ μµμ κ°’ λ†“μΉ¨
  - λ„λ¬΄ λ„“μΌλ©΄ κ³„μ‚° λΉ„μ© μ¦κ°€

β–΅ μ‹κ°„/μμ› κ³ λ ¤
  - n_iter, cv μ μ΅°μ •
  - λ³‘λ ¬ μ²λ¦¬ ν™μ© (n_jobs=-1)

β–΅ κ²°κ³Ό λ¶„μ„
  - learning curve ν™•μΈ
  - κ³Όμ ν•©/κ³Όμ†μ ν•© νλ‹¨

β–΅ μµμΆ… ν‰κ°€
  - ν…μ¤νΈ μ„ΈνΈλ΅ μµμΆ… μ„±λ¥ ν™•μΈ
```

---

## 5. μ •λ¦¬ λ° μ”μ•½

### 5.1 ν•µμ‹¬ ν¬μΈνΈ

**νλΌλ―Έν„°**:
- β… λ¨λΈμ΄ **μλ™μΌλ΅ ν•™μµ**
- β… μμΈ΅μ— **μ§μ ‘ μ‚¬μ©**λ¨
- β… μμ²~μλ°±λ§ κ° (λ³µμ΅ν• λ¨λΈ)
- β… λ°μ΄ν„° μμ΅΄μ 

**ν•μ΄νΌνλΌλ―Έν„°**:
- β… μ‚¬μ©μκ°€ **μλ™μΌλ΅ μ„¤μ •**
- β… ν•™μµ κ³Όμ • **μ μ–΄**
- β… μκ°~μμ‹­ κ°
- β… λ„λ©”μΈ μ§€μ‹ ν•„μ”

### 5.2 λΉ…λ°μ΄ν„°λ¶„μ„κΈ°μ‚¬ μ‹ν— λ€λΉ„

**μμ£Ό μ¶μ λλ” κ°λ…**:
1. νλΌλ―Έν„°μ™€ ν•μ΄νΌνλΌλ―Έν„°μ μ •μ μ°¨μ΄
2. κ° λ¨λΈμ λ€ν‘μ μΈ ν•μ΄νΌνλΌλ―Έν„° (νΉν νΈλ¦¬ κΈ°λ° λ¨λΈ)
3. Grid Search vs Random Search μ°¨μ΄
4. κµμ°¨ κ²€μ¦μ μ¤‘μ”μ„±
5. ν•™μµλ¥ , μ •κ·ν™” κ°•λ„ κ°™μ€ μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°μ μ—­ν• 

**μμƒ λ¬Έμ  μ ν•**:
```
Q: λ‹¤μ μ¤‘ ν•μ΄νΌνλΌλ―Έν„°κ°€ μ•„λ‹ κ²ƒμ€?
   β‘  Random Forestμ max_depth
   β‘΅ μ„ ν•νκ·€μ κ°€μ¤‘μΉ w
   β‘Ά SVMμ C
   β‘£ KNNμ k

A: β‘΅ (κ°€μ¤‘μΉλ” ν•™μµμΌλ΅ κ²°μ •λλ” νλΌλ―Έν„°)
```

### 5.3 λ¨λΈ μ„ νƒ κ°€μ΄λ“

| λ°μ΄ν„°/μƒν™© | μ¶”μ² λ¨λΈ | μ£Όμ” νλ‹ λ€μƒ |
|------------|----------|---------------|
| ν‘ ν•νƒ, μ¤‘μ†κ·λ¨ | Random Forest, XGBoost | n_estimators, max_depth, learning_rate |
| ν‘ ν•νƒ, λ€κ·λ¨ | LightGBM | num_leaves, learning_rate |
| μ΄λ―Έμ§€ λ°μ΄ν„° | CNN | λ„¤νΈμ›ν¬ κµ¬μ΅°, learning_rate, dropout |
| μ‹κ³„μ—΄ λ°μ΄ν„° | LSTM, GRU | λ„¤νΈμ›ν¬ κµ¬μ΅°, sequence_length |
| ν…μ¤νΈ λ°μ΄ν„° | Transformer | learning_rate, num_layers, hidden_size |
| μ„ ν• κ΄€κ³„ λ…ν™• | Linear/Logistic Regression | μ •κ·ν™” κ°•λ„ |
| κ³ μ°¨μ›, ν¬μ† | Linear SVM, Naive Bayes | C (SVM), alpha (NB) |

---

**μ‹ν— μ¤€λΉ„ ν™”μ΄ν…! π―**

νλΌλ―Έν„°λ” **ν•™μµλκ³ **, ν•μ΄νΌνλΌλ―Έν„°λ” **μ„¤μ •λλ‹¤**λ” μ μ„ λ…ν™•ν κΈ°μ–µν•μ„Έμ”!
