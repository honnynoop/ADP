# ë² ì´ì§€ì•ˆ ì •ë¦¬ì™€ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì™„ë²½ ê°€ì´ë“œ

## 1. ë² ì´ì§€ì•ˆ ì •ë¦¬ (Bayesian Theorem)

### 1.1 ê¸°ë³¸ ê°œë…
ë² ì´ì§€ì•ˆ ì •ë¦¬ëŠ” **í™•ë¥ ë¡ ì˜ ê¸°ë³¸ ì •ë¦¬**ë¡œ, ì‚¬ì „í™•ë¥ ê³¼ ìš°ë„ë¥¼ í†µí•´ ì‚¬í›„í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### 1.2 ë² ì´ì§€ì•ˆ ì •ë¦¬ ìˆ˜ì‹

```
P(A|B) = P(B|A) Ã— P(A) / P(B)
```

ë˜ëŠ” ë” ìì„¸íˆ:

```
P(A|B) = [P(B|A) Ã— P(A)] / [P(B|A) Ã— P(A) + P(B|Â¬A) Ã— P(Â¬A)]
```

**ìš©ì–´ ì„¤ëª…:**
- **P(A|B)**: ì‚¬í›„í™•ë¥ (Posterior) - Bê°€ ë°œìƒí–ˆì„ ë•Œ Aì˜ í™•ë¥ 
- **P(B|A)**: ìš°ë„(Likelihood) - Aê°€ ì°¸ì¼ ë•Œ Bê°€ ë°œìƒí•  í™•ë¥ 
- **P(A)**: ì‚¬ì „í™•ë¥ (Prior) - Aì˜ ê¸°ë³¸ í™•ë¥ 
- **P(B)**: ì¦ê±°(Evidence) - Bì˜ ì „ì²´ í™•ë¥ 

### 1.3 ìˆ˜ì¹˜ ì˜ˆì œ: ì§ˆë³‘ ì§„ë‹¨

**ë¬¸ì œ ì„¤ì •:**
- íŠ¹ì • ì§ˆë³‘ ìœ ë³‘ë¥ : P(ì§ˆë³‘) = 0.01 (1%)
- ê²€ì‚¬ ë¯¼ê°ë„: P(ì–‘ì„±|ì§ˆë³‘) = 0.95 (95%)
- ê²€ì‚¬ íŠ¹ì´ë„: P(ìŒì„±|ì •ìƒ) = 0.90 â†’ P(ì–‘ì„±|ì •ìƒ) = 0.10 (10%)

**ì§ˆë¬¸:** ê²€ì‚¬ ê²°ê³¼ê°€ ì–‘ì„±ì¼ ë•Œ, ì‹¤ì œë¡œ ì§ˆë³‘ì¼ í™•ë¥ ì€?

**í’€ì´:**
```
P(ì§ˆë³‘|ì–‘ì„±) = P(ì–‘ì„±|ì§ˆë³‘) Ã— P(ì§ˆë³‘) / P(ì–‘ì„±)

P(ì–‘ì„±) = P(ì–‘ì„±|ì§ˆë³‘) Ã— P(ì§ˆë³‘) + P(ì–‘ì„±|ì •ìƒ) Ã— P(ì •ìƒ)
        = 0.95 Ã— 0.01 + 0.10 Ã— 0.99
        = 0.0095 + 0.099
        = 0.1085

P(ì§ˆë³‘|ì–‘ì„±) = (0.95 Ã— 0.01) / 0.1085
             = 0.0095 / 0.1085
             = 0.0875 â‰ˆ 8.75%
```

**í•´ì„:** ì–‘ì„±ì´ì–´ë„ ì‹¤ì œ ì§ˆë³‘ì¼ í™•ë¥ ì€ ì•½ 8.75%ì…ë‹ˆë‹¤.

---

## 2. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (Naive Bayes)

### 2.1 ê¸°ë³¸ ê°œë…
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” **ë² ì´ì§€ì•ˆ ì •ë¦¬ë¥¼ ì‘ìš©í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ, ëª¨ë“  íŠ¹ì„±ì´ **ì¡°ê±´ë¶€ ë…ë¦½**ì´ë¼ëŠ” "ìˆœì§„í•œ(Naive)" ê°€ì •ì„ í•©ë‹ˆë‹¤.

### 2.2 ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ìˆ˜ì‹

**ë‹¤ì¤‘ íŠ¹ì„±ì˜ ê²½ìš°:**
```
P(C|Xâ‚, Xâ‚‚, ..., Xâ‚™) = P(Xâ‚, Xâ‚‚, ..., Xâ‚™|C) Ã— P(C) / P(Xâ‚, Xâ‚‚, ..., Xâ‚™)
```

**ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì • ì ìš©:**
```
P(C|Xâ‚, Xâ‚‚, ..., Xâ‚™) âˆ P(C) Ã— âˆáµ¢ P(Xáµ¢|C)

                     = P(C) Ã— P(Xâ‚|C) Ã— P(Xâ‚‚|C) Ã— ... Ã— P(Xâ‚™|C)
```

**ìµœì¢… ë¶„ë¥˜:**
```
Å· = argmax[P(C) Ã— âˆáµ¢ P(Xáµ¢|C)]
      C
```

### 2.3 ìˆ˜ì¹˜ ì˜ˆì œ: ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜

**ë°ì´í„°ì…‹:**

| ë©”ì¼ | ë‹¨ì–´1: "ë¬´ë£Œ" | ë‹¨ì–´2: "íšŒì˜" | ë‹¨ì–´3: "ëˆ" | í´ë˜ìŠ¤ |
|------|-------------|-------------|-----------|--------|
| 1    | 1           | 0           | 1         | ìŠ¤íŒ¸   |
| 2    | 1           | 0           | 1         | ìŠ¤íŒ¸   |
| 3    | 0           | 1           | 0         | ì •ìƒ   |
| 4    | 0           | 1           | 0         | ì •ìƒ   |
| 5    | 1           | 1           | 0         | ì •ìƒ   |

**1ë‹¨ê³„: ì‚¬ì „í™•ë¥  ê³„ì‚°**
```
P(ìŠ¤íŒ¸) = 2/5 = 0.4
P(ì •ìƒ) = 3/5 = 0.6
```

**2ë‹¨ê³„: ìš°ë„ ê³„ì‚°**

ìŠ¤íŒ¸ ë©”ì¼ì—ì„œ:
```
P(ë¬´ë£Œ=1|ìŠ¤íŒ¸) = 2/2 = 1.0
P(íšŒì˜=1|ìŠ¤íŒ¸) = 0/2 = 0.0 â†’ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: (0+1)/(2+2) = 0.25
P(ëˆ=1|ìŠ¤íŒ¸) = 2/2 = 1.0
```

ì •ìƒ ë©”ì¼ì—ì„œ:
```
P(ë¬´ë£Œ=1|ì •ìƒ) = 1/3 = 0.333
P(íšŒì˜=1|ì •ìƒ) = 2/3 = 0.667
P(ëˆ=1|ì •ìƒ) = 0/3 = 0.0 â†’ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: (0+1)/(3+2) = 0.2
```

**3ë‹¨ê³„: ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜**

ìƒˆ ë©”ì¼: "ë¬´ë£Œ", "íšŒì˜", "ëˆ" â†’ [1, 1, 1]

```
P(ìŠ¤íŒ¸|X) âˆ P(ìŠ¤íŒ¸) Ã— P(ë¬´ë£Œ=1|ìŠ¤íŒ¸) Ã— P(íšŒì˜=1|ìŠ¤íŒ¸) Ã— P(ëˆ=1|ìŠ¤íŒ¸)
          = 0.4 Ã— 1.0 Ã— 0.25 Ã— 1.0
          = 0.1

P(ì •ìƒ|X) âˆ P(ì •ìƒ) Ã— P(ë¬´ë£Œ=1|ì •ìƒ) Ã— P(íšŒì˜=1|ì •ìƒ) Ã— P(ëˆ=1|ì •ìƒ)
          = 0.6 Ã— 0.333 Ã— 0.667 Ã— 0.2
          = 0.0267
```

**ê²°ê³¼:** 0.1 > 0.0267 â†’ **ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜**

---

## 3. ë² ì´ì§€ì•ˆ ì •ë¦¬ vs ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¹„êµ

| êµ¬ë¶„ | ë² ì´ì§€ì•ˆ ì •ë¦¬ | ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ |
|------|--------------|--------------|
| **ì •ì˜** | í™•ë¥ ë¡ ì˜ ê¸°ë³¸ ì •ë¦¬ | ë² ì´ì§€ì•ˆ ì •ë¦¬ë¥¼ ì‘ìš©í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ |
| **ìš©ë„** | ì‚¬í›„í™•ë¥  ê³„ì‚° | ë¶„ë¥˜ ë¬¸ì œ í•´ê²° |
| **íŠ¹ì„± ìˆ˜** | ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì¼ ë˜ëŠ” ì†Œìˆ˜ | ë‹¤ìˆ˜ì˜ íŠ¹ì„± ì²˜ë¦¬ |
| **ë…ë¦½ì„± ê°€ì •** | ê°€ì • ì—†ìŒ | ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì • (í•µì‹¬!) |
| **ìˆ˜ì‹** | P(A\|B) = P(B\|A)P(A)/P(B) | P(C\|X) âˆ P(C)âˆP(Xáµ¢\|C) |
| **ê³„ì‚° ë³µì¡ë„** | íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ ê³ ë ¤ í•„ìš” | ê° íŠ¹ì„± ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚° |
| **ì¥ì ** | ì´ë¡ ì ìœ¼ë¡œ ì •í™• | ë¹ ë¥´ê³  íš¨ìœ¨ì  |
| **ë‹¨ì ** | ê³„ì‚° ë³µì¡ | ë…ë¦½ì„± ê°€ì •ì´ ë¹„í˜„ì‹¤ì ì¼ ìˆ˜ ìˆìŒ |

---

## 4. í•µì‹¬ ì°¨ì´ì 

### 4.1 ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •

**ë² ì´ì§€ì•ˆ ì •ë¦¬ (ì¼ë°˜ì ):**
```
P(C|Xâ‚, Xâ‚‚) = P(Xâ‚, Xâ‚‚|C) Ã— P(C) / P(Xâ‚, Xâ‚‚)
```
- Xâ‚ê³¼ Xâ‚‚ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•¨

**ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (ë…ë¦½ ê°€ì •):**
```
P(C|Xâ‚, Xâ‚‚) â‰ˆ P(Xâ‚|C) Ã— P(Xâ‚‚|C) Ã— P(C) / P(Xâ‚, Xâ‚‚)
```
- Xâ‚ê³¼ Xâ‚‚ê°€ Cê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë…ë¦½ì´ë¼ê³  ê°€ì •
- ê³„ì‚°ì´ í›¨ì”¬ ê°„ë‹¨í•´ì§

### 4.2 êµ¬ì²´ì  ì˜ˆì œ: ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²½ìš°

**í‚¤ì™€ ëª¸ë¬´ê²Œë¡œ ì„±ë³„ ì˜ˆì¸¡**

**ë² ì´ì§€ì•ˆ ì •ë¦¬ (ì •í™•í•œ ë°©ë²•):**
- í‚¤ì™€ ëª¸ë¬´ê²ŒëŠ” ìƒê´€ê´€ê³„ê°€ ë†’ìŒ
- P(í‚¤, ëª¸ë¬´ê²Œ|ë‚¨ì„±)ì„ ê²°í•©í™•ë¥ ë¶„í¬ë¡œ ëª¨ë¸ë§í•´ì•¼ í•¨

**ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (ê·¼ì‚¬ ë°©ë²•):**
- P(í‚¤|ë‚¨ì„±) Ã— P(ëª¸ë¬´ê²Œ|ë‚¨ì„±)ìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°
- ì‹¤ì œë¡œëŠ” ë¶€ì •í™•í•˜ì§€ë§Œ ì‹¤ìš©ì ìœ¼ë¡œ ì˜ ì‘ë™í•˜ëŠ” ê²½ìš° ë§ìŒ

---

## 5. Python êµ¬í˜„ ì˜ˆì œ

### 5.1 ì§ì ‘ êµ¬í˜„

```python
import numpy as np
from collections import defaultdict

class NaiveBayesClassifier:
    def __init__(self, alpha=1.0):
        """
        alpha: ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°
        """
        self.alpha = alpha
        self.class_probs = {}
        self.feature_probs = defaultdict(lambda: defaultdict(dict))
        
    def fit(self, X, y):
        """
        X: (n_samples, n_features) íŠ¹ì„± ë°°ì—´
        y: (n_samples,) í´ë˜ìŠ¤ ë ˆì´ë¸”
        """
        n_samples = len(y)
        self.classes = np.unique(y)
        
        # ì‚¬ì „í™•ë¥  ê³„ì‚°
        for c in self.classes:
            self.class_probs[c] = np.sum(y == c) / n_samples
        
        # ìš°ë„ ê³„ì‚° (ê° íŠ¹ì„±ë³„ë¡œ)
        for feature_idx in range(X.shape[1]):
            feature_values = np.unique(X[:, feature_idx])
            
            for c in self.classes:
                X_c = X[y == c]
                n_c = len(X_c)
                
                for value in feature_values:
                    # ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©
                    count = np.sum(X_c[:, feature_idx] == value)
                    prob = (count + self.alpha) / (n_c + self.alpha * len(feature_values))
                    self.feature_probs[feature_idx][value][c] = prob
    
    def predict_proba(self, X):
        """
        ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ê³„ì‚°
        """
        probs = []
        
        for x in X:
            class_scores = {}
            
            for c in self.classes:
                # ì‚¬ì „í™•ë¥ ë¡œ ì‹œì‘
                score = np.log(self.class_probs[c])
                
                # ê° íŠ¹ì„±ì˜ ìš°ë„ë¥¼ ê³±í•¨ (ë¡œê·¸ ê³µê°„ì—ì„œëŠ” ë”í•¨)
                for feature_idx, value in enumerate(x):
                    if value in self.feature_probs[feature_idx]:
                        prob = self.feature_probs[feature_idx][value].get(c, 1e-10)
                        score += np.log(prob)
                
                class_scores[c] = score
            
            probs.append(class_scores)
        
        return probs
    
    def predict(self, X):
        """
        ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡
        """
        probs = self.predict_proba(X)
        predictions = [max(p, key=p.get) for p in probs]
        return np.array(predictions)


# ì‚¬ìš© ì˜ˆì œ: ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜
X_train = np.array([
    [1, 0, 1],  # ë¬´ë£Œ, íšŒì˜, ëˆ
    [1, 0, 1],
    [0, 1, 0],
    [0, 1, 0],
    [1, 1, 0]
])

y_train = np.array(['ìŠ¤íŒ¸', 'ìŠ¤íŒ¸', 'ì •ìƒ', 'ì •ìƒ', 'ì •ìƒ'])

# ëª¨ë¸ í•™ìŠµ
nb = NaiveBayesClassifier(alpha=1.0)
nb.fit(X_train, y_train)

# ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
X_test = np.array([[1, 1, 1], [0, 1, 0]])
predictions = nb.predict(X_test)
probabilities = nb.predict_proba(X_test)

print("ì˜ˆì¸¡ ê²°ê³¼:", predictions)
print("\ní™•ë¥  ì ìˆ˜:")
for i, prob in enumerate(probabilities):
    print(f"ìƒ˜í”Œ {i+1}: {prob}")
```

### 5.2 scikit-learn ì‚¬ìš©

```python
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_features=20, 
                          n_informative=15, n_redundant=5, 
                          random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (ì—°ì†í˜• ë°ì´í„°)
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

print("ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì •í™•ë„:", accuracy_score(y_test, y_pred))
print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred))

# í™•ë¥  ì¶œë ¥
proba = gnb.predict_proba(X_test[:5])
print("\nìƒìœ„ 5ê°œ ìƒ˜í”Œì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ :")
print(proba)
```

---

## 6. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì¢…ë¥˜

### 6.1 MultinomialNB (ë‹¤í•­ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ)
- **ìš©ë„**: í…ìŠ¤íŠ¸ ë¶„ë¥˜, ë¬¸ì„œ ë¶„ë¥˜
- **íŠ¹ì§•**: ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê¸°ë°˜
- **ì˜ˆ**: ìŠ¤íŒ¸ í•„í„°, ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
- **ìˆ˜ì‹**: P(Xáµ¢|C) = (Náµ¢c + Î±) / (Nc + Î±|V|)

### 6.2 GaussianNB (ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ)
- **ìš©ë„**: ì—°ì†í˜• ë°ì´í„°
- **íŠ¹ì§•**: íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •
- **ìˆ˜ì‹**: 
```
P(Xáµ¢|C) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(Xáµ¢-Î¼)Â²/(2ÏƒÂ²))
```
- **íŒŒë¼ë¯¸í„°**:
  - Î¼: í´ë˜ìŠ¤ Cì—ì„œ íŠ¹ì„± Xáµ¢ì˜ í‰ê· 
  - ÏƒÂ²: í´ë˜ìŠ¤ Cì—ì„œ íŠ¹ì„± Xáµ¢ì˜ ë¶„ì‚°

### 6.3 BernoulliNB (ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ)
- **ìš©ë„**: ì´ì§„ íŠ¹ì„± ë°ì´í„°
- **íŠ¹ì§•**: ê° íŠ¹ì„±ì´ 0 ë˜ëŠ” 1
- **ì˜ˆ**: í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ì˜ ì¡´ì¬/ë¶€ì¬
- **ìˆ˜ì‹**: P(Xáµ¢|C) = p(Xáµ¢) if Xáµ¢=1 else (1-p(Xáµ¢))

---

## 7. ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (Laplace Smoothing)

### 7.1 í•„ìš”ì„±
í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” íŠ¹ì„±ê°’ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ë©´ í™•ë¥ ì´ 0ì´ ë˜ì–´ ì „ì²´ í™•ë¥ ì´ 0ì´ ë˜ëŠ” ë¬¸ì œ ë°œìƒ

### 7.2 ìˆ˜ì‹
```
P(Xáµ¢|C) = (count(Xáµ¢, C) + Î±) / (count(C) + Î± Ã— |V|)
```

**íŒŒë¼ë¯¸í„°:**
- Î±: ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„° (ì¼ë°˜ì ìœ¼ë¡œ 1)
- |V|: íŠ¹ì„±ì˜ ê°€ëŠ¥í•œ ê°’ì˜ ê°œìˆ˜
- count(Xáµ¢, C): í´ë˜ìŠ¤ Cì—ì„œ íŠ¹ì„±ê°’ Xáµ¢ê°€ ë‚˜íƒ€ë‚œ íšŸìˆ˜
- count(C): í´ë˜ìŠ¤ Cì˜ ì´ ìƒ˜í”Œ ìˆ˜

### 7.3 ì˜ˆì œ
```
# ìŠ¤ë¬´ë”© ì—†ì´
P(íšŒì˜=1|ìŠ¤íŒ¸) = 0/2 = 0  â†’ ë¬¸ì œ!

# ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (Î±=1, |V|=2)
P(íšŒì˜=1|ìŠ¤íŒ¸) = (0+1)/(2+1Ã—2) = 1/4 = 0.25  â†’ í•´ê²°!
```

---

## 8. ë¡œê·¸ í™•ë¥  (Log Probability)

### 8.1 í•„ìš”ì„±
í™•ë¥ ì„ ì—¬ëŸ¬ ë²ˆ ê³±í•˜ë©´ ì–¸ë”í”Œë¡œìš°(underflow) ë°œìƒ ê°€ëŠ¥

### 8.2 ë³€í™˜
```
P(C|X) = P(C) Ã— P(Xâ‚|C) Ã— P(Xâ‚‚|C) Ã— ... Ã— P(Xâ‚™|C)

log P(C|X) = log P(C) + log P(Xâ‚|C) + log P(Xâ‚‚|C) + ... + log P(Xâ‚™|C)
```

### 8.3 ì¥ì 
- ì–¸ë”í”Œë¡œìš° ë°©ì§€
- ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ ì¦ê°€
- ìˆ˜ì¹˜ì  ì•ˆì •ì„± í–¥ìƒ

---

## 9. ì¥ë‹¨ì  ë¶„ì„

### 9.1 ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ì¥ì 
1. **ë¹ ë¥¸ í•™ìŠµ ë° ì˜ˆì¸¡**: ê³„ì‚° ë³µì¡ë„ O(nÃ—d)
2. **ì ì€ í•™ìŠµ ë°ì´í„°**: ê° íŠ¹ì„±ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
3. **í™•ë¥ ì  ì˜ˆì¸¡**: í´ë˜ìŠ¤ë³„ í™•ë¥  ì œê³µ
4. **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜**: ìì—°ìŠ¤ëŸ½ê²Œ ì§€ì›
5. **ì˜¨ë¼ì¸ í•™ìŠµ**: ì¦ë¶„ í•™ìŠµ ê°€ëŠ¥
6. **í•´ì„ ê°€ëŠ¥ì„±**: ê° íŠ¹ì„±ì˜ ê¸°ì—¬ë„ íŒŒì•… ìš©ì´

### 9.2 ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ë‹¨ì 
1. **ë…ë¦½ì„± ê°€ì •**: í˜„ì‹¤ì ì´ì§€ ì•Šì€ ê°€ì •
2. **ì œë¡œ í™•ë¥  ë¬¸ì œ**: ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© í•„ìš”
3. **ìˆ˜ì¹˜í˜• íŠ¹ì„±**: ì´ì‚°í™” ë˜ëŠ” ë¶„í¬ ê°€ì • í•„ìš”
4. **íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„**: ë¬´ì‹œë¨

### 9.3 ì ìš© ì‚¬ë¡€
**ì í•©í•œ ê²½ìš°:**
- í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ìŠ¤íŒ¸ í•„í„°, ê°ì„± ë¶„ì„)
- ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ì´ í•„ìš”í•œ ê²½ìš°
- í•™ìŠµ ë°ì´í„°ê°€ ì ì€ ê²½ìš°

**ë¶€ì í•©í•œ ê²½ìš°:**
- íŠ¹ì„± ê°„ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²½ìš°
- ë†’ì€ ì •í™•ë„ê°€ í•„ìˆ˜ì¸ ê²½ìš°
- ë³µì¡í•œ íŒ¨í„´ ì¸ì‹

---

## 10. ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ì •ë¦¬

### âœ… ë°˜ë“œì‹œ ì•”ê¸°í•  ê³µì‹

1. **ë² ì´ì§€ì•ˆ ì •ë¦¬**
   ```
   P(A|B) = P(B|A) Ã— P(A) / P(B)
   ```

2. **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜**
   ```
   P(C|X) âˆ P(C) Ã— âˆ P(Xáµ¢|C)
   ```

3. **ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©**
   ```
   P(Xáµ¢|C) = (count(Xáµ¢, C) + Î±) / (count(C) + Î± Ã— |V|)
   ```

4. **ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ**
   ```
   P(Xáµ¢|C) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(Xáµ¢-Î¼)Â²/(2ÏƒÂ²))
   ```

### âœ… ìì£¼ ë‚˜ì˜¤ëŠ” ë¬¸ì œ ìœ í˜•

1. **í™•ë¥  ê³„ì‚° ë¬¸ì œ**
   - ì‚¬í›„í™•ë¥  ì§ì ‘ ê³„ì‚°
   - ë² ì´ì¦ˆ ì •ë¦¬ ì ìš©

2. **ë¶„ë¥˜ ë¬¸ì œ**
   - ìƒˆë¡œìš´ ìƒ˜í”Œì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡
   - í™•ë¥  ë¹„êµ

3. **ê°œë… ë¬¸ì œ**
   - ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •ì˜ ì˜ë¯¸
   - ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©ì˜ í•„ìš”ì„±

4. **ë¹„êµ ë¬¸ì œ**
   - ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì¢…ë¥˜ ë¹„êµ
   - ì¥ë‹¨ì  ì„œìˆ 

### âœ… ê³„ì‚° ìˆœì„œ

1. **ì‚¬ì „í™•ë¥  ê³„ì‚°**: P(C) = n_C / n_total
2. **ìš°ë„ ê³„ì‚°**: P(Xáµ¢|C) (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©)
3. **ì‚¬í›„í™•ë¥  ê³„ì‚°**: P(C|X) âˆ P(C) Ã— âˆP(Xáµ¢|C)
4. **ë¶„ë¥˜ ê²°ì •**: argmax P(C|X)

### âœ… ì£¼ì˜ì‚¬í•­

- **í™•ë¥  0 ì²˜ë¦¬**: ë°˜ë“œì‹œ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©
- **ì–¸ë”í”Œë¡œìš° ë°©ì§€**: ë¡œê·¸ í™•ë¥  ì‚¬ìš© ê³ ë ¤
- **ë…ë¦½ì„± ê°€ì •**: ì´ë¡ ê³¼ ì‹¤ì œì˜ ì°¨ì´ ì´í•´
- **ì •ê·œí™”**: ë¹„êµì‹œ ìƒëŒ€ì  í¬ê¸°ë§Œ ì¤‘ìš” (âˆ ê¸°í˜¸)

---

## 11. ì‹¤ì „ ì˜ˆì œ ë¬¸ì œ

### ë¬¸ì œ 1: ë² ì´ì§€ì•ˆ ì •ë¦¬

í•œ íšŒì‚¬ì˜ ì§ì› ì¤‘ 30%ê°€ ìš´ë™ì„ í•œë‹¤. ìš´ë™í•˜ëŠ” ì§ì› ì¤‘ 80%ê°€ ê±´ê°•í•˜ê³ , ìš´ë™í•˜ì§€ ì•ŠëŠ” ì§ì› ì¤‘ 40%ê°€ ê±´ê°•í•˜ë‹¤. í•œ ì§ì›ì´ ê±´ê°•í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ì„ ë•Œ, ì´ ì§ì›ì´ ìš´ë™í•  í™•ë¥ ì€?

**í’€ì´:**
```
P(ìš´ë™) = 0.3, P(Â¬ìš´ë™) = 0.7
P(ê±´ê°•|ìš´ë™) = 0.8, P(ê±´ê°•|Â¬ìš´ë™) = 0.4

P(ê±´ê°•) = P(ê±´ê°•|ìš´ë™)Ã—P(ìš´ë™) + P(ê±´ê°•|Â¬ìš´ë™)Ã—P(Â¬ìš´ë™)
        = 0.8Ã—0.3 + 0.4Ã—0.7
        = 0.24 + 0.28 = 0.52

P(ìš´ë™|ê±´ê°•) = P(ê±´ê°•|ìš´ë™) Ã— P(ìš´ë™) / P(ê±´ê°•)
             = 0.8 Ã— 0.3 / 0.52
             = 0.24 / 0.52
             = 0.4615 â‰ˆ 46.15%
```

### ë¬¸ì œ 2: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜

ë‹¤ìŒ í•™ìŠµ ë°ì´í„°ë¡œ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , [ë‚ ì”¨=ë§‘ìŒ, ì˜¨ë„=ë†’ìŒ]ì¼ ë•Œ ìš´ë™ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ì‹œì˜¤.

| ë‚ ì”¨ | ì˜¨ë„ | ìš´ë™ |
|------|------|------|
| ë§‘ìŒ | ë†’ìŒ | Yes  |
| ë§‘ìŒ | ë†’ìŒ | Yes  |
| íë¦¼ | ë†’ìŒ | Yes  |
| ë¹„   | ë‚®ìŒ | No   |
| ë¹„   | ë‚®ìŒ | No   |

**í’€ì´:**

1ë‹¨ê³„: ì‚¬ì „í™•ë¥ 
```
P(Yes) = 3/5 = 0.6
P(No) = 2/5 = 0.4
```

2ë‹¨ê³„: ìš°ë„ (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© Î±=1)
```
P(ë§‘ìŒ|Yes) = (2+1)/(3+3) = 3/6 = 0.5
P(ë†’ìŒ|Yes) = (3+1)/(3+2) = 4/5 = 0.8

P(ë§‘ìŒ|No) = (0+1)/(2+3) = 1/5 = 0.2
P(ë†’ìŒ|No) = (0+1)/(2+2) = 1/4 = 0.25
```

3ë‹¨ê³„: ì‚¬í›„í™•ë¥ 
```
P(Yes|ë§‘ìŒ,ë†’ìŒ) âˆ 0.6 Ã— 0.5 Ã— 0.8 = 0.24
P(No|ë§‘ìŒ,ë†’ìŒ) âˆ 0.4 Ã— 0.2 Ã— 0.25 = 0.02
```

**ê²°ê³¼:** 0.24 > 0.02 â†’ **ìš´ë™í•¨(Yes)**

---

## 12. ì°¸ê³  ìë£Œ

### ì¶”ê°€ í•™ìŠµ í‚¤ì›Œë“œ
- ë² ì´ì¦ˆ ì •ë¦¬ì˜ ì¦ëª…
- MAP (Maximum A Posteriori) vs MLE (Maximum Likelihood Estimation)
- ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬
- ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ í™•ë¥ ì  í•´ì„
- í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„

### scikit-learn ë¬¸ì„œ
- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)
- [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)

---

**ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬ ì‹œí—˜ ì¤€ë¹„ í™”ì´íŒ…! ğŸ“Šâœ¨**
