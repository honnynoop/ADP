# 의사결정나무(Decision Tree) 완벽 가이드
## rpart를 활용한 분류 분석

## 목차
1. [의사결정나무 개요](#1-의사결정나무-개요)
2. [의사결정나무 원리](#2-의사결정나무-원리)
3. [분할 기준](#3-분할-기준)
4. [Kyphosis 데이터 분석](#4-kyphosis-데이터-분석)
5. [단계별 계산 과정](#5-단계별-계산-과정)
6. [가지치기](#6-가지치기)
7. [Python 구현](#7-python-구현)
8. [시험 대비](#8-시험-대비)

---

## 1. 의사결정나무 개요

### 1.1 의사결정나무란?

**의사결정나무(Decision Tree)**는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내는 예측 모델입니다.

**특징:**
- 나무 구조로 의사결정 규칙 표현
- 해석이 쉽고 직관적
- 분류(Classification)와 회귀(Regression) 모두 가능

### 1.2 의사결정나무 구조

```
               [루트 노드]
              /           \
         [내부 노드]     [내부 노드]
         /      \         /      \
    [리프]   [리프]   [리프]   [리프]
```

| 구성 요소 | 설명 | 역할 |
|----------|------|------|
| **루트 노드 (Root Node)** | 최상위 노드 | 전체 데이터, 첫 번째 분할 |
| **내부 노드 (Internal Node)** | 중간 노드 | 분할 조건 포함 |
| **리프 노드 (Leaf Node)** | 끝 노드 | 최종 예측 결과 |
| **가지 (Branch)** | 노드 연결선 | 분할 경로 |
| **깊이 (Depth)** | 루트부터 리프까지 거리 | 트리 복잡도 |

### 1.3 의사결정나무 알고리즘 종류

| 알고리즘 | 분할 기준 | 특징 | 용도 |
|---------|----------|------|------|
| **CART** | 지니지수 | 이진 분할 | 분류, 회귀 |
| **ID3** | 정보이득 | 다진 분할 | 분류 |
| **C4.5** | 정보이득 비율 | ID3 개선 | 분류 |
| **C5.0** | - | C4.5 개선 | 분류 |
| **CHAID** | 카이제곱 | 통계적 검정 | 분류 |

**rpart는 CART 알고리즘을 사용합니다.**

### 1.4 의사결정나무 장단점

**장점:**
- 해석이 쉽고 이해하기 직관적
- 전처리 작업이 적음 (정규화 불필요)
- 범주형/연속형 변수 모두 처리 가능
- 비선형 관계 포착 가능
- 변수 중요도 파악 가능

**단점:**
- 과적합(Overfitting) 가능성
- 작은 데이터 변화에 민감
- 편향된 트리 생성 가능
- 연속형 변수에 약함 (계단 형태 예측)

---

## 2. 의사결정나무 원리

### 2.1 트리 구축 과정

```
1단계: 루트 노드에서 시작 (전체 데이터)
   ↓
2단계: 최적 분할 변수와 분할점 찾기
   ↓
3단계: 데이터를 두 개 이상의 부분집합으로 분할
   ↓
4단계: 각 부분집합에 대해 2-3단계 반복 (재귀)
   ↓
5단계: 중지 조건 만족 시 리프 노드 생성
   ↓
6단계: 가지치기 (Pruning)
```

### 2.2 분할의 원리

**핵심 아이디어:** 
> 데이터를 더 순수한(homogeneous) 부분집합으로 나누기

**예시:**

```
초기 데이터 (100개):
클래스 A: 60개 (60%)
클래스 B: 40개 (40%)
→ 불순도 높음 (섞여있음)

분할 후:
왼쪽: A=50, B=10 (83% vs 17%) → 순수도 증가
오른쪽: A=10, B=30 (25% vs 75%) → 순수도 증가
```

### 2.3 재귀적 분할 (Recursive Partitioning)

```python
def build_tree(data, depth):
    # 중지 조건 확인
    if stopping_condition(data, depth):
        return create_leaf(data)
    
    # 최적 분할 찾기
    best_feature, best_threshold = find_best_split(data)
    
    # 데이터 분할
    left_data = data[data[best_feature] <= best_threshold]
    right_data = data[data[best_feature] > best_threshold]
    
    # 재귀 호출
    left_child = build_tree(left_data, depth + 1)
    right_child = build_tree(right_data, depth + 1)
    
    return Node(best_feature, best_threshold, left_child, right_child)
```

### 2.4 중지 조건 (Stopping Criteria)

| 조건 | 설명 |
|------|------|
| **최대 깊이** | depth ≥ max_depth |
| **최소 샘플 수** | n_samples < min_samples_split |
| **최소 리프 샘플** | leaf_samples < min_samples_leaf |
| **순수 노드** | 모든 샘플이 같은 클래스 |
| **불순도 감소** | 불순도 감소량 < threshold |

---

## 3. 분할 기준

### 3.1 지니 지수 (Gini Index)

#### 이론

**지니 지수**는 불순도(impurity)를 측정하는 지표입니다.

**공식:**
```
Gini(t) = 1 - Σ[p(i|t)]²

여기서:
- p(i|t): 노드 t에서 클래스 i의 비율
- k: 클래스 개수
```

**특징:**
- 범위: 0 ≤ Gini ≤ 0.5 (이진 분류)
- Gini = 0: 완전히 순수 (모두 같은 클래스)
- Gini = 0.5: 완전히 불순 (클래스가 균등)

#### 계산 예제 1: 지니 지수

**데이터:**

| 노드 | 클래스 A | 클래스 B | 전체 |
|------|---------|---------|------|
| 전체 | 60 | 40 | 100 |

**계산:**
```
p(A) = 60/100 = 0.6
p(B) = 40/100 = 0.4

Gini = 1 - [p(A)² + p(B)²]
     = 1 - [0.6² + 0.4²]
     = 1 - [0.36 + 0.16]
     = 1 - 0.52
     = 0.48
```

#### 계산 예제 2: 분할 후 지니 지수

**분할 후 데이터:**

| 노드 | 클래스 A | 클래스 B | 전체 |
|------|---------|---------|------|
| 왼쪽 | 50 | 10 | 60 |
| 오른쪽 | 10 | 30 | 40 |

**왼쪽 노드:**
```
p(A) = 50/60 = 0.833
p(B) = 10/60 = 0.167

Gini_left = 1 - [0.833² + 0.167²]
          = 1 - [0.694 + 0.028]
          = 1 - 0.722
          = 0.278
```

**오른쪽 노드:**
```
p(A) = 10/40 = 0.25
p(B) = 30/40 = 0.75

Gini_right = 1 - [0.25² + 0.75²]
           = 1 - [0.0625 + 0.5625]
           = 1 - 0.625
           = 0.375
```

**가중 평균 지니 지수:**
```
Gini_split = (60/100) × 0.278 + (40/100) × 0.375
           = 0.1668 + 0.15
           = 0.3168
```

**지니 감소량:**
```
Δ Gini = 0.48 - 0.3168 = 0.1632
```
→ 이 분할은 불순도를 **0.1632 감소**시킵니다!

### 3.2 엔트로피 (Entropy)

#### 이론

**엔트로피**는 정보 이론에 기반한 불순도 측정 지표입니다.

**공식:**
```
Entropy(t) = -Σ p(i|t) × log₂(p(i|t))
```

**특징:**
- 범위: 0 ≤ Entropy ≤ log₂(k)
- 이진 분류: 0 ≤ Entropy ≤ 1
- Entropy = 0: 완전 순수
- Entropy = 1: 완전 불순 (이진, 50:50)

#### 계산 예제: 엔트로피

**데이터:** 클래스 A=60, B=40

```
p(A) = 0.6, p(B) = 0.4

Entropy = -[p(A)log₂(p(A)) + p(B)log₂(p(B))]
        = -[0.6 × log₂(0.6) + 0.4 × log₂(0.4)]
        = -[0.6 × (-0.737) + 0.4 × (-1.322)]
        = -[-0.442 + (-0.529)]
        = -(-0.971)
        = 0.971
```

### 3.3 정보 이득 (Information Gain)

**정보 이득**은 분할 전후의 엔트로피 감소량입니다.

**공식:**
```
IG(S, A) = Entropy(S) - Σ (|Sv|/|S|) × Entropy(Sv)

여기서:
- S: 부모 노드
- A: 분할 속성
- Sv: 속성 A로 분할된 자식 노드 v
```

#### 계산 예제: 정보 이득

**분할 후:**
- 왼쪽: A=50, B=10 (60개)
- 오른쪽: A=10, B=30 (40개)

```
Entropy_left = -[0.833×log₂(0.833) + 0.167×log₂(0.167)]
             = -[0.833×(-0.264) + 0.167×(-2.585)]
             = -[-0.220 + (-0.432)]
             = 0.652

Entropy_right = -[0.25×log₂(0.25) + 0.75×log₂(0.75)]
              = -[0.25×(-2) + 0.75×(-0.415)]
              = -[-0.5 + (-0.311)]
              = 0.811

Weighted_Entropy = (60/100)×0.652 + (40/100)×0.811
                 = 0.391 + 0.324
                 = 0.715

IG = 0.971 - 0.715 = 0.256
```

### 3.4 분할 기준 비교

| 측도 | 범위 (이진) | 완전 순수 | 완전 불순 | 특징 |
|------|-----------|----------|----------|------|
| **지니** | 0 ~ 0.5 | 0 | 0.5 | 계산 빠름 |
| **엔트로피** | 0 ~ 1 | 0 | 1 | 이론적 배경 |
| **정보이득** | - | 높을수록 좋음 | - | 다진 분할 |

**실무에서:**
- CART (지니) vs ID3/C4.5 (엔트로피)
- 결과는 대체로 비슷
- 지니가 계산상 약간 더 빠름

---

## 4. Kyphosis 데이터 분석

### 4.1 Kyphosis 데이터셋 소개

**Kyphosis**는 척추 측만증 수술 후 기형(kyphosis) 발생 여부를 예측하는 데이터입니다.

**변수 설명:**

| 변수 | 타입 | 설명 | 범위 |
|------|------|------|------|
| **Kyphosis** | 범주형 (목적) | 기형 발생 여부 | absent, present |
| **Age** | 연속형 (독립) | 환자 나이 (개월) | 1 ~ 206 |
| **Number** | 연속형 (독립) | 수술한 척추뼈 개수 | 2 ~ 10 |
| **Start** | 연속형 (독립) | 수술 시작 척추뼈 번호 | 1 ~ 18 |

**데이터 크기:** 81개 관측치

### 4.2 R 코드: 기본 분석

```r
# 패키지 로드
library(rpart)
library(rpart.plot)

# 데이터 확인
data(kyphosis)
head(kyphosis)
```

**출력:**
```
  Kyphosis Age Number Start
1   absent  71      3     5
2   absent 158      3    14
3  present 128      4     5
4   absent   2      5     1
5   absent   1      4    15
6   absent   1      2    16
```

```r
# 데이터 구조
str(kyphosis)
```

**출력:**
```
'data.frame':	81 obs. of  4 variables:
 $ Kyphosis: Factor w/ 2 levels "absent","present": 1 1 2 1 1 1 1 1 1 2 ...
 $ Age     : int  71 158 128 2 1 1 61 37 113 59 ...
 $ Number  : int  3 3 4 5 4 2 2 3 2 6 ...
 $ Start   : int  5 14 5 1 15 16 17 16 16 12 ...
```

```r
# 기초 통계
summary(kyphosis)
```

**출력:**
```
   Kyphosis       Age            Number          Start      
 absent :64   Min.   :  1.0   Min.   : 2.00   Min.   : 1.00  
 present:17   1st Qu.: 26.0   1st Qu.: 3.00   1st Qu.: 9.00  
              Median : 87.0   Median : 4.00   Median :13.00  
              Mean   : 83.7   Mean   : 4.05   Mean   :11.49  
              3rd Qu.:130.0   3rd Qu.: 5.00   3rd Qu.:16.00  
              Max.   :206.0   Max.   :10.00   Max.   :18.00
```

**데이터 분포:**
- absent (정상): 64건 (79%)
- present (기형): 17건 (21%)
- 불균형 데이터

### 4.3 R 코드: 의사결정나무 생성

```r
# 의사결정나무 모델 생성
tree_model <- rpart(Kyphosis ~ Age + Number + Start, 
                    data = kyphosis,
                    method = "class")

# 모델 요약
print(tree_model)
```

**출력:**
```
n= 81 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 81 17 absent (0.79012346 0.20987654)  
  2) Start>=8.5 62  6 absent (0.90322581 0.09677419)  
    4) Start>=14.5 29  0 absent (1.00000000 0.00000000) *
    5) Start< 14.5 33  6 absent (0.81818182 0.18181818)  
     10) Age< 55 12  0 absent (1.00000000 0.00000000) *
     11) Age>=55 21  6 absent (0.71428571 0.28571429)  
       22) Age>=111 14  2 absent (0.85714286 0.14285714) *
       23) Age< 111 7  3 present (0.42857143 0.57142857) *
  3) Start< 8.5 19  8 present (0.42105263 0.57894737)  
    6) Age< 39.5 9  2 absent (0.77777778 0.22222222) *
    7) Age>=39.5 10  1 present (0.10000000 0.90000000) *
```

### 4.4 모델 해석

**트리 구조 해석:**

```
노드 1 (루트): 전체 81개
├─ 조건: Start >= 8.5 (62개) → absent 예측
│  ├─ 조건: Start >= 14.5 (29개) → absent (순수)
│  └─ 조건: Start < 14.5 (33개)
│     ├─ 조건: Age < 55 (12개) → absent (순수)
│     └─ 조건: Age >= 55 (21개)
│        ├─ Age >= 111 (14개) → absent
│        └─ Age < 111 (7개) → present
│
└─ 조건: Start < 8.5 (19개) → present 예측
   ├─ 조건: Age < 39.5 (9개) → absent
   └─ 조건: Age >= 39.5 (10개) → present
```

**주요 분할 규칙:**

| 노드 | 분할 변수 | 분할점 | 의미 |
|------|----------|--------|------|
| 1 | Start | 8.5 | 첫 분할, 가장 중요 |
| 2 | Start | 14.5 | 높은 시작점 |
| 5 | Age | 55 | 나이 기준 |
| 3 | Age | 39.5 | 낮은 시작점+나이 |

### 4.5 R 코드: 트리 시각화

```r
# 기본 플롯
plot(tree_model)
text(tree_model, use.n = TRUE, cex = 0.8)
```

```r
# rpart.plot 사용 (더 예쁜 시각화)
rpart.plot(tree_model, 
           type = 4,           # 노드 타입
           extra = 101,        # absent/present 개수 표시
           fallen.leaves = TRUE,
           main = "Kyphosis Decision Tree")
```

```r
# 자세한 플롯
rpart.plot(tree_model,
           box.palette = "RdYlGn",  # 색상
           shadow.col = "gray",      # 그림자
           nn = TRUE)                # 노드 번호 표시
```

### 4.6 R 코드: 모델 평가

```r
# 예측
predictions <- predict(tree_model, kyphosis, type = "class")

# 혼동 행렬
table(Actual = kyphosis$Kyphosis, Predicted = predictions)
```

**출력:**
```
         Predicted
Actual    absent present
  absent      61       3
  present      8       9
```

```r
# 정확도
accuracy <- sum(predictions == kyphosis$Kyphosis) / nrow(kyphosis)
print(paste("Accuracy:", round(accuracy, 4)))
```

**출력:**
```
[1] "Accuracy: 0.8642"
```

**성능 지표:**

| 지표 | 값 | 계산 |
|------|-----|------|
| 정확도 (Accuracy) | 86.42% | (61+9)/81 |
| absent 정확도 | 95.31% | 61/64 |
| present 정확도 | 52.94% | 9/17 |

### 4.7 R 코드: 변수 중요도

```r
# 변수 중요도
importance <- tree_model$variable.importance
print(importance)
```

**출력:**
```
    Start       Age    Number 
26.235294 18.529412  5.058824
```

**해석:**
- **Start**가 가장 중요한 변수 (26.24)
- **Age**가 두 번째로 중요 (18.53)
- **Number**는 상대적으로 덜 중요 (5.06)

```r
# 시각화
barplot(importance, 
        main = "Variable Importance",
        col = "steelblue",
        las = 2)
```

---

## 5. 단계별 계산 과정

### 5.1 초기 상태 (루트 노드)

**데이터:**
- 전체: 81개
- absent: 64개 (79.0%)
- present: 17개 (21.0%)

**지니 지수:**
```
p(absent) = 64/81 = 0.790
p(present) = 17/81 = 0.210

Gini_root = 1 - [0.790² + 0.210²]
          = 1 - [0.624 + 0.044]
          = 1 - 0.668
          = 0.332
```

### 5.2 첫 번째 분할: Start 변수

**후보 분할점 탐색:**

Start 변수의 가능한 모든 분할점에 대해 지니 지수 계산

**최적 분할: Start < 8.5 vs Start >= 8.5**

**분할 결과:**

| 조건 | 개수 | absent | present | 비율 |
|------|------|--------|---------|------|
| Start < 8.5 | 19 | 8 | 11 | 42% / 58% |
| Start >= 8.5 | 62 | 56 | 6 | 90% / 10% |

**왼쪽 노드 (Start < 8.5):**
```
p(absent) = 8/19 = 0.421
p(present) = 11/19 = 0.579

Gini_left = 1 - [0.421² + 0.579²]
          = 1 - [0.177 + 0.335]
          = 1 - 0.512
          = 0.488
```

**오른쪽 노드 (Start >= 8.5):**
```
p(absent) = 56/62 = 0.903
p(present) = 6/62 = 0.097

Gini_right = 1 - [0.903² + 0.097²]
           = 1 - [0.815 + 0.009]
           = 1 - 0.824
           = 0.176
```

**가중 평균 지니:**
```
Gini_split = (19/81) × 0.488 + (62/81) × 0.176
           = 0.234 × 0.488 + 0.765 × 0.176
           = 0.114 + 0.135
           = 0.249
```

**지니 감소량:**
```
Δ Gini = 0.332 - 0.249 = 0.083
```

### 5.3 두 번째 분할: 오른쪽 노드 (Start >= 8.5)

**현재 상태:**
- 개수: 62개
- absent: 56개, present: 6개
- Gini = 0.176

**최적 분할: Start < 14.5 vs Start >= 14.5**

**분할 결과:**

| 조건 | 개수 | absent | present |
|------|------|--------|---------|
| Start >= 14.5 | 29 | 29 | 0 |
| Start < 14.5 | 33 | 27 | 6 |

**노드 4 (Start >= 14.5):**
```
모두 absent → 순수 노드 → 리프 노드
Gini = 0
```

**노드 5 (Start < 14.5):**
```
p(absent) = 27/33 = 0.818
p(present) = 6/33 = 0.182

Gini = 1 - [0.818² + 0.182²]
     = 1 - [0.669 + 0.033]
     = 0.298
```

### 5.4 세 번째 분할: 노드 5 (Age)

**최적 분할: Age < 55 vs Age >= 55**

**분할 결과:**

| 조건 | 개수 | absent | present |
|------|------|--------|---------|
| Age < 55 | 12 | 12 | 0 |
| Age >= 55 | 21 | 15 | 6 |

**노드 10 (Age < 55):**
```
모두 absent → 리프 노드
```

**노드 11 (Age >= 55):**
```
p(absent) = 15/21 = 0.714
p(present) = 6/21 = 0.286

Gini = 1 - [0.714² + 0.286²]
     = 0.408
```

### 5.5 완전한 트리 구조

```
                    [1] Root (81)
                    absent: 64, present: 17
                    Gini: 0.332
                         |
           Start >= 8.5? ┴ Start < 8.5?
                /                    \
               /                      \
        [2] (62)                    [3] (19)
     absent: 56                   absent: 8
     present: 6                   present: 11
     Gini: 0.176                  Gini: 0.488
         |                            |
   Start >= 14.5?              Age >= 39.5?
      /      \                    /      \
     /        \                  /        \
[4] (29)    [5] (33)        [6] (9)    [7] (10)
absent:29   absent:27       absent:7   absent:1
present:0   present:6       present:2  present:9
Gini:0      Gini:0.298      *Leaf*     *Leaf*
*Leaf*          |
          Age >= 55?
            /      \
           /        \
     [10] (12)    [11] (21)
     absent:12    absent:15
     present:0    present:6
     Gini:0       Gini:0.408
     *Leaf*           |
               Age >= 111?
                  /      \
                 /        \
            [22] (14)   [23] (7)
            absent:12   absent:3
            present:2   present:4
            *Leaf*      *Leaf*
```

### 5.6 분할 요약 표

| 노드 | 분할 변수 | 분할점 | Gini (전) | Gini (후) | Δ Gini | 샘플 수 |
|------|----------|--------|-----------|-----------|--------|---------|
| 1 | Start | 8.5 | 0.332 | 0.249 | 0.083 | 81 |
| 2 | Start | 14.5 | 0.176 | 0.165 | 0.011 | 62 |
| 5 | Age | 55 | 0.298 | 0.235 | 0.063 | 33 |
| 3 | Age | 39.5 | 0.488 | 0.358 | 0.130 | 19 |
| 11 | Age | 111 | 0.408 | 0.286 | 0.122 | 21 |

---

## 6. 가지치기 (Pruning)

### 6.1 가지치기의 필요성

**과적합(Overfitting) 문제:**
- 훈련 데이터에 너무 맞춤
- 일반화 성능 저하
- 노이즈까지 학습

**해결책: 가지치기**
- 불필요한 가지 제거
- 모델 단순화
- 일반화 성능 향상

### 6.2 가지치기 방법

**1. 사전 가지치기 (Pre-pruning)**
- 트리 성장 중 중단
- 조건: max_depth, min_samples_split 등

**2. 사후 가지치기 (Post-pruning)**
- 완전한 트리 생성 후 가지치기
- CART의 Cost-Complexity Pruning 사용

### 6.3 Cost-Complexity Pruning

**비용 복잡도 (Cost-Complexity):**
```
R_α(T) = R(T) + α|T|

여기서:
- R(T): 재대입 오류 (Resubstitution Error)
- |T|: 리프 노드 개수
- α: 복잡도 패널티 (≥ 0)
```

**α의 역할:**
- α = 0: 전체 트리 (과적합)
- α 증가: 더 많이 가지치기
- α = ∞: 루트만 남김

### 6.4 R 코드: 가지치기

```r
# CP (Complexity Parameter) 테이블 확인
printcp(tree_model)
```

**출력:**
```
Classification tree:
rpart(formula = Kyphosis ~ Age + Number + Start, data = kyphosis, 
    method = "class")

Variables actually used in tree construction:
[1] Age   Start

Root node error: 17/81 = 0.20988

n= 81 

        CP nsplit rel error  xerror     xstd
1 0.176471      0   1.00000 1.00000 0.215647
2 0.019608      1   0.82353 1.17647 0.227729
3 0.010000      4   0.76471 1.11765 0.224433
```

**CP 테이블 해석:**

| CP | nsplit | rel error | xerror | 의미 |
|----|--------|-----------|--------|------|
| 0.176 | 0 | 1.000 | 1.000 | 루트 노드 |
| 0.020 | 1 | 0.824 | 1.176 | 첫 분할 후 |
| 0.010 | 4 | 0.765 | 1.118 | 4번 분할 후 |

```r
# CP 값에 따른 xerror 시각화
plotcp(tree_model)
```

```r
# 최적 CP 찾기
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
print(paste("Optimal CP:", optimal_cp))
```

```r
# 가지치기 수행
pruned_tree <- prune(tree_model, cp = optimal_cp)

# 가지치기된 트리 시각화
rpart.plot(pruned_tree, 
           main = "Pruned Decision Tree")
```

### 6.5 교차검증 (Cross-Validation)

```r
# 10-겹 교차검증으로 트리 생성
set.seed(123)
tree_cv <- rpart(Kyphosis ~ Age + Number + Start,
                 data = kyphosis,
                 method = "class",
                 control = rpart.control(cp = 0, xval = 10))

# 최적 CP 선택
optimal_cp <- tree_cv$cptable[which.min(tree_cv$cptable[,"xerror"]), "CP"]

# 최종 모델
final_tree <- prune(tree_cv, cp = optimal_cp)
```

---

## 7. Python 구현

### 7.1 scikit-learn 구현

```python
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# 데이터 로드 (R의 kyphosis 데이터셋)
# 실제로는 CSV 파일 등으로 로드
data = {
    'Age': [71, 158, 128, 2, 1, 1, 61, 37, 113, 59, 82, 148, 
            18, 1, 168, 1, 78, 175, 80, 27, 22, 105, 96, 131,
            15, 9, 8, 100, 4, 151, 31, 125, 130, 112, 140, 93,
            1, 52, 20, 91, 73, 35, 143, 61, 97, 139, 136, 131,
            121, 177, 68, 9, 139, 2, 140, 72, 2, 120, 51, 102,
            130, 114, 81, 118, 118, 17, 195, 159, 18, 15, 158,
            127, 87, 206, 11, 178, 157, 26, 120, 42, 36],
    'Number': [3, 3, 4, 5, 4, 2, 2, 3, 2, 6, 5, 3, 5, 4, 3, 3,
               6, 5, 5, 4, 2, 6, 3, 2, 7, 5, 4, 3, 3, 4, 4, 2,
               4, 3, 2, 4, 3, 2, 5, 5, 3, 3, 9, 4, 3, 4, 3, 5,
               3, 2, 5, 5, 3, 7, 4, 4, 4, 3, 2, 2, 3, 2, 3, 3,
               4, 4, 3, 3, 4, 2, 5, 4, 4, 7, 3, 6, 4, 10, 6, 7, 7],
    'Start': [5, 14, 5, 1, 15, 16, 17, 16, 16, 12, 14, 16, 16, 15,
              15, 16, 15, 13, 16, 9, 16, 5, 12, 3, 13, 13, 13, 8,
              6, 11, 16, 11, 8, 8, 11, 12, 11, 5, 13, 14, 1, 13,
              3, 1, 16, 6, 15, 13, 1, 14, 15, 16, 9, 16, 1, 16,
              15, 8, 9, 11, 1, 8, 12, 14, 14, 16, 10, 13, 11, 16,
              13, 1, 5, 10, 6, 11, 11, 12, 14, 6, 13],
    'Kyphosis': ['absent', 'absent', 'present', 'absent', 'absent',
                 'absent', 'absent', 'absent', 'absent', 'present',
                 'absent', 'absent', 'present', 'absent', 'absent',
                 'absent', 'present', 'absent', 'absent', 'absent',
                 'absent', 'present', 'absent', 'absent', 'present',
                 'present', 'absent', 'absent', 'absent', 'absent',
                 'absent', 'absent', 'present', 'absent', 'absent',
                 'absent', 'absent', 'present', 'absent', 'absent',
                 'present', 'absent', 'present', 'present', 'absent',
                 'present', 'absent', 'absent', 'present', 'absent',
                 'absent', 'absent', 'present', 'absent', 'present',
                 'absent', 'absent', 'absent', 'present', 'absent',
                 'present', 'absent', 'present', 'absent', 'absent',
                 'absent', 'absent', 'absent', 'absent', 'absent',
                 'absent', 'present', 'absent', 'absent', 'absent',
                 'absent', 'absent', 'absent', 'absent', 'absent',
                 'absent']
}

df = pd.DataFrame(data)

# 데이터 확인
print(df.head())
print("\n데이터 요약:")
print(df.describe())
print("\n클래스 분포:")
print(df['Kyphosis'].value_counts())

# 특성과 타겟 분리
X = df[['Age', 'Number', 'Start']]
y = df['Kyphosis']

# 학습/테스트 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 의사결정나무 모델 (CART)
dt_model = DecisionTreeClassifier(
    criterion='gini',      # 지니 지수 사용
    max_depth=None,        # 제한 없음
    min_samples_split=2,   # 최소 분할 샘플
    min_samples_leaf=1,    # 최소 리프 샘플
    random_state=42
)

# 모델 학습
dt_model.fit(X_train, y_train)

# 예측
y_pred = dt_model.predict(X_test)

# 평가
print("\n=== 성능 평가 ===")
print("혼동 행렬:")
print(confusion_matrix(y_test, y_pred))
print("\n분류 리포트:")
print(classification_report(y_test, y_pred))

# 특성 중요도
print("\n=== 변수 중요도 ===")
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': dt_model.feature_importances_
}).sort_values('Importance', ascending=False)
print(feature_importance)
```

### 7.2 트리 시각화

```python
# 트리 시각화
plt.figure(figsize=(20, 10))
plot_tree(dt_model, 
          feature_names=X.columns,
          class_names=['absent', 'present'],
          filled=True,
          rounded=True,
          fontsize=10)
plt.title("Kyphosis Decision Tree", fontsize=16)
plt.tight_layout()
plt.savefig('decision_tree.png', dpi=300, bbox_inches='tight')
plt.show()
```

### 7.3 가지치기 (Cost Complexity Pruning)

```python
from sklearn.model_selection import cross_val_score

# 가지치기 경로 찾기
path = dt_model.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas
impurities = path.impurities

# 다양한 alpha 값으로 모델 학습
models = []
for ccp_alpha in ccp_alphas:
    model = DecisionTreeClassifier(
        criterion='gini',
        ccp_alpha=ccp_alpha,
        random_state=42
    )
    model.fit(X_train, y_train)
    models.append(model)

# 교차검증 점수
train_scores = [model.score(X_train, y_train) for model in models]
test_scores = [model.score(X_test, y_test) for model in models]

# 시각화
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(ccp_alphas, train_scores, marker='o', label='Train', drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='s', label='Test', drawstyle="steps-post")
ax.set_xlabel('Alpha (Complexity Parameter)')
ax.set_ylabel('Accuracy')
ax.set_title('Accuracy vs Alpha for Training and Testing sets')
ax.legend()
plt.grid(True)
plt.show()

# 최적 alpha 찾기
optimal_idx = np.argmax(test_scores)
optimal_alpha = ccp_alphas[optimal_idx]
print(f"최적 Alpha: {optimal_alpha:.6f}")
print(f"최적 테스트 정확도: {test_scores[optimal_idx]:.4f}")

# 최적 모델
best_model = DecisionTreeClassifier(
    criterion='gini',
    ccp_alpha=optimal_alpha,
    random_state=42
)
best_model.fit(X_train, y_train)

# 최적 모델 평가
print("\n=== 최적 모델 성능 ===")
print(f"Train Accuracy: {best_model.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {best_model.score(X_test, y_test):.4f}")
```

### 7.4 수동 지니 계산

```python
def calculate_gini(y):
    """지니 지수 계산"""
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    gini = 1 - np.sum(probabilities ** 2)
    return gini

def gini_split(X, y, feature, threshold):
    """분할 후 가중 지니 지수 계산"""
    # 분할
    left_mask = X[feature] <= threshold
    right_mask = ~left_mask
    
    left_y = y[left_mask]
    right_y = y[right_mask]
    
    # 가중 평균
    n = len(y)
    n_left = len(left_y)
    n_right = len(right_y)
    
    gini_left = calculate_gini(left_y)
    gini_right = calculate_gini(right_y)
    
    weighted_gini = (n_left / n) * gini_left + (n_right / n) * gini_right
    
    return weighted_gini, gini_left, gini_right

# 예제: Start 변수로 분할
print("=== 지니 지수 계산 예제 ===")
print(f"전체 지니: {calculate_gini(y_train):.4f}")

threshold = 8.5
weighted_gini, gini_left, gini_right = gini_split(
    X_train, y_train, 'Start', threshold
)

print(f"\nStart <= {threshold} 분할:")
print(f"  왼쪽 지니: {gini_left:.4f}")
print(f"  오른쪽 지니: {gini_right:.4f}")
print(f"  가중 지니: {weighted_gini:.4f}")
print(f"  지니 감소: {calculate_gini(y_train) - weighted_gini:.4f}")
```

---

## 8. 시험 대비

### 8.1 핵심 개념 정리

| 개념 | 설명 | 시험 출제 |
|------|------|----------|
| **지니 지수** | 불순도 측정 (0~0.5) | ★★★★★ |
| **엔트로피** | 정보 이론 기반 불순도 | ★★★★ |
| **정보 이득** | 엔트로피 감소량 | ★★★★ |
| **분할 기준** | 최대 지니 감소 | ★★★★★ |
| **가지치기** | 과적합 방지 | ★★★ |
| **CART** | 이진 분할 알고리즘 | ★★★★ |

### 8.2 공식 암기

```
┌─────────────────────────────────────┐
│ 지니 지수 (Gini Index)               │
│ Gini = 1 - Σ p²ᵢ                   │
│                                     │
│ 엔트로피 (Entropy)                   │
│ H = -Σ pᵢ log₂(pᵢ)                 │
│                                     │
│ 정보 이득 (Information Gain)         │
│ IG = H(parent) - Σ(nⱼ/n)H(childⱼ) │
│                                     │
│ 가중 지니                            │
│ Gini_split = Σ(nⱼ/n) × Gini(j)    │
└─────────────────────────────────────┘
```

### 8.3 계산 문제 풀이 팁

**지니 지수 계산:**
1. 각 클래스 비율 구하기
2. 비율 제곱하여 합산
3. 1에서 빼기

**분할 평가:**
1. 각 자식 노드의 지니 계산
2. 샘플 비율로 가중 평균
3. 감소량 = 부모 지니 - 가중 평균 지니

### 8.4 자주 나오는 문제 유형

**유형 1: 지니 지수 계산**

```
문제: 다음 노드의 지니 지수를 구하시오.
- 클래스 A: 40개
- 클래스 B: 60개

풀이:
p(A) = 40/100 = 0.4
p(B) = 60/100 = 0.6
Gini = 1 - (0.4² + 0.6²)
     = 1 - (0.16 + 0.36)
     = 1 - 0.52 = 0.48
```

**유형 2: 최적 분할 찾기**

```
문제: 어느 분할이 더 좋은가?

분할 A: Gini = 0.32
분할 B: Gini = 0.28

답: 분할 B (지니가 더 낮음 = 더 순수)
```

**유형 3: 트리 해석**

```
문제: 다음 규칙의 의미는?
Age > 55 AND Start < 8.5 → present

답: 나이가 55개월 초과이고 수술 시작 위치가 
8.5번 척추뼈 미만이면 기형(present) 예측
```

### 8.5 실전 체크리스트

**의사결정나무 계산 시:**
- [ ] 전체 샘플 수 확인
- [ ] 각 클래스 개수 세기
- [ ] 비율 정확히 계산
- [ ] 지니/엔트로피 공식 적용
- [ ] 가중 평균 계산
- [ ] 감소량 확인

**트리 해석 시:**
- [ ] 루트부터 순서대로
- [ ] 분할 조건 명확히
- [ ] 리프 노드 예측 확인
- [ ] 중요 변수 파악

### 8.6 R vs Python 비교

| 측면 | R (rpart) | Python (sklearn) |
|------|-----------|------------------|
| **문법** | 공식 기반 (formula) | 배열 기반 |
| **시각화** | rpart.plot | plot_tree |
| **가지치기** | prune() | ccp_alpha |
| **교차검증** | xval 파라미터 | cross_val_score |
| **CP** | printcp() | cost_complexity_pruning_path |

---

## 요약

### 핵심 포인트

1. **의사결정나무는 직관적이고 해석 가능한 모델**
   - 나무 구조로 규칙 표현
   - 비전문가도 이해 가능

2. **CART는 지니 지수를 사용한 이진 분할**
   - 각 노드에서 최적 분할 찾기
   - 재귀적으로 트리 구축

3. **가지치기로 과적합 방지**
   - CP(Complexity Parameter) 조절
   - 교차검증으로 최적값 찾기

4. **Start가 Kyphosis 예측의 가장 중요한 변수**
   - 첫 분할에 사용
   - 가장 큰 지니 감소

### 시험 전 최종 점검

- [ ] 지니 지수 공식 암기
- [ ] 엔트로피와 정보이득 이해
- [ ] 가중 평균 계산 연습
- [ ] 트리 구조 해석 능력
- [ ] rpart 함수 사용법
- [ ] 가지치기 개념 이해

---

**작성일:** 2026년 1월  
**용도:** 빅데이터분석기사 의사결정나무 완벽 대비  
**참고:** R rpart 패키지, scikit-learn DecisionTreeClassifier
